{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb45076",
   "metadata": {},
   "source": [
    "#### Data Ingestion - Document Loaders\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/integrations/document_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/speech.txt'}, page_content='Avul Pakir Jainulabdeen Abdul Kalam (/ˈʌbdʊl kəˈlɑːm/ ⓘ UB-duul kə-LAHM; 15 October 1931 – 27 July 2015) was an Indian aerospace scientist and statesman who served as the president of India from 2002 to 2007.\\n\\nBorn and raised in a Muslim family in Rameswaram, Tamil Nadu, Kalam studied physics and aerospace engineering. He spent the next four decades as a scientist and science administrator, mainly at the Defence Research and Development Organisation (DRDO) and Indian Space Research Organisation (ISRO) and was intimately involved in India\\'s civilian space programme and military missile development efforts. He was known as the \"Missile Man of India\" for his work on the development of ballistic missile and launch vehicle technology. He also played a pivotal organisational, technical, and political role in Pokhran-II nuclear tests in 1998, India\\'s second such test after the first test in 1974.\\n\\nKalam was elected as the president of India in 2002 with the support of both the ruling Bharatiya Janata Party and the then-opposition Indian National Congress. He was widely referred to as the \"People\\'s President\". He engaged in teaching, writing and public service after his presidency. He was a recipient of several awards, including the Bharat Ratna, India\\'s highest civilian honour.\\n\\nWhile delivering a lecture at IIM Shillong, Kalam collapsed and died from an apparent cardiac arrest on 27 July 2015, aged 83. Thousands attended the funeral ceremony held in his hometown of Rameswaram, where he was buried with full state honours. A memorial was inaugurated near his home town in 2017.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the Text file\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('./data/speech.txt')\n",
    "\n",
    "text_documents = loader.load()\n",
    "\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0f1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/yolov7.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors\\nChien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\\n1Institute of Information Science, Academia Sinica, Taiwan\\nkinyiu@iis.sinica.edu.tw, alexeyab84@gmail.com, and liao@iis.sinica.edu.tw\\nAbstract\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS\\nand has the highest accuracy 56.8% AP among all known\\nreal-time object detectors with 30 FPS or higher on GPU\\nV100. YOLOv7-E6 object detector (56 FPS V100, 55.9%\\nAP) outperforms both transformer-based detector SWIN-\\nL Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by\\n509% in speed and 2% in accuracy, and convolutional-\\nbased detector ConvNeXt-XL Cascade-Mask R-CNN (8.6\\nFPS A100, 55.2% AP) by 551% in speed and 0.7% AP\\nin accuracy, as well as YOLOv7 outperforms: YOLOR,\\nYOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable\\nDETR, DINO-5scale-R50, ViT-Adapter-B and many other\\nobject detectors in speed and accuracy. Moreover, we train\\nYOLOv7 only on MS COCO dataset from scratch without\\nusing any other datasets or pre-trained weights. Source\\ncode is released in https://github.com/WongKinYiu/yolov7.\\n1. Introduction\\nReal-time object detection is a very important topic in\\ncomputer vision, as it is often a necessary component in\\ncomputer vision systems. For example, multi-object track-\\ning [94, 93], autonomous driving [40, 18], robotics [35, 58],\\nmedical image analysis [34, 46], etc. The computing de-\\nvices that execute real-time object detection is usually some\\nmobile CPU or GPU, as well as various neural processing\\nunits (NPU) developed by major manufacturers. For exam-\\nple, the Apple neural engine (Apple), the neural compute\\nstick (Intel), Jetson AI edge devices (Nvidia), the edge TPU\\n(Google), the neural processing engine (Qualcomm), the AI\\nprocessing unit (MediaTek), and the AI SoCs (Kneron), are\\nall NPUs. Some of the above mentioned edge devices focus\\non speeding up different operations such as vanilla convolu-\\ntion, depth-wise convolution, or MLP operations. In this pa-\\nper, the real-time object detector we proposed mainly hopes\\nthat it can support both mobile GPU and GPU devices from\\nthe edge to the cloud.\\nIn recent years, the real-time object detector is still de-\\nveloped for different edge device. For example, the devel-\\nFigure 1: Comparison with other real-time object detectors, our\\nproposed methods achieve state-of-the-arts performance.\\nopment of MCUNet [49, 48] and NanoDet [54] focused on\\nproducing low-power single-chip and improving the infer-\\nence speed on edge CPU. As for methods such as YOLOX\\n[21] and YOLOR [81], they focus on improving the infer-\\nence speed of various GPUs. More recently, the develop-\\nment of real-time object detector has focused on the de-\\nsign of efﬁcient architecture. As for real-time object de-\\ntectors that can be used on CPU [54, 88, 84, 83], their de-\\nsign is mostly based on MobileNet [28, 66, 27], ShufﬂeNet\\n[92, 55], or GhostNet [25]. Another mainstream real-time\\nobject detectors are developed for GPU [81, 21, 97], they\\nmostly use ResNet [26], DarkNet [63], or DLA [87], and\\nthen use the CSPNet [80] strategy to optimize the architec-\\nture. The development direction of the proposed methods in\\nthis paper are different from that of the current mainstream\\nreal-time object detectors. In addition to architecture op-\\ntimization, our proposed methods will focus on the opti-\\nmization of the training process. Our focus will be on some\\noptimized modules and optimization methods which may\\nstrengthen the training cost for improving the accuracy of\\nobject detection, but without increasing the inference cost.\\nWe call the proposed modules and optimization methods\\ntrainable bag-of-freebies.\\n1\\narXiv:2207.02696v1  [cs.CV]  6 Jul 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/yolov7.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='Recently, model re-parameterization [13, 12, 29] and dy-\\nnamic label assignment [20, 17, 42] have become important\\ntopics in network training and object detection. Mainly af-\\nter the above new concepts are proposed, the training of\\nobject detector evolves many new issues. In this paper, we\\nwill present some of the new issues we have discovered and\\ndevise effective methods to address them. For model re-\\nparameterization, we analyze the model re-parameterization\\nstrategies applicable to layers in different networks with the\\nconcept of gradient propagation path, and propose planned\\nre-parameterized model. In addition, when we discover that\\nwith dynamic label assignment technology, the training of\\nmodel with multiple output layers will generate new issues.\\nThat is: “How to assign dynamic targets for the outputs of\\ndifferent branches?” For this problem, we propose a new\\nlabel assignment method called coarse-to-ﬁne lead guided\\nlabel assignment.\\nThe contributions of this paper are summarized as fol-\\nlows: (1) we design several trainable bag-of-freebies meth-\\nods, so that real-time object detection can greatly improve\\nthe detection accuracy without increasing the inference\\ncost; (2) for the evolution of object detection methods, we\\nfound two new issues, namely how re-parameterized mod-\\nule replaces original module, and how dynamic label as-\\nsignment strategy deals with assignment to different output\\nlayers. In addition, we also propose methods to address the\\ndifﬁculties arising from these issues; (3) we propose “ex-\\ntend” and “compound scaling” methods for the real-time\\nobject detector that can effectively utilize parameters and\\ncomputation; and (4) the method we proposed can effec-\\ntively reduce about 40% parameters and 50% computation\\nof state-of-the-art real-time object detector, and has faster\\ninference speed and higher detection accuracy.\\n2. Related work\\n2.1. Real-time object detectors\\nCurrently state-of-the-art real-time object detectors are\\nmainly based on YOLO [61, 62, 63] and FCOS [76, 77],\\nwhich are [3, 79, 81, 21, 54, 85, 23]. Being able to become\\na state-of-the-art real-time object detector usually requires\\nthe following characteristics: (1) a faster and stronger net-\\nwork architecture; (2) a more effective feature integration\\nmethod [22, 97, 37, 74, 59, 30, 9, 45]; (3) a more accurate\\ndetection method [76, 77, 69]; (4) a more robust loss func-\\ntion [96, 64, 6, 56, 95, 57]; (5) a more efﬁcient label assign-\\nment method [99, 20, 17, 82, 42]; and (6) a more efﬁcient\\ntraining method. In this paper, we do not intend to explore\\nself-supervised learning or knowledge distillation methods\\nthat require additional data or large model. Instead, we will\\ndesign new trainable bag-of-freebies method for the issues\\nderived from the state-of-the-art methods associated with\\n(4), (5), and (6) mentioned above.\\n2.2. Model re-parameterization\\nModel re-parametrization techniques [71, 31, 75, 19, 33,\\n11, 4, 24, 13, 12, 10, 29, 14, 78] merge multiple compu-\\ntational modules into one at inference stage. The model\\nre-parameterization technique can be regarded as an en-\\nsemble technique, and we can divide it into two cate-\\ngories, i.e., module-level ensemble and model-level ensem-\\nble. There are two common practices for model-level re-\\nparameterization to obtain the ﬁnal inference model. One\\nis to train multiple identical models with different train-\\ning data, and then average the weights of multiple trained\\nmodels. The other is to perform a weighted average of the\\nweights of models at different iteration number. Module-\\nlevel re-parameterization is a more popular research issue\\nrecently. This type of method splits a module into multi-\\nple identical or different module branches during training\\nand integrates multiple branched modules into a completely\\nequivalent module during inference. However, not all pro-\\nposed re-parameterized module can be perfectly applied to\\ndifferent architectures. With this in mind, we have devel-\\noped new re-parameterization module and designed related\\napplication strategies for various architectures.\\n2.3. Model scaling\\nModel scaling [72, 60, 74, 73, 15, 16, 2, 51] is a way\\nto scale up or down an already designed model and make\\nit ﬁt in different computing devices. The model scaling\\nmethod usually uses different scaling factors, such as reso-\\nlution (size of input image), depth (number of layer), width\\n(number of channel), and stage (number of feature pyra-\\nmid), so as to achieve a good trade-off for the amount of\\nnetwork parameters, computation, inference speed, and ac-\\ncuracy. Network architecture search (NAS) is one of the\\ncommonly used model scaling methods. NAS can automat-\\nically search for suitable scaling factors from search space\\nwithout deﬁning too complicated rules. The disadvantage\\nof NAS is that it requires very expensive computation to\\ncomplete the search for model scaling factors. In [15], the\\nresearcher analyzes the relationship between scaling factors\\nand the amount of parameters and operations, trying to di-\\nrectly estimate some rules, and thereby obtain the scaling\\nfactors required by model scaling. Checking the literature,\\nwe found that almost all model scaling methods analyze in-\\ndividual scaling factor independently, and even the methods\\nin the compound scaling category also optimized scaling\\nfactor independently. The reason for this is because most\\npopular NAS architectures deal with scaling factors that are\\nnot very correlated. We observed that all concatenation-\\nbased models, such as DenseNet [32] or V oVNet [39], will\\nchange the input width of some layers when the depth of\\nsuch models is scaled. Since the proposed architecture is\\nconcatenation-based, we have to design a new compound\\nscaling method for this model.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/yolov7.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 2: Extended efﬁcient layer aggregation networks. The proposed extended ELAN (E-ELAN) does not change the gradient transmis-\\nsion path of the original architecture at all, but use group convolution to increase the cardinality of the added features, and combine the\\nfeatures of different groups in a shufﬂe and merge cardinality manner. This way of operation can enhance the features learned by different\\nfeature maps and improve the use of parameters and calculations.\\n3. Architecture\\n3.1. Extended efﬁcient layer aggregation networks\\nIn most of the literature on designing the efﬁcient ar-\\nchitectures, the main considerations are no more than the\\nnumber of parameters, the amount of computation, and the\\ncomputational density. Starting from the characteristics of\\nmemory access cost, Ma et al. [55] also analyzed the in-\\nﬂuence of the input/output channel ratio, the number of\\nbranches of the architecture, and the element-wise opera-\\ntion on the network inference speed. Doll´ar et al. [15] addi-\\ntionally considered activation when performing model scal-\\ning, that is, to put more consideration on the number of el-\\nements in the output tensors of convolutional layers. The\\ndesign of CSPV oVNet [79] in Figure 2 (b) is a variation of\\nV oVNet [39]. In addition to considering the aforementioned\\nbasic designing concerns, the architecture of CSPV oVNet\\n[79] also analyzes the gradient path, in order to enable the\\nweights of different layers to learn more diverse features.\\nThe gradient analysis approach described above makes in-\\nferences faster and more accurate. ELAN [1] in Figure 2 (c)\\nconsiders the following design strategy – “How to design an\\nefﬁcient network?.” They came out with a conclusion: By\\ncontrolling the shortest longest gradient path, a deeper net-\\nwork can learn and converge effectively. In this paper, we\\npropose Extended-ELAN (E-ELAN) based on ELAN and\\nits main architecture is shown in Figure 2 (d).\\nRegardless of the gradient path length and the stacking\\nnumber of computational blocks in large-scale ELAN, it has\\nreached a stable state. If more computational blocks are\\nstacked unlimitedly, this stable state may be destroyed, and\\nthe parameter utilization rate will decrease. The proposed\\nE-ELAN uses expand, shufﬂe, merge cardinality to achieve\\nthe ability to continuously enhance the learning ability of\\nthe network without destroying the original gradient path.\\nIn terms of architecture, E-ELAN only changes the archi-\\ntecture in computational block, while the architecture of\\ntransition layer is completely unchanged. Our strategy is\\nto use group convolution to expand the channel and car-\\ndinality of computational blocks. We will apply the same\\ngroup parameter and channel multiplier to all the compu-\\ntational blocks of a computational layer. Then, the feature\\nmap calculated by each computational block will be shuf-\\nﬂed into g groups according to the set group parameter g,\\nand then concatenate them together. At this time, the num-\\nber of channels in each group of feature map will be the\\nsame as the number of channels in the original architec-\\nture. Finally, we add g groups of feature maps to perform\\nmerge cardinality. In addition to maintaining the original\\nELAN design architecture, E-ELAN can also guide differ-\\nent groups of computational blocks to learn more diverse\\nfeatures.\\n3.2. Model scaling for concatenation-based models\\nThe main purpose of model scaling is to adjust some at-\\ntributes of the model and generate models of different scales\\nto meet the needs of different inference speeds. For ex-\\nample the scaling model of EfﬁcientNet [72] considers the\\nwidth, depth, and resolution. As for the scaled-YOLOv4\\n[79], its scaling model is to adjust the number of stages. In\\n[15], Doll´ar et al. analyzed the inﬂuence of vanilla convolu-\\ntion and group convolution on the amount of parameter and\\ncomputation when performing width and depth scaling, and\\nused this to design the corresponding model scaling method.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/yolov7.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Figure 3: Model scaling for concatenation-based models. From (a) to (b), we observe that when depth scaling is performed on\\nconcatenation-based models, the output width of a computational block also increases. This phenomenon will cause the input width\\nof the subsequent transmission layer to increase. Therefore, we propose (c), that is, when performing model scaling on concatenation-\\nbased models, only the depth in a computational block needs to be scaled, and the remaining of transmission layer is performed with\\ncorresponding width scaling.\\nThe above methods are mainly used in architectures such as\\nPlainNet or ResNet. When these architectures are in execut-\\ning scaling up or scaling down, the in-degree and out-degree\\nof each layer will not change, so we can independently an-\\nalyze the impact of each scaling factor on the amount of\\nparameters and computation. However, if these methods\\nare applied to the concatenation-based architecture, we will\\nﬁnd that when scaling up or scaling down is performed on\\ndepth, the in-degree of a translation layer which is immedi-\\nately after a concatenation-based computational block will\\ndecrease or increase, as shown in Figure 3 (a) and (b).\\nIt can be inferred from the above phenomenon that\\nwe cannot analyze different scaling factors separately for\\na concatenation-based model but must be considered to-\\ngether. Take scaling-up depth as an example, such an ac-\\ntion will cause a ratio change between the input channel and\\noutput channel of a transition layer, which may lead to a de-\\ncrease in the hardware usage of the model. Therefore, we\\nmust propose the corresponding compound model scaling\\nmethod for a concatenation-based model. When we scale\\nthe depth factor of a computational block, we must also cal-\\nculate the change of the output channel of that block. Then,\\nwe will perform width factor scaling with the same amount\\nof change on the transition layers, and the result is shown\\nin Figure 3 (c). Our proposed compound scaling method\\ncan maintain the properties that the model had at the initial\\ndesign and maintains the optimal structure.\\n4. Trainable bag-of-freebies\\n4.1. Planned re-parameterized convolution\\nAlthough RepConv [13] has achieved excellent perfor-\\nmance on the VGG [68], when we directly apply it to\\nResNet [26] and DenseNet [32] and other architectures,\\nits accuracy will be signiﬁcantly reduced. We use gradi-\\nent ﬂow propagation paths to analyze how re-parameterized\\nconvolution should be combined with different network.\\nWe also designed planned re-parameterized convolution ac-\\ncordingly.\\nFigure 4: Planned re-parameterized model. In the proposed\\nplanned re-parameterized model, we found that a layer with resid-\\nual or concatenation connections, its RepConv should not have\\nidentity connection. Under these circumstances, it can be replaced\\nby RepConvN that contains no identity connections.\\nRepConv actually combines 3 × 3 convolution, 1 × 1\\nconvolution, and identity connection in one convolutional\\nlayer. After analyzing the combination and correspond-\\ning performance of RepConv and different architectures,\\nwe ﬁnd that the identity connection in RepConv destroys\\nthe residual in ResNet and the concatenation in DenseNet,\\nwhich provides more diversity of gradients for different fea-\\nture maps. For the above reasons, we use RepConv with-\\nout identity connection (RepConvN) to design the architec-\\nture of planned re-parameterized convolution. In our think-\\ning, when a convolutional layer with residual or concate-\\nnation is replaced by re-parameterized convolution, there\\nshould be no identity connection. Figure 4 shows an exam-\\nple of our designed “planned re-parameterized convolution”\\nused in PlainNet and ResNet. As for the complete planned\\nre-parameterized convolution experiment in residual-based\\nmodel and concatenation-based model, it will be presented\\nin the ablation study session.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/yolov7.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='Figure 5: Coarse for auxiliary and ﬁne for lead head label assigner. Compare with normal model (a), the schema in (b) has auxiliary head.\\nDifferent from the usual independent label assigner (c), we propose (d) lead head guided label assigner and (e) coarse-to-ﬁne lead head\\nguided label assigner. The proposed label assigner is optimized by lead head prediction and the ground truth to get the labels of training\\nlead head and auxiliary head at the same time. The detailed coarse-to-ﬁne implementation method and constraint design details will be\\nelaborated in Apendix.\\n4.2. Coarse for auxiliary and ﬁne for lead loss\\nDeep supervision [38] is a technique that is often used\\nin training deep networks. Its main concept is to add\\nextra auxiliary head in the middle layers of the network,\\nand the shallow network weights with assistant loss as the\\nguide. Even for architectures such as ResNet [26] and\\nDenseNet [32] which usually converge well, deep supervi-\\nsion [70, 98, 67, 47, 82, 65, 86, 50] can still signiﬁcantly\\nimprove the performance of the model on many tasks. Fig-\\nure 5 (a) and (b) show, respectively, the object detector ar-\\nchitecture “without” and “with” deep supervision. In this\\npaper, we call the head responsible for the ﬁnal output as\\nthe lead head, and the head used to assist training is called\\nauxiliary head.\\nNext we want to discuss the issue of label assignment. In\\nthe past, in the training of deep network, label assignment\\nusually refers directly to the ground truth and generate hard\\nlabel according to the given rules. However, in recent years,\\nif we take object detection as an example, researchers often\\nuse the quality and distribution of prediction output by the\\nnetwork, and then consider together with the ground truth to\\nuse some calculation and optimization methods to generate\\na reliable soft label [61, 8, 36, 99, 91, 44, 43, 90, 20, 17, 42].\\nFor example, YOLO [61] use IoU of prediction of bounding\\nbox regression and ground truth as the soft label of object-\\nness. In this paper, we call the mechanism that considers\\nthe network prediction results together with the ground truth\\nand then assigns soft labels as “label assigner.”\\nDeep supervision needs to be trained on the target ob-\\njectives regardless of the circumstances of auxiliary head or\\nlead head. During the development of soft label assigner re-\\nlated techniques, we accidentally discovered a new deriva-\\ntive issue, i.e., “How to assign soft label to auxiliary head\\nand lead head ?” To the best of our knowledge, the relevant\\nliterature has not explored this issue so far. The results of\\nthe most popular method at present is as shown in Figure 5\\n(c), which is to separate auxiliary head and lead head, and\\nthen use their own prediction results and the ground truth\\nto execute label assignment. The method proposed in this\\npaper is a new label assignment method that guides both\\nauxiliary head and lead head by the lead head prediction.\\nIn other words, we use lead head prediction as guidance to\\ngenerate coarse-to-ﬁne hierarchical labels, which are used\\nfor auxiliary head and lead head learning, respectively. The\\ntwo proposed deep supervision label assignment strategies\\nare shown in Figure 5 (d) and (e), respectively.\\nLead head guided label assigner is mainly calculated\\nbased on the prediction result of the lead head and the\\nground truth, and generate soft label through the optimiza-\\ntion process. This set of soft labels will be used as the tar-\\nget training model for both auxiliary head and lead head.\\nThe reason to do this is because lead head has a relatively\\nstrong learning capability, so the soft label generated from it\\nshould be more representative of the distribution and corre-\\nlation between the source data and the target. Furthermore,\\nwe can view such learning as a kind of generalized residual\\nlearning. By letting the shallower auxiliary head directly\\nlearn the information that lead head has learned, lead head\\nwill be more able to focus on learning residual information\\nthat has not yet been learned.\\nCoarse-to-ﬁne lead head guided label assigner also\\nused the predicted result of the lead head and the ground\\ntruth to generate soft label. However, in the process we gen-\\nerate two different sets of soft label, i.e., coarse label and\\nﬁne label, where ﬁne label is the same as the soft label gen-\\nerated by lead head guided label assigner, and coarse label\\nis generated by allowing more grids to be treated as posi-\\ntive target by relaxing the constraints of the positive sample\\nassignment process. The reason for this is that the learning\\nability of an auxiliary head is not as strong as that of a lead\\nhead, and in order to avoid losing the information that needs\\nto be learned, we will focus on optimizing the recall of aux-\\niliary head in the object detection task. As for the output\\nof lead head, we can ﬁlter the high precision results from\\nthe high recall results as the ﬁnal output. However, we must\\nnote that if the additional weight of coarse label is close to\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/yolov7.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Table 1: Comparison of baseline object detectors.\\nModel #Param. FLOPs Size AP val APval\\n50 APval\\n75 APval\\nS APval\\nM APval\\nL\\nYOLOv4 [3] 64.4M 142.8G 640 49.7% 68.2% 54.3% 32.9% 54.8% 63.7%\\nYOLOR-u5 (r6.1) [81] 46.5M 109.1G 640 50.2% 68.7% 54.6% 33.2% 55.5% 63.7%\\nYOLOv4-CSP [79] 52.9M 120.4G 640 50.3% 68.6% 54.9% 34.2% 55.6% 65.1%\\nYOLOR-CSP [81] 52.9M 120.4G 640 50.8% 69.5% 55.3% 33.7% 56.0% 65.4%\\nYOLOv7 36.9M 104.7G 640 51.2% 69.7% 55.5% 35.2% 56.0% 66.7%\\nimprovement -43% -15% - +0.4 +0.2 +0.2 +1.5 = +1.3\\nYOLOR-CSP-X [81] 96.9M 226.8G 640 52.7% 71.3% 57.4% 36.3% 57.5% 68.3%\\nYOLOv7-X 71.3M 189.9G 640 52.9% 71.1% 57.5% 36.9% 57.7% 68.6%\\nimprovement -36% -19% - +0.2 -0.2 +0.1 +0.6 +0.2 +0.3\\nYOLOv4-tiny [79] 6.1 6.9 416 24.9% 42.1% 25.7% 8.7% 28.4% 39.2%\\nYOLOv7-tiny 6.2 5.8 416 35.2% 52.8% 37.3% 15.7% 38.0% 53.4%\\nimprovement +2% -19% - +10.3 +10.7 +11.6 +7.0 +9.6 +14.2\\nYOLOv4-tiny-3l [79] 8.7 5.2 320 30.8% 47.3% 32.2% 10.9% 31.9% 51.5%\\nYOLOv7-tiny 6.2 3.5 320 30.8% 47.3% 32.2% 10.0% 31.9% 52.2%\\nimprovement -39% -49% - = = = -0.9 = +0.7\\nYOLOR-E6 [81] 115.8M 683.2G 1280 55.7% 73.2% 60.7% 40.1% 60.4% 69.2%\\nYOLOv7-E6 97.2M 515.2G 1280 55.9% 73.5% 61.1% 40.6% 60.3% 70.0%\\nimprovement -19% -33% - +0.2 +0.3 +0.4 +0.5 -0.1 +0.8\\nYOLOR-D6 [81] 151.7M 935.6G 1280 56.1% 73.9% 61.2% 42.4% 60.5% 69.9%\\nYOLOv7-D6 154.7M 806.8G 1280 56.3% 73.8% 61.4% 41.3% 60.6% 70.1%\\nYOLOv7-E6E 151.7M 843.2G 1280 56.8% 74.4% 62.1% 40.8% 62.1% 70.6%\\nimprovement = -11% - +0.7 +0.5 +0.9 -1.6 +1.6 +0.7\\nthat of ﬁne label, it may produce bad prior at ﬁnal predic-\\ntion. Therefore, in order to make those extra coarse positive\\ngrids have less impact, we put restrictions in the decoder,\\nso that the extra coarse positive grids cannot produce soft\\nlabel perfectly. The mechanism mentioned above allows\\nthe importance of ﬁne label and coarse label to be dynam-\\nically adjusted during the learning process, and makes the\\noptimizable upper bound of ﬁne label always higher than\\ncoarse label.\\n4.3. Other trainable bag-of-freebies\\nIn this section we will list some trainable bag-of-\\nfreebies. These freebies are some of the tricks we used\\nin training, but the original concepts were not proposed\\nby us. The training details of these freebies will be elab-\\norated in the Appendix, including (1) Batch normalization\\nin conv-bn-activation topology: This part mainly connects\\nbatch normalization layer directly to convolutional layer.\\nThe purpose of this is to integrate the mean and variance\\nof batch normalization into the bias and weight of convolu-\\ntional layer at the inference stage. (2) Implicit knowledge\\nin YOLOR [81] combined with convolution feature map in\\naddition and multiplication manner: Implicit knowledge in\\nYOLOR can be simpliﬁed to a vector by pre-computing at\\nthe inference stage. This vector can be combined with the\\nbias and weight of the previous or subsequent convolutional\\nlayer. (3) EMA model: EMA is a technique used in mean\\nteacher [75], and in our system we use EMA model purely\\nas the ﬁnal inference model.\\n5. Experiments\\n5.1. Experimental setup\\nWe use Microsoft COCO dataset to conduct experiments\\nand validate our object detection method. All our experi-\\nments did not use pre-trained models. That is, all models\\nwere trained from scratch. During the development pro-\\ncess, we used train 2017 set for training, and then used val\\n2017 set for veriﬁcation and choosing hyperparameters. Fi-\\nnally, we show the performance of object detection on the\\ntest 2017 set and compare it with the state-of-the-art object\\ndetection algorithms. Detailed training parameter settings\\nare described in Appendix.\\nWe designed basic model for edge GPU, normal GPU,\\nand cloud GPU, and they are respectively called YOLOv7-\\ntiny, YOLOv7, and YOLOv7-W6. At the same time, we\\nalso use basic model for model scaling for different ser-\\nvice requirements and get different types of models. For\\nYOLOv7, we do stack scaling on neck, and use the pro-\\nposed compound scaling method to perform scaling-up of\\nthe depth and width of the entire model, and use this to ob-\\ntain YOLOv7-X. As for YOLOv7-W6, we use the newly\\nproposed compound scaling method to obtain YOLOv7-E6\\nand YOLOv7-D6. In addition, we use the proposed E-\\nELAN for YOLOv7-E6, and thereby complete YOLOv7-\\nE6E. Since YOLOv7-tiny is an edge GPU-oriented archi-\\ntecture, it will use leaky ReLU as activation function. As\\nfor other models we use SiLU as activation function. We\\nwill describe the scaling factor of each model in detail in\\nAppendix.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/yolov7.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='Table 2: Comparison of state-of-the-art real-time object detectors.\\nModel #Param. FLOPs Size FPS APtest / APval APtest\\n50 APtest\\n75 APtest\\nS APtest\\nM APtest\\nL\\nYOLOX-S [21] 9.0M 26.8G 640 102 40.5% / 40.5% - - - - -\\nYOLOX-M [21] 25.3M 73.8G 640 81 47.2% / 46.9% - - - - -\\nYOLOX-L [21] 54.2M 155.6G 640 69 50.1% / 49.7% - - - - -\\nYOLOX-X [21] 99.1M 281.9G 640 58 51.5% / 51.1% - - - - -\\nPPYOLOE-S [85] 7.9M 17.4G 640 208 43.1% / 42.7% 60.5% 46.6% 23.2% 46.4% 56.9%\\nPPYOLOE-M [85] 23.4M 49.9G 640 123 48.9% / 48.6% 66.5% 53.0% 28.6% 52.9% 63.8%\\nPPYOLOE-L [85] 52.2M 110.1G 640 78 51.4% / 50.9% 68.9% 55.6% 31.4% 55.3% 66.1%\\nPPYOLOE-X [85] 98.4M 206.6G 640 45 52.2% / 51.9% 69.9% 56.5% 33.3% 56.3% 66.4%\\nYOLOv5-N (r6.1) [23] 1.9M 4.5G 640 159 - / 28.0% - - - - -\\nYOLOv5-S (r6.1) [23] 7.2M 16.5G 640 156 - / 37.4% - - - - -\\nYOLOv5-M (r6.1) [23] 21.2M 49.0G 640 122 - / 45.4% - - - - -\\nYOLOv5-L (r6.1) [23] 46.5M 109.1G 640 99 - / 49.0% - - - - -\\nYOLOv5-X (r6.1) [23] 86.7M 205.7G 640 83 - / 50.7% - - - - -\\nYOLOR-CSP [81] 52.9M 120.4G 640 106 51.1% / 50.8% 69.6% 55.7% 31.7% 55.3% 64.7%\\nYOLOR-CSP-X [81] 96.9M 226.8G 640 87 53.0% / 52.7% 71.4% 57.9% 33.7% 57.1% 66.8%\\nYOLOv7-tiny-SiLU 6.2M 13.8G 640 286 38.7% / 38.7% 56.7% 41.7% 18.8% 42.4% 51.9%\\nYOLOv7 36.9M 104.7G 640 161 51.4% / 51.2% 69.7% 55.9% 31.8% 55.5% 65.0%\\nYOLOv7-X 71.3M 189.9G 640 114 53.1% / 52.9% 71.2% 57.8% 33.8% 57.1% 67.4%\\nYOLOv5-N6 (r6.1) [23] 3.2M 18.4G 1280 123 - / 36.0% - - - - -\\nYOLOv5-S6 (r6.1) [23] 12.6M 67.2G 1280 122 - / 44.8% - - - - -\\nYOLOv5-M6 (r6.1) [23] 35.7M 200.0G 1280 90 - / 51.3% - - - - -\\nYOLOv5-L6 (r6.1) [23] 76.8M 445.6G 1280 63 - / 53.7% - - - - -\\nYOLOv5-X6 (r6.1) [23] 140.7M 839.2G 1280 38 - / 55.0% - - - - -\\nYOLOR-P6 [81] 37.2M 325.6G 1280 76 53.9% / 53.5% 71.4% 58.9% 36.1% 57.7% 65.6%\\nYOLOR-W6 [81] 79.8G 453.2G 1280 66 55.2% / 54.8% 72.7% 60.5% 37.7% 59.1% 67.1%\\nYOLOR-E6 [81] 115.8M 683.2G 1280 45 55.8% / 55.7% 73.4% 61.1% 38.4% 59.7% 67.7%\\nYOLOR-D6 [81] 151.7M 935.6G 1280 34 56.5% / 56.1% 74.1% 61.9% 38.9% 60.4% 68.7%\\nYOLOv7-W6 70.4M 360.0G 1280 84 54.9% / 54.6% 72.6% 60.1% 37.3% 58.7% 67.1%\\nYOLOv7-E6 97.2M 515.2G 1280 56 56.0% / 55.9% 73.5% 61.2% 38.0% 59.9% 68.4%\\nYOLOv7-D6 154.7M 806.8G 1280 44 56.6% / 56.3% 74.0% 61.8% 38.8% 60.1% 69.5%\\nYOLOv7-E6E 151.7M 843.2G 1280 36 56.8% / 56.8% 74.4% 62.1% 39.3% 60.5% 69.0%\\n1 Our FLOPs is calaculated by rectangle input resolution like 640 × 640 or 1280 × 1280.\\n2 Our inference time is estimated by using letterbox resize input image to make its long side equals to 640 or 1280.\\n5.2. Baselines\\nWe choose previous version of YOLO [3, 79] and state-\\nof-the-art object detector YOLOR [81] as our baselines. Ta-\\nble 1 shows the comparison of our proposed YOLOv7 mod-\\nels and those baseline that are trained with the same settings.\\nFrom the results we see that if compared with YOLOv4,\\nYOLOv7 has 75% less parameters, 36% less computation,\\nand brings 1.5% higher AP. If compared with state-of-the-\\nart YOLOR-CSP, YOLOv7 has 43% fewer parameters, 15%\\nless computation, and 0.4% higher AP. In the performance\\nof tiny model, compared with YOLOv4-tiny-31, YOLOv7-\\ntiny reduces the number of parameters by 39% and the\\namount of computation by 49%, but maintains the same AP.\\nOn the cloud GPU model, our model can still have a higher\\nAP while reducing the number of parameters by 19% and\\nthe amount of computation by 33%.\\n5.3. Comparison with state-of-the-arts\\nWe compare the proposed method with state-of-the-art\\nobject detectors for general GPUs and Mobile GPUs, and\\nthe results are shown in Table 2. From the results in\\nTable 2 we know that the proposed method has the best\\nspeed-accuracy trade-off comprehensively. If we compare\\nYOLOv7-tiny-SiLU with YOLOv5-N (r6.1), our method\\nis 127 fps faster and 10.7% more accurate on AP. In ad-\\ndition, YOLOv7 has 51.4% AP at frame rate of 161 fps,\\nwhile PPYOLOE-L with the same AP has only 78 fps frame\\nrate. In terms of parameter usage, YOLOv7 is 41% less than\\nPPYOLOE-L. If we compare YOLOv7-X with 114 fps in-\\nference speed to YOLOv5-L (r6.1) with 99 fps inference\\nspeed, YOLOv7-X can improve AP by 3.9%. If YOLOv7-\\nX is compared with YOLOv5-X (r6.1) of similar scale, the\\ninference speed of YOLOv7-X is 31 fps faster. In addi-\\ntion, in terms of the amount of parameters and computation,\\nYOLOv7-X reduces 22% of parameters and 8% of compu-\\ntation compared to YOLOv5-X (r6.1), but improves AP by\\n2.2%.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/yolov7.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='If we compare YOLOv7 with YOLOR using the input\\nresolution 1280, the inference speed of YOLOv7-W6 is 8\\nfps faster than that of YOLOR-P6, and the detection rate is\\nalso increased by 1% AP. As for the comparison between\\nYOLOv7-E6 and YOLOv5-X6 (r6.1), the former has 0.9%\\nAP gain than the latter, 45% less parameters and 63% less\\ncomputation, and the inference speed is increased by 47%.\\nYOLOv7-D6 has close inference speed to YOLOR-E6, but\\nimproves AP by 0.8%. YOLOv7-E6E has close inference\\nspeed to YOLOR-D6, but improves AP by 0.3%.\\n5.4. Ablation study\\n5.4.1 Proposed compound scaling method\\nTable 3 shows the results obtained when using different\\nmodel scaling strategies for scaling up. Among them, our\\nproposed compound scaling method is to scale up the depth\\nof computational block by 1.5 times and the width of tran-\\nsition block by 1.25 times. If our method is compared with\\nthe method that only scaled up the width, our method can\\nimprove the AP by 0.5% with less parameters and amount\\nof computation. If our method is compared with the method\\nthat only scales up the depth, our method only needs to in-\\ncrease the number of parameters by 2.9% and the amount of\\ncomputation by 1.2%, which can improve the AP by 0.2%.\\nIt can be seen from the results of Table 3 that our proposed\\ncompound scaling strategy can utilize parameters and com-\\nputation more efﬁciently.\\nTable 3: Ablation study on proposed model scaling.\\nModel #Param. FLOPs Size AP val APval\\n50 APval\\n75\\nbase (v7-X light) 47.0M 125.5G 640 51.7% 70.1% 56.0%\\nwidth only (1.25 w) 73.4M 195.5G 640 52.4% 70.9% 57.1%\\ndepth only (2.0 d) 69.3M 187.6G 640 52.7% 70.8% 57.3%\\ncompound (v7-X) 71.3M 189.9G 640 52.9% 71.1% 57.5%\\nimprovement - - - +1.2 +1.0 +1.5\\n5.4.2 Proposed planned re-parameterized model\\nIn order to verify the generality of our proposed planed\\nre-parameterized model, we use it on concatenation-based\\nmodel and residual-based model respectively for veriﬁca-\\ntion. The concatenation-based model and residual-based\\nmodel we chose for veriﬁcation are 3-stacked ELAN and\\nCSPDarknet, respectively.\\nIn the experiment of concatenation-based model, we re-\\nplace the 3 ×3 convolutional layers in different positions in\\n3-stacked ELAN with RepConv, and the detailed conﬁgura-\\ntion is shown in Figure 6. From the results shown in Table 4\\nwe see that all higher AP values are present on our proposed\\nplanned re-parameterized model.\\nIn the experiment dealing with residual-based model,\\nsince the original dark block does not have a 3 × 3 con-\\nFigure 6: Planned RepConv 3-stacked ELAN. Blue circles are the\\nposition we replace Conv by RepConv.\\nTable 4: Ablation study on planned RepConcatenation model.\\nModel AP val APval\\n50 APval\\n75 APval\\nS APval\\nM APval\\nL\\nbase (3-S ELAN) 52.26% 70.41% 56.77% 35.81% 57.00% 67.59%\\nFigure 6 (a) 52.18% 70.34% 56.90% 35.71% 56.83% 67.51%\\nFigure 6 (b) 52.30% 70.30% 56.92% 35.76% 56.95% 67.74%\\nFigure 6 (c) 52.33% 70.56% 56.91% 35.90% 57.06% 67.50%\\nFigure 6 (d) 52.17% 70.32% 56.82% 35.33% 57.06% 68.09%\\nFigure 6 (e) 52.23% 70.20% 56.81% 35.34% 56.97% 66.88%\\nvolution block that conforms to our design strategy, we ad-\\nditionally design a reversed dark block for the experiment,\\nwhose architecture is shown in Figure 7. Since the CSP-\\nDarknet with dark block and reversed dark block has exactly\\nthe same amount of parameters and operations, it is fair to\\ncompare. The experiment results illustrated in Table 5 fully\\nconﬁrm that the proposed planned re-parameterized model\\nis equally effective on residual-based model. We ﬁnd that\\nthe design of RepCSPResNet [85] also ﬁt our design pat-\\ntern.\\nFigure 7: Reversed CSPDarknet. We reverse the position of 1 × 1\\nand 3 × 3 convolutional layer in dark block to ﬁt our planned re-\\nparameterized model design strategy.\\nTable 5: Ablation study on planned RepResidual model.\\nModel AP val APval\\n50 APval\\n75 APval\\nS APval\\nM APval\\nL\\nbase (YOLOR-W6) 54.82% 72.39% 59.95% 39.68% 59.38% 68.30%\\nRepCSP 54.67% 72.50% 59.58% 40.22% 59.61% 67.87%\\nRCSP 54.36% 71.95% 59.54% 40.15% 59.02% 67.44%\\nRepRCSP 54.85% 72.51% 60.08% 40.53% 59.52% 68.06%\\nbase (YOLOR-CSP) 50.81% 69.47% 55.28% 33.74% 56.01% 65.38%\\nRepRCSP 50.91% 69.54% 55.55% 34.44% 55.74% 65.46%\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/yolov7.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Figure 8: Objectness map predicted by different methods at auxiliary head and lead head.\\n5.4.3 Proposed assistant loss for auxiliary head\\nIn the assistant loss for auxiliary head experiments, we com-\\npare the general independent label assignment for lead head\\nand auxiliary head methods, and we also compare the two\\nproposed lead guided label assignment methods. We show\\nall comparison results in Table 6. From the results listed in\\nTable 6, it is clear that any model that increases assistant\\nloss can signiﬁcantly improve the overall performance. In\\naddition, our proposed lead guided label assignment strat-\\negy receives better performance than the general indepen-\\ndent label assignment strategy in AP, AP 50, and AP75. As\\nfor our proposed coarse for assistant and ﬁne for lead label\\nassignment strategy, it results in best results in all cases. In\\nFigure 8 we show the objectness map predicted by different\\nmethods at auxiliary head and lead head. From Figure 8 we\\nﬁnd that if auxiliary head learns lead guided soft label, it\\nwill indeed help lead head to extract the residual informa-\\ntion from the consistant targets.\\nTable 6: Ablation study on proposed auxiliary head.\\nModel Size AP val APval\\n50 APval\\n75\\nbase (v7-E6) 1280 55.6% 73.2% 60.7%\\nindependent 1280 55.8% 73.4% 60.9%\\nlead guided 1280 55.9% 73.5% 61.0%\\ncoarse-to-ﬁne lead guided 1280 55.9% 73.5% 61.1%\\nimprovement - +0.3 +0.3 +0.4\\nIn Table 7 we further analyze the effect of the proposed\\ncoarse-to-ﬁne lead guided label assignment method on the\\ndecoder of auxiliary head. That is, we compared the results\\nof with/without the introduction of upper bound constraint.\\nJudging from the numbers in the Table, the method of con-\\nstraining the upper bound of objectness by the distance from\\nthe center of the object can achieve better performance.\\nTable 7: Ablation study on constrained auxiliary head.\\nModel Size AP val APval\\n50 APval\\n75\\nbase (v7-E6) 1280 55.6% 73.2% 60.7%\\naux without constraint 1280 55.9% 73.5% 61.0%\\naux with constraint 1280 55.9% 73.5% 61.1%\\nimprovement - +0.3 +0.3 +0.4\\nSince the proposed YOLOv7 uses multiple pyramids to\\njointly predict object detection results, we can directly con-\\nnect auxiliary head to the pyramid in the middle layer for\\ntraining. This type of training can make up for informa-\\ntion that may be lost in the next level pyramid prediction.\\nFor the above reasons, we designed partial auxiliary head\\nin the proposed E-ELAN architecture. Our approach is to\\nconnect auxiliary head after one of the sets of feature map\\nbefore merging cardinality, and this connection can make\\nthe weight of the newly generated set of feature map not\\ndirectly updated by assistant loss. Our design allows each\\npyramid of lead head to still get information from objects\\nwith different sizes. Table 8 shows the results obtained us-\\ning two different methods, i.e., coarse-to-ﬁne lead guided\\nand partial coarse-to-ﬁne lead guided methods. Obviously,\\nthe partial coarse-to-ﬁne lead guided method has a better\\nauxiliary effect.\\nTable 8: Ablation study on partial auxiliary head.\\nModel Size AP val APval\\n50 APval\\n75\\nbase (v7-E6E) 1280 56.3% 74.0% 61.5%\\naux 1280 56.5% 74.0% 61.6%\\npartial aux 1280 56.8% 74.4% 62.1%\\nimprovement - +0.5 +0.4 +0.6\\n6. Conclusions\\nIn this paper we propose a new architecture of real-\\ntime object detector and the corresponding model scaling\\nmethod. Furthermore, we ﬁnd that the evolving process\\nof object detection methods generates new research top-\\nics. During the research process, we found the replace-\\nment problem of re-parameterized module and the alloca-\\ntion problem of dynamic label assignment. To solve the\\nproblem, we propose the trainable bag-of-freebies method\\nto enhance the accuracy of object detection. Based on the\\nabove, we have developed the YOLOv7 series of object de-\\ntection systems, which receives the state-of-the-art results.\\n7. Acknowledgements\\nThe authors wish to thank National Center for High-\\nperformance Computing (NCHC) for providing computa-\\ntional and storage resources.\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/yolov7.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Table 9: More comparison (batch=1, no-TRT, without extra object detection training data)\\nModel #Param. FLOPs Size FPSV 100 APtest / APval APtest\\n50 APtest\\n75\\nYOLOv7-tiny-SiLU 6.2M 13.8G 640 286 38.7% / 38.7% 56.7% 41.7%\\nPPYOLOE-S [85] 7.9M 17.4G 640 208 43.1% / 42.7% 60.5% 46.6%\\nYOLOv7 36.9M 104.7G 640 161 51.4% / 51.2% 69.7% 55.9%\\nYOLOv5-N (r6.1) [23] 1.9M 4.5G 640 159 - / 28.0% - -\\nYOLOv5-S (r6.1) [23] 7.2M 16.5G 640 156 - / 37.4% - -\\nPPYOLOE-M [85] 23.4M 49.9G 640 123 48.9% / 48.6% 66.5% 53.0%\\nYOLOv5-N6 (r6.1) [23] 3.2M 18.4G 1280 123 - / 36.0% - -\\nYOLOv5-S6 (r6.1) [23] 12.6M 67.2G 1280 122 - / 44.8% - -\\nYOLOv5-M (r6.1) [23] 21.2M 49.0G 640 122 - / 45.4% - -\\nYOLOv7-X 71.3M 189.9G 640 114 53.1% / 52.9% 71.2% 57.8%\\nYOLOR-CSP [81] 52.9M 120.4G 640 106 51.1% / 50.8% 69.6% 55.7%\\nYOLOX-S [21] 9.0M 26.8G 640 102 40.5% / 40.5% - -\\nYOLOv5-L (r6.1) [23] 46.5M 109.1G 640 99 - / 49.0% - -\\nYOLOv5-M6 (r6.1) [23] 35.7M 200.0G 1280 90 - / 51.3% - -\\nYOLOR-CSP-X [81] 96.9M 226.8G 640 87 53.0% / 52.7% 71.4% 57.9%\\nYOLOv7-W6 70.4M 360.0G 1280 84 54.9% / 54.6% 72.6% 60.1%\\nYOLOv5-X (r6.1) [23] 86.7M 205.7G 640 83 - / 50.7% - -\\nYOLOX-M [21] 25.3M 73.8G 640 81 47.2% / 46.9% - -\\nPPYOLOE-L [85] 52.2M 110.1G 640 78 51.4% / 50.9% 68.9% 55.6%\\nYOLOR-P6 [81] 37.2M 325.6G 1280 76 53.9% / 53.5% 71.4% 58.9%\\nYOLOX-L [21] 54.2M 155.6G 640 69 50.1% / 49.7% - -\\nYOLOR-W6 [81] 79.8G 453.2G 1280 66 55.2% / 54.8% 72.7% 60.5%\\nYOLOv5-L6 (r6.1) [23] 76.8M 445.6G 1280 63 - / 53.7% - -\\nYOLOX-X [21] 99.1M 281.9G 640 58 51.5% / 51.1% - -\\nYOLOv7-E6 97.2M 515.2G 1280 56 56.0% / 55.9% 73.5% 61.2%\\nYOLOR-E6 [81] 115.8M 683.2G 1280 45 55.8% / 55.7% 73.4% 61.1%\\nPPYOLOE-X [85] 98.4M 206.6G 640 45 52.2% / 51.9% 69.9% 56.5%\\nYOLOv7-D6 154.7M 806.8G 1280 44 56.6% / 56.3% 74.0% 61.8%\\nYOLOv5-X6 (r6.1) [23] 140.7M 839.2G 1280 38 - / 55.0% - -\\nYOLOv7-E6E 151.7M 843.2G 1280 36 56.8% / 56.8% 74.4% 62.1%\\nYOLOR-D6 [81] 151.7M 935.6G 1280 34 56.5% / 56.1% 74.1% 61.9%\\nF-RCNN-R101-FPN+ [5] 60.0M 246.0G 1333 20 - / 44.0% - -\\nDeformable DETR [100] 40.0M 173.0G - 19 - / 46.2% - -\\nSwin-B (C-M-RCNN) [52] 145.0M 982.0G 1333 11.6 - / 51.9% - -\\nDETR DC5-R101 [5] 60.0M 253.0G 1333 10 - / 44.9% - -\\nEfﬁcientDet-D7x [74] 77.0M 410.0G 1536 6.5 55.1% / 54.4% 72.4% 58.4%\\nDual-Swin-T (C-M-RCNN) [47] 113.8M 836.0G 1333 6.5 - / 53.6% - -\\nViT-Adapter-B [7] 122.0M 997.0G - 4.4 - / 50.8% - -\\nDual-Swin-B (HTC) [47] 235.0M - 1600 2.5 58.7% / 58.4% - -\\nDual-Swin-L (HTC) [47] 453.0M - 1600 1.5 59.4% / 59.1% - -\\nModel #Param. FLOPs Size FPSA100 APtest / APval APtest\\n50 APtest\\n75\\nDN-Deformable-DETR [41] 48.0M 265.0G 1333 23.0 - / 48.6% - -\\nConvNeXt-B (C-M-RCNN) [53] - 964.0G 1280 11.5 - / 54.0% 73.1% 58.8%\\nSwin-B (C-M-RCNN) [52] - 982.0G 1280 10.7 - / 53.0% 71.8% 57.5%\\nDINO-5scale (R50) [89] 47.0M 860.0G 1333 10.0 - / 51.0% - -\\nConvNeXt-L (C-M-RCNN) [53] - 1354.0G 1280 10.0 - / 54.8% 73.8% 59.8%\\nSwin-L (C-M-RCNN) [52] - 1382.0G 1280 9.2 - / 53.9% 72.4% 58.8%\\nConvNeXt-XL (C-M-RCNN) [53] - 1898.0G 1280 8.6 - / 55.2% 74.2% 59.9%\\n8. More comparison\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS and\\nhas the highest accuracy 56.8% AP test-dev / 56.8% AP\\nmin-val among all known real-time object detectors with 30\\nFPS or higher on GPU V100. YOLOv7-E6 object detector\\n(56 FPS V100, 55.9% AP) outperforms both transformer-\\nbased detector SWIN-L Cascade-Mask R-CNN (9.2 FPS\\nA100, 53.9% AP) by 509% in speed and 2% in accuracy,\\nand convolutional-based detector ConvNeXt-XL Cascade-\\nMask R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed\\nand 0.7% AP in accuracy, as well as YOLOv7 outperforms:\\nYOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, De-\\nformable DETR, DINO-5scale-R50, ViT-Adapter-B and\\nmany other object detectors in speed and accuracy. More\\nover, we train YOLOv7 only on MS COCO dataset from\\nscratch without using any other datasets or pre-trained\\nweights.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/yolov7.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='Figure 9: Comparison with other object detectors.\\nFigure 10: Comparison with other real-time object detectors.\\nTable 10: Comparison of different setting.\\nModel Presicion IoU threshold AP val\\nYOLOv7-X FP16 (default) 0.65 (default) 52.9%\\nYOLOv7-X FP32 0.65 53.0%\\nYOLOv7-X FP16 0.70 53.0%\\nYOLOv7-X FP32 0.70 53.1%\\nimprovement - - +0.2%\\n* Similar to meituan/YOLOv6 and PPYOLOE, our model could\\nget higher AP when set higher IoU threshold.\\nThe maximum accuracy of the YOLOv7-E6E (56.8%\\nAP) real-time model is +13.7% AP higher than the cur-\\nrent most accurate meituan/YOLOv6-s model (43.1% AP)\\non COCO dataset. Our YOLOv7-tiny (35.2% AP, 0.4\\nms) model is +25% faster and +0.2% AP higher than\\nmeituan/YOLOv6-n (35.0% AP, 0.5 ms) under identical\\nconditions on COCO dataset and V100 GPU with batch=32.\\nFigure 11: Comparison with other real-time object detectors.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/yolov7.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='References\\n[1] anonymous. Designing network design strategies. anony-\\nmous submission, 2022. 3\\n[2] Irwan Bello, William Fedus, Xianzhi Du, Ekin Dogus\\nCubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon Shlens,\\nand Barret Zoph. Revisiting ResNets: Improved training\\nand scaling strategies.Advances in Neural Information Pro-\\ncessing Systems (NeurIPS), 34, 2021. 2\\n[3] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-\\nYuan Mark Liao. YOLOv4: Optimal speed and accuracy of\\nobject detection. arXiv preprint arXiv:2004.10934 , 2020.\\n2, 6, 7\\n[4] Yue Cao, Thomas Andrew Geddes, Jean Yee Hwa Yang,\\nand Pengyi Yang. Ensemble deep learning in bioinformat-\\nics. Nature Machine Intelligence, 2(9):500–508, 2020. 2\\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\\nEnd-to-end object detection with transformers. In Pro-\\nceedings of the European Conference on Computer Vision\\n(ECCV), pages 213–229, 2020. 10\\n[6] Kean Chen, Weiyao Lin, Jianguo Li, John See, Ji Wang, and\\nJunni Zou. AP-loss for accurate one-stage object detection.\\nIEEE Transactions on Pattern Analysis and Machine Intel-\\nligence (TPAMI), 43(11):3782–3798, 2020. 2\\n[7] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong\\nLu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for\\ndense predictions. arXiv preprint arXiv:2205.08534, 2022.\\n10\\n[8] Jiwoong Choi, Dayoung Chun, Hyun Kim, and Hyuk-Jae\\nLee. Gaussian YOLOv3: An accurate and fast object detec-\\ntor using localization uncertainty for autonomous driving.\\nIn Proceedings of the IEEE/CVF International Conference\\non Computer Vision (ICCV), pages 502–511, 2019. 5\\n[9] Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen,\\nMengchen Liu, Lu Yuan, and Lei Zhang. Dynamic head:\\nUnifying object detection heads with attentions. In Pro-\\nceedings of the IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition (CVPR), pages 7373–7382, 2021.\\n2\\n[10] Xiaohan Ding, Honghao Chen, Xiangyu Zhang, Kaiqi\\nHuang, Jungong Han, and Guiguang Ding. Re-\\nparameterizing your optimizers rather than architectures.\\narXiv preprint arXiv:2205.15242, 2022. 2\\n[11] Xiaohan Ding, Yuchen Guo, Guiguang Ding, and Jungong\\nHan. ACNet: Strengthening the kernel skeletons for pow-\\nerful CNN via asymmetric convolution blocks. In Proceed-\\nings of the IEEE/CVF International Conference on Com-\\nputer Vision (ICCV), pages 1911–1920, 2019. 2\\n[12] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and\\nGuiguang Ding. Diverse branch block: Building a con-\\nvolution as an inception-like unit. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 10886–10895, 2021. 2\\n[13] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong\\nHan, Guiguang Ding, and Jian Sun. RepVGG: Making\\nVGG-style convnets great again. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 13733–13742, 2021. 2, 4\\n[14] Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong\\nHan, Guiguang Ding, and Jian Sun. Scaling up your ker-\\nnels to 31x31: Revisiting large kernel design in CNNs. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition (CVPR), 2022. 2\\n[15] Piotr Doll ´ar, Mannat Singh, and Ross Girshick. Fast and\\naccurate model scaling. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR), pages 924–932, 2021. 2, 3\\n[16] Xianzhi Du, Barret Zoph, Wei-Chih Hung, and Tsung-Yi\\nLin. Simple training strategies and model scaling for object\\ndetection. arXiv preprint arXiv:2107.00057, 2021. 2\\n[17] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott,\\nand Weilin Huang. TOOD: Task-aligned one-stage object\\ndetection. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision (ICCV), pages 3490–3499,\\n2021. 2, 5\\n[18] Di Feng, Christian Haase-Sch ¨utz, Lars Rosenbaum, Heinz\\nHertlein, Claudius Glaeser, Fabian Timm, Werner Wies-\\nbeck, and Klaus Dietmayer. Deep multi-modal object de-\\ntection and semantic segmentation for autonomous driv-\\ning: Datasets, methods, and challenges. IEEE Transac-\\ntions on Intelligent Transportation Systems , 22(3):1341–\\n1360, 2020. 1\\n[19] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin,\\nDmitry P Vetrov, and Andrew G Wilson. Loss sur-\\nfaces, mode connectivity, and fast ensembling of DNNs.\\nAdvances in Neural Information Processing Systems\\n(NeurIPS), 31, 2018. 2\\n[20] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and\\nJian Sun. OTA: Optimal transport assignment for object\\ndetection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n303–312, 2021. 2, 5\\n[21] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\\nSun. YOLOX: Exceeding YOLO series in 2021. arXiv\\npreprint arXiv:2107.08430, 2021. 1, 2, 7, 10\\n[22] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. NAS-FPN:\\nLearning scalable feature pyramid architecture for object\\ndetection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n7036–7045, 2019. 2\\n[23] Jocher Glenn. YOLOv5 release v6.1. https://github.com/\\nultralytics/yolov5/releases/tag/v6.1, 2022. 2, 7, 10\\n[24] Shuxuan Guo, Jose M Alvarez, and Mathieu Salzmann. Ex-\\npandNets: Linear over-parameterization to train compact\\nconvolutional networks. Advances in Neural Information\\nProcessing Systems (NeurIPS), 33:1298–1310, 2020. 2\\n[25] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing\\nXu, and Chang Xu. GhostNet: More features from cheap\\noperations. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1580–1589, 2020. 1\\n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/yolov7.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='ings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR), pages 770–778, 2016. 1, 4, 5\\n[27] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\\nRuoming Pang, Vijay Vasudevan, et al. Searching for Mo-\\nbileNetV3. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1314–1324, 2019. 1\\n[28] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\\ndreetto, and Hartwig Adam. MobileNets: Efﬁcient con-\\nvolutional neural networks for mobile vision applications.\\narXiv preprint arXiv:1704.04861, 2017. 1\\n[29] Mu Hu, Junyi Feng, Jiashen Hua, Baisheng Lai, Jian-\\nqiang Huang, Xiaojin Gong, and Xiansheng Hua. On-\\nline convolutional re-parameterization. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), 2022. 2\\n[30] Miao Hu, Yali Li, Lu Fang, and Shengjin Wang. A 2-FPN:\\nAttention aggregation based feature pyramid network for\\ninstance segmentation. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition\\n(CVPR), pages 15343–15352, 2021. 2\\n[31] Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E\\nHopcroft, and Kilian Q Weinberger. Snapshot ensembles:\\nTrain 1, get m for free. International Conference on Learn-\\ning Representations (ICLR), 2017. 2\\n[32] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\\nian Q Weinberger. Densely connected convolutional net-\\nworks. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n4700–4708, 2017. 2, 4, 5\\n[33] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov,\\nDmitry Vetrov, and Andrew Gordon Wilson. Averaging\\nweights leads to wider optima and better generalization. In\\nConference on Uncertainty in Artiﬁcial Intelligence (UAI),\\n2018. 2\\n[34] Paul F Jaeger, Simon AA Kohl, Sebastian Bickel-\\nhaupt, Fabian Isensee, Tristan Anselm Kuder, Heinz-Peter\\nSchlemmer, and Klaus H Maier-Hein. Retina U-Net: Em-\\nbarrassingly simple exploitation of segmentation supervi-\\nsion for medical object detection. In Machine Learning for\\nHealth Workshop, pages 171–183, 2020. 1\\n[35] Hakan Karaoguz and Patric Jensfelt. Object detection ap-\\nproach for robot grasp detection. In IEEE International\\nConference on Robotics and Automation (ICRA) , pages\\n4953–4959, 2019. 1\\n[36] Kang Kim and Hee Seok Lee. Probabilistic anchor as-\\nsignment with iou prediction for object detection. In Pro-\\nceedings of the European conference on computer vision\\n(ECCV), pages 355–371, 2020. 5\\n[37] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr\\nDoll´ar. Panoptic feature pyramid networks. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR), pages 6399–6408, 2019. 2\\n[38] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou\\nZhang, and Zhuowen Tu. Deeply-supervised nets. In Arti-\\nﬁcial Intelligence and Statistics, pages 562–570, 2015. 5\\n[39] Youngwan Lee, Joong-won Hwang, Sangrok Lee, Yuseok\\nBae, and Jongyoul Park. An energy and GPU-computation\\nefﬁcient backbone network for real-time object detection.\\nIn Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition Workshops (CVPRW),\\npages 0–0, 2019. 2, 3\\n[40] Buyu Li, Wanli Ouyang, Lu Sheng, Xingyu Zeng, and\\nXiaogang Wang. GS3D: An efﬁcient 3d object detection\\nframework for autonomous driving. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 1019–1028, 2019. 1\\n[41] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M\\nNi, and Lei Zhang. DN-DETR: Accelerate detr training\\nby introducing query denoising. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 13619–13627, 2022. 10\\n[42] Shuai Li, Chenhang He, Ruihuang Li, and Lei Zhang. A\\ndual weighting label assignment scheme for object detec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR), pages 9387–\\n9396, 2022. 2, 5\\n[43] Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang,\\nand Jian Yang. Generalized focal loss v2: Learning reliable\\nlocalization quality estimation for dense object detection. In\\nProceedings of the IEEE/CVF Conference on Computer Vi-\\nsion and Pattern Recognition (CVPR), pages 11632–11641,\\n2021. 5\\n[44] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin\\nHu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal\\nloss: Learning qualiﬁed and distributed bounding boxes for\\ndense object detection. Advances in Neural Information\\nProcessing Systems (NeurIPS), 33:21002–21012, 2020. 5\\n[45] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\\nExploring plain vision transformer backbones for object de-\\ntection. arXiv preprint arXiv:2203.16527, 2022. 2\\n[46] Zhuoling Li, Minghui Dong, Shiping Wen, Xiang Hu, Pan\\nZhou, and Zhigang Zeng. CLU-CNNs: Object detection for\\nmedical images. Neurocomputing, 350:53–59, 2019. 1\\n[47] Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang,\\nZhi Tang, Wei Chu, Jingdong Chen, and Haibin Ling. CB-\\nNetV2: A composite backbone network architecture for ob-\\nject detection. arXiv preprint arXiv:2107.00420, 2021. 5,\\n10\\n[48] Ji Lin, Wei-Ming Chen, Han Cai, Chuang Gan, and Song\\nHan. Memory-efﬁcient patch-based inference for tiny deep\\nlearning. Advances in Neural Information Processing Sys-\\ntems (NeurIPS), 34:2346–2358, 2021. 1\\n[49] Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song\\nHan, et al. MCUNet: Tiny deep learning on IoT de-\\nvices. Advances in Neural Information Processing Systems\\n(NeurIPS), 33:11711–11722, 2020. 1\\n[50] Yuxuan Liu, Lujia Wang, and Ming Liu. YOLOStereo3D:\\nA step back to 2D for efﬁcient stereo 3D detection. In\\nIEEE International Conference on Robotics and Automa-\\ntion (ICRA), pages 13018–13024, 2021. 5\\n[51] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong,\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/yolov7.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='et al. Swin transformer v2: Scaling up capacity and res-\\nolution. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR), 2022. 2\\n[52] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\\nHierarchical vision transformer using shifted windows. In\\nProceedings of the IEEE/CVF International Conference on\\nComputer Vision (ICCV), pages 10012–10022, 2021. 10\\n[53] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\\nenhofer, Trevor Darrell, and Saining Xie. A ConvNet for\\nthe 2020s. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n11976–11986, 2022. 10\\n[54] Rangi Lyu. NanoDet-Plus. https://github.com/RangiLyu/\\nnanodet/releases/tag/v1.0.0-alpha-1, 2021. 1, 2\\n[55] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian\\nSun. ShufﬂeNet V2: Practical guidelines for efﬁcient CNN\\narchitecture design. In Proceedings of the European Con-\\nference on Computer Vision (ECCV), pages 116–131, 2018.\\n1, 3\\n[56] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. A ranking-based, balanced loss function unifying\\nclassiﬁcation and localisation in object detection. Advances\\nin Neural Information Processing Systems (NeurIPS) ,\\n33:15534–15545, 2020. 2\\n[57] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. Rank & sort loss for object detection and in-\\nstance segmentation. In Proceedings of the IEEE/CVF In-\\nternational Conference on Computer Vision (ICCV), pages\\n3009–3018, 2021. 2\\n[58] Shuvo Kumar Paul, Muhammed Tawﬁq Chowdhury,\\nMircea Nicolescu, Monica Nicolescu, and David Feil-\\nSeifer. Object detection and pose estimation from rgb and\\ndepth data for real-time, adaptive robotic grasping. In Ad-\\nvances in Computer Vision and Computational Biology ,\\npages 121–142. 2021. 1\\n[59] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. De-\\ntectoRS: Detecting objects with recursive feature pyramid\\nand switchable atrous convolution. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 10213–10224, 2021. 2\\n[60] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\\nKaiming He, and Piotr Doll ´ar. Designing network design\\nspaces. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n10428–10436, 2020. 2\\n[61] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\\nFarhadi. You only look once: Uniﬁed, real-time object de-\\ntection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n779–788, 2016. 2, 5\\n[62] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster,\\nstronger. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n7263–7271, 2017. 2\\n[63] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental\\nimprovement. arXiv preprint arXiv:1804.02767, 2018. 1, 2\\n[64] Hamid Rezatoﬁghi, Nathan Tsoi, JunYoung Gwak, Amir\\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\\ntersection over union: A metric and a loss for bounding\\nbox regression. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR),\\npages 658–666, 2019. 2\\n[65] Byungseok Roh, JaeWoong Shin, Wuhyun Shin, and\\nSaehoon Kim. Sparse DETR: Efﬁcient end-to-end ob-\\nject detection with learnable sparsity. arXiv preprint\\narXiv:2111.14330, 2021. 5\\n[66] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey\\nZhmoginov, and Liang-Chieh Chen. MobileNetV2: In-\\nverted residuals and linear bottlenecks. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 4510–4520, 2018. 1\\n[67] Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang,\\nYurong Chen, and Xiangyang Xue. Object detection\\nfrom scratch with deep supervision. IEEE Transactions\\non Pattern Analysis and Machine Intelligence (TPAMI) ,\\n42(2):398–412, 2019. 5\\n[68] Karen Simonyan and Andrew Zisserman. Very deep convo-\\nlutional networks for large-scale image recognition. arXiv\\npreprint arXiv:1409.1556, 2014. 4\\n[69] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng\\nXu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan,\\nChanghu Wang, et al. Sparse R-CNN: End-to-end ob-\\nject detection with learnable proposals. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 14454–14463, 2021. 2\\n[70] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich. Going deeper with\\nconvolutions. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR),\\npages 1–9, 2015. 5\\n[71] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\\nShlens, and Zbigniew Wojna. Rethinking the inception\\narchitecture for computer vision. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 2818–2826, 2016. 2\\n[72] Mingxing Tan and Quoc Le. EfﬁcientNet: Rethinking\\nmodel scaling for convolutional neural networks. In Inter-\\nnational Conference on Machine Learning (ICML) , pages\\n6105–6114, 2019. 2, 3\\n[73] Mingxing Tan and Quoc Le. EfﬁcientNetv2: Smaller mod-\\nels and faster training. In International Conference on Ma-\\nchine Learning (ICML), pages 10096–10106, 2021. 2\\n[74] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efﬁcient-\\nDet: Scalable and efﬁcient object detection. In Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pat-\\ntern Recognition (CVPR), pages 10781–10790, 2020. 2, 10\\n[75] Antti Tarvainen and Harri Valpola. Mean teachers are better\\nrole models: Weight-averaged consistency targets improve\\nsemi-supervised deep learning results. Advances in Neural\\nInformation Processing Systems (NeurIPS), 30, 2017. 2, 6\\n[76] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nFully convolutional one-stage object detection. InProceed-\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-07-07T00:21:22+00:00', 'author': '', 'keywords': '', 'moddate': '2022-07-07T00:21:22+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': './data/yolov7.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='ings of the IEEE/CVF International Conference on Com-\\nputer Vision (ICCV), pages 9627–9636, 2019. 2\\n[77] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS:\\nA simple and strong anchor-free object detector. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence\\n(TPAMI), 44(4):1922–1933, 2022. 2\\n[78] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff\\nZhu, Oncel Tuzel, and Anurag Ranjan. An im-\\nproved one millisecond mobile backbone. arXiv preprint\\narXiv:2206.04040, 2022. 2\\n[79] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-\\nYuan Mark Liao. Scaled-YOLOv4: Scaling cross stage\\npartial network. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR),\\npages 13029–13038, 2021. 2, 3, 6, 7\\n[80] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,\\nPing-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. CSP-\\nNet: A new backbone that can enhance learning capabil-\\nity of CNN. In Proceedings of the IEEE/CVF Conference\\non Computer Vision and Pattern Recognition Workshops\\n(CVPRW), pages 390–391, 2020. 1\\n[81] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao.\\nYou only learn one representation: Uniﬁed network for\\nmultiple tasks. arXiv preprint arXiv:2105.04206, 2021. 1,\\n2, 6, 7, 10\\n[82] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian\\nSun, and Nanning Zheng. End-to-end object detection\\nwith fully convolutional network. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 15849–15858, 2021. 2, 5\\n[83] Bichen Wu, Chaojian Li, Hang Zhang, Xiaoliang Dai,\\nPeizhao Zhang, Matthew Yu, Jialiang Wang, Yingyan Lin,\\nand Peter Vajda. FBNetv5: Neural architecture search for\\nmultiple tasks in one run.arXiv preprint arXiv:2111.10007,\\n2021. 1\\n[84] Yunyang Xiong, Hanxiao Liu, Suyog Gupta, Berkin Akin,\\nGabriel Bender, Yongzhe Wang, Pieter-Jan Kindermans,\\nMingxing Tan, Vikas Singh, and Bo Chen. MobileDets:\\nSearching for object detection architectures for mobile ac-\\ncelerators. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n3825–3834, 2021. 1\\n[85] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao\\nChang, Cheng Cui, Kaipeng Deng, Guanzhong Wang,\\nQingqing Dang, Shengyu Wei, Yuning Du, et al. PP-\\nYOLOE: An evolved version of YOLO. arXiv preprint\\narXiv:2203.16250, 2022. 2, 7, 8, 10\\n[86] Zetong Yang, Yin Zhou, Zhifeng Chen, and Jiquan Ngiam.\\n3D-MAN: 3D multi-frame attention network for object de-\\ntection. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n1863–1872, 2021. 5\\n[87] Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor\\nDarrell. Deep layer aggregation. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 2403–2412, 2018. 1\\n[88] Guanghua Yu, Qinyao Chang, Wenyu Lv, Chang Xu, Cheng\\nCui, Wei Ji, Qingqing Dang, Kaipeng Deng, Guanzhong\\nWang, Yuning Du, et al. PP-PicoDet: A better real-\\ntime object detector on mobile devices. arXiv preprint\\narXiv:2111.00902, 2021. 1\\n[89] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun\\nZhu, Lionel M Ni, and Heung-Yeung Shum. DINO: DETR\\nwith improved denoising anchor boxes for end-to-end ob-\\nject detection. arXiv preprint arXiv:2203.03605, 2022. 10\\n[90] Haoyang Zhang, Ying Wang, Feras Dayoub, and Niko Sun-\\nderhauf. VarifocalNet: An IoU-aware dense object detector.\\nIn Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR), pages 8514–8523,\\n2021. 5\\n[91] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and\\nStan Z Li. Bridging the gap between anchor-based and\\nanchor-free detection via adaptive training sample selec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR), pages 9759–\\n9768, 2020. 5\\n[92] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian\\nSun. ShufﬂeNet: An extremely efﬁcient convolutional neu-\\nral network for mobile devices. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 6848–6856, 2018. 1\\n[93] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Zehuan\\nYuan, Ping Luo, Wenyu Liu, and Xinggang Wang. BYTE-\\nTrack: Multi-object tracking by associating every detection\\nbox. arXiv preprint arXiv:2110.06864, 2021. 1\\n[94] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,\\nand Wenyu Liu. FAIRMOT: On the fairness of detec-\\ntion and re-identiﬁcation in multiple object tracking. Inter-\\nnational Journal of Computer Vision, 129(11):3069–3087,\\n2021. 1\\n[95] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang\\nYe, and Dongwei Ren. Distance-IoU loss: Faster and bet-\\nter learning for bounding box regression. In Proceedings\\nof the AAAI Conference on Artiﬁcial Intelligence (AAAI) ,\\nvolume 34, pages 12993–13000, 2020. 2\\n[96] Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, Junbo\\nYin, Yuchao Dai, and Ruigang Yang. IoU loss for 2D/3D\\nobject detection. In International Conference on 3D Vision\\n(3DV), pages 85–94, 2019. 2\\n[97] Xingyi Zhou, Dequan Wang, and Philipp Kr ¨ahenb¨uhl. Ob-\\njects as points. arXiv preprint arXiv:1904.07850, 2019. 1,\\n2\\n[98] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima\\nTajbakhsh, and Jianming Liang. UNet++: A nested U-\\nNet architecture for medical image segmentation. In\\nDeep Learning in Medical Image Analysis and Multimodal\\nLearning for Clinical Decision Support, 2018. 5\\n[99] Benjin Zhu, Jianfeng Wang, Zhengkai Jiang, Fuhang Zong,\\nSongtao Liu, Zeming Li, and Jian Sun. AutoAssign: Differ-\\nentiable label assignment for dense object detection. arXiv\\npreprint arXiv:2007.03496, 2020. 2, 5\\n[100] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\\nand Jifeng Dai. Deformable DETR: Deformable trans-\\nformers for end-to-end object detection. In Proceedings of\\nthe International Conference on Learning Representations\\n(ICLR), 2021. 10\\n15')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the PDF documents\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('./data/yolov7.pdf')\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e7bcc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\n\\nExamples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\n\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\n\\nIllustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\n\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\n\\nExperiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\n\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\n\\nAfter fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\n\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\n\\nIllustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\n\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\n\\nComparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\n\\nComparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\n\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\n\\nA picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\n\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\n\\nIllustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\n\\nPseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\n\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\\n\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4 # Beautiful Soup is to retrieve data from html attribute\n",
    "\n",
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                       bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                           class_=(\"post-title\", \"post-content\", \"post-header\")\n",
    "                       )))\n",
    "\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'Published': '2024-07-22', 'Title': \"Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\", 'Authors': 'Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini', 'Summary': 'The inference demand for LLMs has skyrocketed in recent months, and serving\\nmodels with low latencies remains challenging due to the quadratic input length\\ncomplexity of the attention layers. In this work, we investigate the effect of\\ndropping MLP and attention layers at inference time on the performance of\\nLlama-v2 models. We find that dropping dreeper attention layers only marginally\\ndecreases performance but leads to the best speedups alongside dropping entire\\nlayers. For example, removing 33\\\\% of attention layers in a 13B Llama2 model\\nresults in a 1.8\\\\% drop in average performance over the OpenLLM benchmark. We\\nalso observe that skipping layers except the latter layers reduces performances\\nfor more layers skipped, except for skipping the attention layers.'}, page_content='Attention Is All You Need But You Don’t Need All Of It\\nFor Inference of Large Language Models\\nGeorgy Tyukin * 1 Gbetondji J-S Dovonon 1 Jean Kaddour 1 Pasquale Minervini 2\\nAbstract\\nThe inference demand for LLMs has skyrocketed\\nin recent months, and serving models with low\\nlatencies remains challenging due to the quadratic\\ninput length complexity of the attention layers.\\nIn this work, we investigate the effect of drop-\\nping MLP and attention layers at inference time\\non the performance of Llama-v2 models. We\\nfind that dropping dreeper attention layers only\\nmarginally decreases performance but leads to the\\nbest speedups alongside dropping entire layers.\\nFor example, removing 33% of attention layers\\nin a 13B Llama2 model results in a 1.8% drop in\\naverage performance over the OpenLLM bench-\\nmark. We also observe that skipping layers except\\nthe latter layers reduces performances for more\\nlayers skipped, except for skipping the attention\\nlayers.\\n1. Introduction\\nThe ubiquitous deployment of Large Language Models\\n(LLMs) results in ever-growing amounts of compute spent\\non inference (Patterson et al., 2021; Chen et al., 2023; Kad-\\ndour et al., 2023a; Xia et al., 2024; Reid et al., 2024). Fur-\\nther, serving models with low latencies remains challenging\\nbecause contemporary Transformer architectures employ\\nthe self-attention mechanism with quadratic input complex-\\nity (Touvron et al., 2023b; Jiang et al., 2023; Bi et al., 2024).\\nIn this work, we delve deeper into the concept of layer\\nskipping (Fan et al., 2019; Wang et al., 2022a) to reduce\\nthe computation on superfluous LLM components. Our\\nfindings demonstrate that pruning deeper attention layers\\ndoes not significantly affect performance. When applied\\nto Llama-v2 (Touvron et al., 2023b), we maintain good\\nperformance on the OpenLLM (ARC (Clark et al., 2018),\\n*Equal contribution\\n1University College London,\\nUK\\n2University of Edinburgh, UK. Correspondence to: Georgy Tyukin\\n<tyukinegor@gmail.com>.\\nWork presented at TF2M workshop at ICML 2024, Vienna, Austria.\\nPMLR 235, 2024. Copyright 2024 by the author(s).\\nHellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al.,\\n2021), TruthfulQA (Lin et al., 2022)) benchmarks (Beech-\\ning et al., 2023), recording only minimal performance devi-\\nations compared to the full model.\\n2. Method\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\nLayer\\n0.65\\n0.70\\n0.75\\n0.80\\n0.85\\n0.90\\n0.95\\n1.00\\nCosine Similarity\\nCosine Similarity with previous layer for LLaMA-v2 7b and LLaMA-v2 13\\nLLaMA-v2 7b\\nLLaMA-v2 13b\\nFigure 1. Cosine similarity of Llama-v2 layers with the previous\\nlayer: We observe that the deeper the layer, the more its features\\nare similar to the previous layer except for the very last layer.\\n2.1. Layer skipping\\nConsider a Transformer model M with L layers, each\\nconsisting of an attention sub-layer followed by a multi-\\nlayer perceptron (MLP) sub-layer. We denote each layer as\\nMi = (Attentioni, MLPi) for i ∈{1, 2, . . . , L}.\\nTo compare the performance of Transformer models when\\nskipping specific sub-layers, we create two variants of the\\nmodel:\\n1. Skipping MLP Layers: We construct a model Mskip MLP\\n1\\narXiv:2407.15516v1  [cs.LG]  22 Jul 2024\\nAttention Is All You Need But You Don’t Need All Of It\\nby skipping the MLP sub-layer from the last k layers. The\\nresulting model is Mskip MLP = {(Attentioni, MLPi) | i ∈\\n{1, 2, . . . , L −k}} ∪{(Attentioni, ∅) | i ∈{L −k +\\n1, . . . , L}}.\\n2. Skipping Attention Layers: We construct a model\\nMskip Attention by skipping the attention sub-layer from the\\nlast k layers.\\nThe resulting model is Mskip Attention =\\n{(Attentioni, MLPi)\\n|\\ni\\n∈\\n{1, 2, . . . , L −k}} ∪\\n{(∅, MLPi) | i ∈{L −k + 1, . . . , L}}.\\n3. Skipping Transformer Blocks: We construct a model\\nMskip Attention by skipping the entire last k layers. The re-\\nsulting model is Mskip Block = {(Attentioni, MLPi) | i ∈\\n{1, 2, . . . , L −k}} ∪{(∅) | i ∈{L −k + 1, . . . , L}}.\\nWe then evaluate the performance of these modified models\\non the OpenLLM benchmark (Beeching et al., 2023), com-\\nparing metrics such as accuracy, computational efficiency,\\nand memory usage. This comparison helps in understand-\\ning the individual contributions of the attention and MLP\\nsub-layers to the overall performance of the Transformer\\nmodel.\\n(a) Skip attention lay-\\ners.\\n(b) Skip attention lay-\\ners,\\nkeep last full\\nblock.\\n(c) Skip ffwd layers.\\n(d) Skip ffwd layers,\\nkeep last full block.\\n(e) Skip full blocks.\\n(f) Skip full blocks,\\nkeep last full block.\\nFigure 2. Skip mechanisms for skipping single layers and entire\\nTransformer blocks (ffwd and attention layers) during inference.\\n2.2. Motivation: Are Deeper Layers More Redundant?\\nIn Transformer models, the last layers have been shown to\\ncontribute less information than earlier layers, making it\\npossible to drop those layers at a minimal performance cost\\n(Fan et al., 2019; Zhang & He, 2020; Wang et al., 2022a;\\nSchuster et al., 2022; Kaddour et al., 2023b; Belrose et al.,\\n2023).\\nTo verify this, we experiment with removing either the at-\\ntention sublayers or the MLP sublayers. Figure 1 shows the\\ncosine similarities between a layer’s features and the previ-\\nous layer showing that deeper layers have a lower impact\\non the features than earlier layers. One notable exception\\nto this trend is that the last layer for both Llama-v2 7B and\\n13B has the lowest cosine similarity with the previous layer.\\nPrevious analysis of the attention mechanism has shown\\nthat they can converge to the same value due to attention\\ncollapse (Zhai et al., 2023) and token features that also con-\\nverge to the same value due to over-smoothing (Wang et al.,\\n2022b; Dovonon et al., 2024) or rank collapse (Dong et al.,\\n2023), with solutions to these issues typically improving\\nperformance (Ali et al., 2023; Choi et al., 2024).\\n3. Results\\nExperimental Setup\\nFor all experiments, we use either\\nLlama-v2-7B or Llama-v2-13B (Touvron et al., 2023a;b),\\ntwo LLMs trained on trillions of publically available tokens.\\nWe experiment with keeping 66%, 75%, 90% and 100% of\\nthe network and report the corresponding results in Table 1.\\nWe also experiment with removing attention sublayers in\\nTable 2, MLP sublayers in Table 3, and a varying number of\\nlayers similar to Table 1 but keeping the last layer in Table 4.\\n3.1. Chopping Layers\\nTable 1. Llama-v2 skipping full layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n35.2\\n46.8\\n46.2\\n40.3\\n42.1\\n7B-75%\\n38.3\\n53.0\\n45.1\\n45.9\\n45.6\\n7B-90%\\n47.7\\n69.3\\n39.6\\n46.4\\n50.8\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n37.8\\n46.8\\n45.3\\n51.8\\n45.4\\n13B-75%\\n40.9\\n53.6\\n42.5\\n53.2\\n47.6\\n13B-90%\\n51.3\\n71.3\\n37.1\\n54.8\\n53.6\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nOn all datasets except TruthfulQA, performance drops\\nwhich is expected. It had already been observed that larger\\nlanguage models are less truthful (Lin et al., 2022), but we\\nnow also observe that reducing the size of already trained\\nmodels can also make them more truthful. The observa-\\ntion still holds when the last layer is preserved. Skipping\\n2\\nAttention Is All You Need But You Don’t Need All Of It\\nTable 2. Llama-v2 skipping attention sublayers\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n51.2\\n77.0\\n42.2\\n39.4\\n52.5\\n7B-75%\\n52.5\\n78.3\\n42.3\\n41.4\\n53.6\\n7B-90%\\n52.8\\n78.9\\n40.0\\n44.0\\n53.9\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n55.6\\n80.1\\n40.1\\n51.3\\n56.8\\n13B-75%\\n55.9\\n79.7\\n39.9\\n52.1\\n56.9\\n13B-90%\\n57.0\\n81.3\\n38.2\\n54.8\\n57.8\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nTable 3. Llama-v2 skipping ffwd sublayers\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n35.1\\n52.5\\n42.2\\n43.9\\n43.4\\n7B-75%\\n40.4\\n60.3\\n39.2\\n46.3\\n46.6\\n7B-90%\\n48.5\\n71.4\\n38.0\\n46.1\\n51.0\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n41.6\\n56.9\\n40.7\\n53.4\\n48.2\\n13B-75%\\n47.3\\n65.2\\n40.0\\n53.2\\n51.4\\n13B-90%\\n54.2\\n75.8\\n38.3\\n54.7\\n55.8\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nattention layers only leads to better results with only a 1.8%\\ndecrease in performance when keeping 66% of the network\\ncompared to a 13.1% decrease in performance when drop-\\nping dropping the MLP layers only. This seems to indicate\\nthat MLP layers are more important than attention layers, at\\nleast in deeper parts of the network.\\n3.2. Last Layer Inclusion\\nTable 4. Llama-v2 skip full layers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n32.0\\n45.8\\n46.9\\n40.7\\n41.3\\n7B-75%\\n34.5\\n49.4\\n45.9\\n38.3\\n42.0\\n7B-90%\\n46.5\\n73.1\\n41.8\\n41.4\\n50.7\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n35.1\\n50.0\\n46.9\\n19.1\\n37.8\\n13B-75%\\n38.7\\n56.6\\n43.7\\n25.2\\n41.1\\n13B-90%\\n51.2\\n78.1\\n38.0\\n27.1\\n47.9\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nSurprisingly, we notice that skipping layers except the lat-\\nter layers reduces performances for more layers skipped,\\nexcept for skipping the attention layers. This is even more\\nexaggerated compared to just dropping layers, including the\\nlast one. The reason for this could be attributed to the (lack\\nof) robustness of feedforward sublayers, as the last layer\\nnow has to process perturbed information from earlier lay-\\ners. For future work, it would be interesting to see if these\\nperformance drops can be compensated by a small amount\\nTable 5. Llama-v2 skip attention sublayers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n49.3\\n77.1\\n40.5\\n42.5\\n52.4\\n7B-75%\\n51.8\\n78.3\\n41.1\\n44.1\\n53.8\\n7B-90%\\n51.9\\n78.7\\n39.4\\n45.7\\n53.9\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n56.8\\n82.1\\n38.0\\n50.3\\n56.8\\n13B-75%\\n57.5\\n82.1\\n37.0\\n51.4\\n57.0\\n13B-90%\\n58.9\\n82.4\\n36.6\\n54.5\\n58.1\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nTable 6. Llama-v2 skip ffwd sublayers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n32.0\\n45.8\\n46.9\\n39.4\\n41.0\\n7B-75%\\n34.5\\n49.4\\n45.9\\n40.2\\n42.5\\n7B-90%\\n46.5\\n73.1\\n41.8\\n40.2\\n50.4\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n35.1\\n50.0\\n46.9\\n20.4\\n38.1\\n13B-75%\\n38.7\\n56.6\\n43.7\\n33.6\\n43.2\\n13B-90%\\n51.2\\n78.1\\n38.0\\n34.4\\n50.4\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nof continued training; since model growing techniques for\\ntraining seem to not suffer from instabilities (Kaddour et al.,\\n2023b).\\n3.3. Compute-matched Comparison\\nTo measure the efficiency of the networks we conducted\\na separate experiment, where we record the time it takes\\nfor the model to output a sequence of length 1, averaging\\nover 1000 sequences. We conducted this experiment for\\nboth 50 and 100 length input sequences. We notice that full\\nlayer droppings do improve time costs the best, followed by\\nattention sublayers, and then feedforward sublayers which\\ndo not impact the speed of processing a lot.\\nWe report the time×102 (for clarity) it takes to predict 1\\ntoken for 1000 sequences as well as the percentage improve-\\nment. We show the results of this experiment for Llama 2\\n7B with 0%, 10%, 25%, 33% of layers skipped and we label\\nthese as 7B-100%, 7B-90%, 7B-75%, 7B-66% respectively.\\nTable 7. Llama-v2 time results, 50 length sequence, no last layer\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) ×102\\n(%)\\nTime(s) ×102\\n(%)\\nTime(s) ×102\\n(%)\\n7B-66%\\n31.35\\n32.96\\n36.72\\n21.47\\n43.51\\n6.95\\n7B-75%\\n35.48\\n24.12\\n39.46\\n15.61\\n42.88\\n8.30\\n7B-90%\\n43.31\\n7.38\\n42.93\\n8.19\\n44.17\\n5.53\\n7B-100%\\n46.76\\n0\\n-\\n-\\n-\\n-\\n3\\nAttention Is All You Need But You Don’t Need All Of It\\nTable 8. Llama-v2 time results, 50 length sequence, last layer in-\\ncluded\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) ×102\\n(%)\\nTime(s) ×102\\n(%)\\nTime(s) ×102\\n(%)\\n7B-66%\\n31.78\\n32.04\\n36.92\\n21.04\\n41.31\\n11.66\\n7B-75%\\n34.98\\n25.19\\n40.24\\n13.94\\n42.62\\n8.85\\n7B-90%\\n40.92\\n12.49\\n42.43\\n9.26\\n43.51\\n6.95\\n7B-100%\\n46.76\\n0\\n-\\n-\\n-\\n-\\nTable 9. Llama-v2 time results, 100 length sequence, no last layer\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) ×102\\n(%)\\nTime(s) ×102\\n(%)\\nTime(s) ×102\\n(%)\\n7B-66%\\n32.36\\n32.58\\n38.97\\n18.18\\n43.08\\n10.25\\n7B-75%\\n36.58\\n23.79\\n41.27\\n14.02\\n44.13\\n8.06\\n7B-90%\\n43.65\\n9.06\\n44.62\\n7.04\\n46.30\\n3.54\\n7B-100%\\n48.00\\n0\\n-\\n-\\n-\\n-\\nTable 10. Llama-v2 time results, 100 length sequence, last layer\\nincluded\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) ×102\\n(%)\\nTime(s) ×102\\n(%)\\nTime(s) ×102\\n(%)\\n7B-66%\\n32.05\\n33.23\\n38.52\\n19.75\\n42.66\\n11.13\\n7B-75%\\n36.41\\n24.15\\n41.00\\n14.58\\n43.92\\n8.50\\n7B-90%\\n43.28\\n9.83\\n44.27\\n7.77\\n45.20\\n5.83\\n7B-100%\\n48.00\\n0\\n-\\n-\\n-\\n-\\n4. Related Work\\nEarly Exit during inference\\nEarly exit methods have also\\nbeen proposed in other domains (Graves, 2017; Teerapit-\\ntayanon et al., 2017) before getting adapted to autoregressive\\nmodels (Elbayad et al., 2020; Schuster et al., 2022; Din et al.,\\n2023; Elhoushi et al., 2024; Fan et al., 2024; Chen et al.,\\n2024). The idea works by dynamically allocating compute\\nbased on the difficulty of the input sequence. Our method\\nprunes the deepest layers and does not involve any level of\\nadaptability. This is beneficial because it does not require\\nthe entire model to be loaded in memory. Dropping layers\\nduring inference has been done on BERT-like models in\\n(Wang et al., 2022a; Sajjad et al., 2023). We apply a similar\\nanalysis to more recent LLMs and study the impact of skip-\\nping attention and/or MLP layers in more detail. Concurrent\\nwork to ours by Gromov et al. (2024) yields similar results\\nby pruning deeper layers and applying fine-tuning on the\\npruned model.\\nLayer dropping/growing during training\\nThere are var-\\nious works studying the dropping/growing layers dynami-\\ncally during training (Fan et al., 2019; Gong et al., 2019;\\nKaddour et al., 2023b; Jiang et al., 2020; Liu et al., 2023). In\\ncontrast, this work focuses on dropping layers of an already\\npre-trained model in a way similar to Men et al. (2024).\\nOther Inference Speedup Methods\\nOther works to speed\\nup inference include compressing KV caches (Nawrot et al.,\\n2024; Wu & Tu, 2024; Bi et al., 2024), speculative decoding\\n(Chen et al., 2023), efficient memory management (Kwon\\net al., 2023), or subqudratic attention architectures (Fu et al.,\\n2022; Peng et al., 2023; Gu & Dao, 2023), an overview has\\nbeen provided by Kaddour et al. (2023a).\\n5. Conclusion\\nWe investigated the effect of dropping the last layers from\\nthe 7B and 13B Llama2 models. We observe that dropping\\nattention sublayers lead to much lower drops in performance\\nthan dropping the MLP sublayers, whether the last layer\\nis included or not, while also leading to better inference\\nspeedups. For example, removing 33% of attention layers\\nleads to an 18% speedup in a 13B Llama2 model at the cost\\nof a 1.8% drop in average performance. This shows that\\nmassive improvements can be made over dropping entire\\nlayers from just dropping the attention sublayer.\\nReferences\\nAli, A., Galanti, T., and Wolf, L. Centered self-attention\\nlayers, 2023.\\nBeeching,\\nE.,\\nFourrier,\\nC.,\\nHabib,\\nN.,\\nHan,\\nS.,\\nLambert,\\nN.,\\nRajani,\\nN.,\\nSanseviero,\\nO.,\\nTun-\\nstall,\\nL.,\\nand\\nWolf,\\nT.\\nOpen\\nllm\\nleader-\\nboard.\\nhttps://huggingface.co/spaces/\\nHuggingFaceH4/open_llm_leaderboard,\\n2023.\\nBelrose, N., Furman, Z., Smith, L., Halawi, D., Ostrovsky, I.,\\nMcKinney, L., Biderman, S., and Steinhardt, J. Eliciting\\nlatent predictions from transformers with the tuned lens.\\narXiv preprint arXiv:2303.08112, 2023.\\nBi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C.,\\nDing, H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm:\\nScaling open-source language models with longtermism.\\narXiv preprint arXiv:2401.02954, 2024.\\nChen, C., Borgeaud, S., Irving, G., Lespiau, J., Sifre, L., and\\nJumper, J. Accelerating large language model decoding\\nwith speculative sampling. CoRR, abs/2302.01318, 2023.\\ndoi: 10.48550/ARXIV.2302.01318. URL https://\\ndoi.org/10.48550/arXiv.2302.01318.\\nChen, Y., Pan, X., Li, Y., Ding, B., and Zhou, J. Ee-llm:\\nLarge-scale training and inference of early-exit large lan-\\nguage models with 3d parallelism, 2024.\\nChoi, J., Wi, H., Kim, J., Shin, Y., Lee, K., Trask, N., and\\nPark, N. Graph convolutions enrich the self-attention in\\ntransformers!, 2024.\\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\\nSchoenick, C., and Tafjord, O. Think you have solved\\n4\\nAttention Is All You Need But You Don’t Need All Of It\\nquestion answering? try arc, the ai2 reasoning challenge,\\n2018.\\nDin, A. Y., Karidi, T., Choshen, L., and Geva, M. Jump\\nto conclusions: Short-cutting transformers with linear\\ntransformations. arXiv preprint arXiv:2303.09435, 2023.\\nDong, Y., Cordonnier, J.-B., and Loukas, A. Attention\\nis not all you need: Pure attention loses rank doubly\\nexponentially with depth, 2023.\\nDovonon, G. J.-S., Bronstein, M. M., and Kusner, M. J.\\nSetting the record straight on transformer oversmoothing,\\n2024.\\nElbayad, M., Gu, J., Grave, E., and Auli, M. Depth-adaptive\\ntransformer. In International Conference on Learning\\nRepresentations, 2020. URL https://openreview.\\nnet/forum?id=SJg7KhVKPH.\\nElhoushi, M., Shrivastava, A., Liskovich, D., Hosmer, B.,\\nWasti, B., Lai, L., Mahmoud, A., Acun, B., Agarwal,\\nS., Roman, A., et al. Layer skip: Enabling early exit\\ninference and self-speculative decoding. arXiv preprint\\narXiv:2404.16710, 2024.\\nFan, A., Grave, E., and Joulin, A. Reducing transformer\\ndepth on demand with structured dropout, 2019.\\nFan, S., Jiang, X., Li, X., Meng, X., Han, P., Shang, S., Sun,\\nA., Wang, Y., and Wang, Z. Not all layers of llms are\\nnecessary during inference, 2024.\\nFu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra,\\nA., and R´e, C. Hungry hungry hippos: Towards lan-\\nguage modeling with state space models. arXiv preprint\\narXiv:2212.14052, 2022.\\nGong, L., He, D., Li, Z., Qin, T., Wang, L., and Liu, T.\\nEfficient training of bert by progressively stacking. In\\nInternational conference on machine learning, pp. 2337–\\n2346. PMLR, 2019.\\nGraves, A. Adaptive computation time for recurrent neural\\nnetworks, 2017.\\nGromov, A., Tirumala, K., Shapourian, H., Glorioso, P., and\\nRoberts, D. A. The unreasonable ineffectiveness of the\\ndeeper layers, 2024.\\nGu, A. and Dao, T. Mamba: Linear-time sequence modeling\\nwith selective state spaces, 2023.\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\\nSong, D., and Steinhardt, J. Measuring massive multitask\\nlanguage understanding, 2021.\\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\\narXiv:2310.06825, 2023.\\nJiang, Y.-G., Cheng, C., Lin, H., and Fu, Y.\\nLearning\\nlayer-skippable inference network. IEEE Transactions on\\nImage Processing, 29:8747–8759, 2020. doi: 10.1109/\\nTIP.2020.3018269.\\nKaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu,\\nR., and McHardy, R. Challenges and applications of\\nlarge language models. CoRR, abs/2307.10169, 2023a.\\ndoi: 10.48550/ARXIV.2307.10169. URL https://\\ndoi.org/10.48550/arXiv.2307.10169.\\nKaddour, J., Key, O., Nawrot, P., Minervini, P., and Kusner,\\nM. J.\\nNo train no gain: Revisiting efficient training\\nalgorithms for transformer-based language models. In\\nOh, A., Naumann, T., Globerson, A., Saenko, K., Hardt,\\nM., and Levine, S. (eds.), Advances in Neural Information\\nProcessing Systems 36: Annual Conference on Neural\\nInformation Processing Systems 2023, NeurIPS 2023,\\nNew Orleans, LA, USA, December 10 - 16, 2023, 2023b.\\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\\nC. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient\\nmemory management for large language model serving\\nwith pagedattention. In Proceedings of the 29th Sym-\\nposium on Operating Systems Principles, pp. 611–626,\\n2023.\\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\\nhow models mimic human falsehoods, 2022.\\nLiu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z.,\\nShrivastava, A., Zhang, C., Tian, Y., Re, C., and Chen,\\nB. Deja vu: Contextual sparsity for efficient LLMs at\\ninference time. In Krause, A., Brunskill, E., Cho, K.,\\nEngelhardt, B., Sabato, S., and Scarlett, J. (eds.), Pro-\\nceedings of the 40th International Conference on Ma-\\nchine Learning, volume 202 of Proceedings of Machine\\nLearning Research, pp. 22137–22176. PMLR, 23–29 Jul\\n2023. URL https://proceedings.mlr.press/\\nv202/liu23am.html.\\nMen, X., Xu, M., Zhang, Q., Wang, B., Lin, H., Lu, Y., Han,\\nX., and Chen, W. Shortgpt: Layers in large language\\nmodels are more redundant than you expect, 2024. URL\\nhttps://arxiv.org/abs/2403.03853.\\nNawrot, P., Ła´ncucki, A., Chochowski, M., Tarjan, D., and\\nPonti, E. M. Dynamic memory compression: Retrofitting\\nllms for accelerated inference, 2024.\\nPatterson, D. A., Gonzalez, J., Le, Q. V., Liang, C., Munguia,\\nL., Rothchild, D., So, D. R., Texier, M., and Dean, J. Car-\\nbon emissions and large neural network training. CoRR,\\n5\\nAttention Is All You Need But You Don’t Need All Of It\\nabs/2104.10350, 2021. URL https://arxiv.org/\\nabs/2104.10350.\\nPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,\\nS., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K.,\\net al. Rwkv: Reinventing rnns for the transformer era.\\narXiv preprint arXiv:2305.13048, 2023.\\nReid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lilli-\\ncrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat,\\nO., Schrittwieser, J., et al. Gemini 1.5: Unlocking multi-\\nmodal understanding across millions of tokens of context.\\narXiv preprint arXiv:2403.05530, 2024.\\nSajjad, H., Dalvi, F., Durrani, N., and Nakov, P. On the\\neffect of dropping layers of pre-trained transformer mod-\\nels.\\nComputer Speech & Language, 77:101429, jan\\n2023. doi: 10.1016/j.csl.2022.101429. URL https:\\n//doi.org/10.1016%2Fj.csl.2022.101429.\\nSchuster, T., Fisch, A., Gupta, J., Dehghani, M., Bahri, D.,\\nTran, V., Tay, Y., and Metzler, D. Confident adaptive\\nlanguage modeling. Advances in Neural Information\\nProcessing Systems, 35:17456–17472, 2022.\\nTeerapittayanon, S., McDanel, B., and Kung, H. T.\\nBranchynet: Fast inference via early exiting from deep\\nneural networks, 2017.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\\nple, G. Llama: Open and efficient foundation language\\nmodels, 2023a.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\\nFuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,\\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\\nV., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,\\nMao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\\nI., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\\nXu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur,\\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\\nand Scialom, T. Llama 2: Open foundation and fine-tuned\\nchat models, 2023b.\\nWang, J., Chen, K., Chen, G., Shou, L., and McAuley, J.\\nSkipbert: Efficient inference with shallow layer skipping.\\nIn Proceedings of the 60th Annual Meeting of the Asso-\\nciation for Computational Linguistics (Volume 1: Long\\nPapers), pp. 7287–7301, 2022a.\\nWang, P., Zheng, W., Chen, T., and Wang, Z.\\nAnti-\\noversmoothing in deep vision transformers via the fourier\\ndomain analysis:\\nFrom theory to practice.\\nIn In-\\nternational Conference on Learning Representations,\\n2022b. URL https://openreview.net/forum?\\nid=O476oWmiNNp.\\nWu, H. and Tu, K. Layer-condensed kv cache for efficient\\ninference of large language models, 2024.\\nXia, H., Yang, Z., Dong, Q., Wang, P., Li, Y., Ge, T., Liu, T.,\\nLi, W., and Sui, Z. Unlocking efficiency in large language\\nmodel inference: A comprehensive survey of speculative\\ndecoding. arXiv preprint arXiv:2401.07851, 2024.\\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.\\nHellaswag: Can a machine really finish your sentence?,\\n2019.\\nZhai, S., Likhomanenko, T., Littwin, E., Busbridge, D.,\\nRamapuram, J., Zhang, Y., Gu, J., and Susskind, J. Sta-\\nbilizing transformer training by preventing attention en-\\ntropy collapse, 2023.\\nZhang, M. and He, Y. Accelerating training of transformer-\\nbased language models with progressive layer dropping.\\nAdvances in neural information processing systems, 33:\\n14011–14023, 2020.\\n6\\n'), Document(metadata={'Published': '2021-07-16', 'Title': 'All the attention you need: Global-local, spatial-channel attention for image retrieval', 'Authors': 'Chull Hwan Song, Hye Joo Han, Yannis Avrithis', 'Summary': 'We address representation learning for large-scale instance-level image\\nretrieval. Apart from backbone, training pipelines and loss functions, popular\\napproaches have focused on different spatial pooling and attention mechanisms,\\nwhich are at the core of learning a powerful global image representation. There\\nare different forms of attention according to the interaction of elements of\\nthe feature tensor (local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses only one or two\\nforms of attention and applies it to different problems like classification,\\ndetection or retrieval.\\n  We present global-local attention module (GLAM), which is attached at the end\\nof a backbone network and incorporates all four forms of attention: local and\\nglobal, spatial and channel. We obtain a new feature tensor and, by spatial\\npooling, we learn a powerful embedding for image retrieval. Focusing on global\\ndescriptors, we provide empirical evidence of the interaction of all forms of\\nattention and improve the state of the art on standard benchmarks.'}, page_content='All the attention you need:\\nGlobal-local, spatial-channel attention for image retrieval\\nChull Hwan Song\\nOdd Concepts\\nHye Joo Han\\nOdd Concepts\\nYannis Avrithis\\nInria, Univ Rennes, CNRS, IRISA\\nAbstract\\nWe address representation learning for large-scale\\ninstance-level image retrieval. Apart from backbone, train-\\ning pipelines and loss functions, popular approaches have\\nfocused on different spatial pooling and attention mecha-\\nnisms, which are at the core of learning a powerful global\\nimage representation. There are different forms of attention\\naccording to the interaction of elements of the feature tensor\\n(local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses\\nonly one or two forms of attention and applies it to different\\nproblems like classiﬁcation, detection or retrieval.\\nWe present global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network and\\nincorporates all four forms of attention: local and global,\\nspatial and channel. We obtain a new feature tensor and, by\\nspatial pooling, we learn a powerful embedding for image\\nretrieval. Focusing on global descriptors, we provide em-\\npirical evidence of the interaction of all forms of attention\\nand improve the state of the art on standard benchmarks.\\n1. Introduction\\nInstance-level image retrieval is at the core of visual rep-\\nresentation learning and is connected with many problems\\nof visual recognition and machine learning, for instance\\nmetric learning [30, 26], few-shot learning [42] and unsu-\\npervised learning [8]. Many large-scale open datasets [3,\\n37, 16, 29, 53], and competitions1 have accelerated progress\\nin instance-level image retrieval, which has been trans-\\nformed by deep learning [3].\\nMany studies on instance-level image retrieval focus\\non learning features from convolutional neural networks\\n(CNN), while others focus on re-ranking, for instance by\\ngraph-based methods [11]. The former can be distinguished\\naccording to feature types: local descriptors, reminiscent of\\nSIFT [27], where an image is mapped to a few hundred vec-\\ntors; and global descriptors, where an image is mapped to a\\n1https://www.kaggle.com/c/landmark-retrieval-2020\\nsingle vector. In fact, deep learning has brought global de-\\nscriptors with astounding performance, while allowing efﬁ-\\ncient search. Our study belongs to this type.\\nStudies on global descriptors have focused on spatial\\npooling [2, 37]. The need for compact, discriminative rep-\\nresentations that are resistant to clutter has naturally given\\nrise to spatial attention methods [24, 28]. Different kinds\\nof attention have been studied in many areas of computer\\nvision research. There is also channel attention [20, 9]; lo-\\ncal attention, applied independently to elements of the rep-\\nresentation (feature map) [54, 25]; global attention, based\\non interaction between elements [52, 9]; and combinations\\nthereof. Unfortunately, each study has been limited to one or\\ntwo kinds of attention only; attention is not always learned;\\nand applications vary.\\nIt is the objective of our work to perform a compre-\\nhensive study of all forms of attention above, apply them\\nto instance-level image retrieval and provide a detailed ac-\\ncount of their interaction and impact on performance. As\\nshown in Figure 1, we collect contextual information from\\nimages with both local and global attention, giving rise to\\ntwo parallel network streams. Importantly, each operates\\non both spatial locations and feature channels. Local at-\\ntention is about individual locations and channels; global is\\nabout interaction between locations and between channels.\\nThe extracted information is separately embedded in local\\nand global attention feature maps, which are combined in a\\nglobal-local attention feature map before pooling.\\nOur contributions can be summarized as follows:\\n1. We propose a novel network that consists of both\\nglobal and local attention for image retrieval. This is\\nthe ﬁrst study that employs both mechanisms.\\n2. Each of the global and local attention mechanisms\\ncomprises both spatial and channel attention.\\n3. Focusing on global descriptors, we provide empirical\\nevidence of the interaction of all forms of attention and\\nimprove the state of the art on standard benchmarks.\\n1\\narXiv:2107.08000v1  [cs.CV]  16 Jul 2021\\nAl\\nc\\nc × 1 × 1\\n×\\n+\\nFl\\nc\\nAl\\ns\\n1 × h × w\\n×\\n+\\nFl\\n×\\nc × h × w\\nF\\n×\\n+\\nc × h × w\\nFgl\\nAg\\nc\\nc × c\\n×\\nFg\\nc\\nAg\\ns\\nhw × hw\\n×\\n+\\nFg\\n×\\nwl\\nw\\nwg\\nchannel attention\\nspatial attention\\nfusion\\nlocal attention\\nglobal attention\\nFigure 1: Our global-local attention module (GLAM) involves both channel and spatial attention, as well as both local atten-\\ntion (channels/locations weighted independently, based on contextual information obtained by pooling) and global attention\\n(based on pairwise interaction between channels/locations). As a result, four attention maps are used: local channel (Al\\nc),\\nlocal spatial (Al\\ns), global channel (Ag\\nc) and global spatial (Ag\\ns). The input feature map F is weighted into local (Fl) and\\nglobal (Fg) attention feature maps, which are fused with F to yield the global-local attention feature map Fgl. The diagram\\nis abstract: The four attention modules are shown in more detail in Figures 2, 3, 4, 5.\\n2. Related work\\nInstance-level image retrieval\\nStudies on instance-level\\nimage retrieval can be roughly, but not exclusively, di-\\nvided into three types: (1) studies on global descriptors\\n[3, 16, 24, 53, 2, 37]; (2) studies on local descriptors and\\ngeometry-based re-ranking [29, 45, 40, 53]; (3) re-ranking\\nby graph-based methods [11, 21, 55]. The ﬁrst two types\\nof studies focus on the feature representation, while the last\\ntype focuses on re-ranking extracted features.\\nStudies on global descriptors focus on spatial pooling\\nof CNN feature maps into vectors, including MAC [38],\\nSPoC [2], CroW [24], R-MAC [48, 15, 16], GeM [37],\\nand NetVLAD [1, 25], as well as learning the representa-\\ntion [3, 15, 16, 36, 37]. Studies before deep learning dom-\\ninated image retrieval were mostly based on local descrip-\\ntors like SIFT [27] and bag-of-words representation [32] or\\naggregated descriptors like VLAD [22] or ASMK [46]. Lo-\\ncal descriptors have been revived in deep learning, e.g. with\\nDELF [29], DELG [5] and ASMK extensions [45, 47].\\nWe focus on learning a global descriptor in this work, be-\\ncause it is the most efﬁcient in terms of storage and search.\\nHowever, our generic attention mechanism produces a fea-\\nture tensor and could be applicable to local descriptors as\\nwell, if global pooling were replaced by local feature detec-\\ntion. Re-ranking methods are complementary to the repre-\\nsentation and we do not consider them in this work.\\nAttention\\nAttention mechanisms have been ﬁrst proposed\\nin image classiﬁcation studies focusing on channel at-\\nMETHOD\\nLOCAL\\nGLOBAL\\nLRN RET\\nSpatial Channel Spatial Channel\\nSENet [20]\\n✓\\n✓\\nECA-Net [51]\\n✓\\n✓\\nGCNet [6]\\n✓\\n✓\\nCBAM [54]\\n✓\\n✓\\n✓\\nGE [19]\\n✓\\n✓\\nNL-Net [52]\\n✓\\n✓\\nAA-Net [4]\\n✓\\n✓\\nSAN [59]\\n✓\\n✓\\nN3Net [34]\\n✓\\n✓\\nA2-Net [9]\\n✓\\n✓\\nGSoP [14]\\n✓\\n✓\\nOnA [23]\\n✓\\n✓\\nAGeM [17]\\n✓\\n✓\\nCroW [24]\\n✓\\n✓\\n✓\\nCRN [25]\\n✓\\n✓\\n✓\\nDELF [29]\\n✓\\n✓\\n✓\\nDELG [5]\\n✓\\n✓\\n✓\\nTolias et al. [47]\\n✓\\n✓\\n✓\\nSOLAR [28]\\n✓\\n✓\\n✓\\nOurs\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nTable 1: Related work on attention. LRN: learned; RET: ap-\\nplied to instance-level image retrieval.\\ntention [20, 51, 6], spatial attention [19] or both, like\\nCBAM [54]. In image retrieval, CroW [24] also employs\\n2\\nfeature map\\nGAP\\nconv1d(k)\\nsigmoid\\nattention map\\nc × h × w\\nc × 1 × 1\\nc × 1 × 1\\nF\\nAl\\nc\\nFigure 2: Local channel attention.\\nboth spatial and channel attention and can be seen as a pre-\\ncursor of CBAM, but, like other studies of spatial attention\\non retrieval [41, 23, 17], it is not learned. CRN [25] ap-\\nplies spatial attention for feature reweighting and is learned.\\nLearned spatial attention mechanisms are common for local\\ndescriptors [29, 5, 47].\\nWe call the above methods local attention, in the sense\\nthat elements of the feature tensor (channels / spatial loca-\\ntions), are weighted independently, based on contextual in-\\nformation obtained by pooling or learned. By constrast, by\\nglobal attention we refer to mechanisms that model inter-\\naction between elements of the feature tensor, for example\\nbetween channels or between locations.\\nIn image classiﬁcation, non-local neural network (NL-\\nNet) [52] is maybe the ﬁrst global attention mechanism, fol-\\nlowed by similar studies [4, 59, 34]. It is global spatial at-\\ntention, allowing interaction between any pair of spatial lo-\\ncations. Similarly, there are studies of global channel atten-\\ntion, allowing interaction between channels [9, 14]. Global\\nattention has focused mostly on image recognition and has\\nbeen applied to either spatial or channel attention so far, not\\nboth. In image retrieval, SOLAR [28] is a direct application\\nof the global spatial attention mechanism of [52].\\nTable 1 attempts to categorize related work on atten-\\ntion according to whether attention is local or global, spa-\\ntial or channel, whether it is learned and whether it is ap-\\nplied to instance-level image retrieval. We observe that all\\nmethods limit to one or two forms of attention only. Of\\nthose studies that focus on image retrieval, many are not\\nlearned [23, 17, 24], and of those that are, some are de-\\nsigned for local descriptors [29, 47].\\nBy contrast, we provide a comprehensive study of all\\nforms of attention, global and local, spatial and channel, to\\nobtain a learned representation in the form of a tensor that\\ncan be used in any way. We spatially pool it into a global\\ndescriptor and we study the relative gain of different forms\\nof attention in image retrieval.\\nfeature map\\nconv 1 × 1\\nconv 3 × 3\\nconv 5 × 5\\nconv 7 × 7\\nconcat\\nconv 1 × 1\\nattention map\\nc × h × w\\n4c′ × h × w\\n1 × h × w\\nc′ × h × w\\ndilated\\nconv\\nF\\nF′\\nAl\\ns\\nFigure 3: Local spatial attention. Convolutional layers in\\nblue implemented by dilated convolutions with kernel size\\n3 × 3 and dilation factors 1, 3, 5.\\n3. Global-local attention\\nWe design a global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network. Figure 1\\nillustrates its main components. We are given a c × h × w\\nfeature tensor F, where c is the number of channels, and\\nh × w is the spatial resolution. Local attention collects con-\\ntext from the image and applies pooling to obtain a c×1×1\\nlocal channel attention map Al\\nc and a 1 × h × w local spa-\\ntial attention map Al\\ns. Global attention allows interaction\\nbetween channels, resulting in a c × c global channel at-\\ntention map Ag\\nc, and between spatial locations, resulting in\\na hw × hw global spatial attention map Ag\\ns. The feature\\nmaps produced by the two attention streams are combined\\nwith the original one by a learned fusion mechanism into\\nthe global-local attention feature map Fgl before being spa-\\ntially pooled into a global image descriptor.\\n3.1. Local attention\\nWe extract an 1D channel and a 2D spatial attention map\\nto weigh the feature map in the corresponding dimensions.\\nLocal channel attention\\nFollowing ECA-Net [51], this\\nattention captures local channel information. As shown in\\nFigure 2, we are given a c×h×w feature tensor F from our\\nbackbone. We ﬁrst reduce it to a c × 1 × 1 tensor by global\\naverage pooling (GAP). Channel attention is then captured\\nby a 1D convolution of kernel size k along the channel di-\\nmension, where k controls the extent of cross-channel inter-\\naction. This is followed by a sigmoid function, resulting in\\nthe c × 1 × 1 local channel attention map Al\\nc.\\nLocal spatial attention\\nInspired by the inception mod-\\nule [43] and similar to [25], this attention map captures local\\nspatial information at different scales. As shown in Figure 3,\\n3\\nfeature map\\nGAP\\nconv1d(k)\\nconv1d(k)\\nsigmoid\\nsigmoid\\n×\\n×\\nsoftmax\\nattention feature map\\n1 × c\\n1 × c\\n1 × c\\nQc\\nc × c\\nhw × c\\nVc\\nAg\\nc\\nc × h × w\\n1 × c\\n1 × c\\nKc\\nF\\nGc\\nFigure 4: Global channel attention.\\ngiven the same c × h × w feature tensor F from our back-\\nbone, we obtain a new tensor F′ with channels reduced to\\nc′, using a 1 × 1 convolution. We then extract local spatial\\ncontextual information using convolutional ﬁlters of kernel\\nsize 3 × 3, 5 × 5, and 7 × 7, which are efﬁciently imple-\\nmented by 3 × 3 dilated convolutions [7, 57] with dilation\\nparameter 1, 2, and 3 respectively. The resulting features,\\nalong with one obtained by 1 × 1 convolution on F′, are\\nconcatenated into a 4c′ × h × w tensor. Finally, we obtain\\nthe 1 × h × w local spatial attention map Al\\ns by a 1 × 1\\nconvolution that reduces the channel dimension to 1.\\nThe middle column of Figure 6 shows heat maps of local\\nspatial attention, localizing target objects in images.\\nLocal attention feature map\\nWe use the local channel\\nattention map Al\\nc to weigh F in the channel dimension\\nFl\\nc := F ⊙Al\\nc + F.\\n(1)\\nWe then use local spatial attention map Al\\ns to weigh Fl\\nc\\nin the spatial dimensions, resulting in the c × h × w local\\nattention feature map\\nFl = Fl\\nc ⊙Al\\ns + Fl\\nc.\\n(2)\\nHere, A⊙B denotes an element-wise multiplication of ten-\\nsors A and B, with broadcasting when one tensor is smaller.\\nWe adopt the choice of applying channel followed by spa-\\ntial attention from convolutional block attention module\\nCBAM [54]. However, apart from computing Al\\ns at differ-\\nent scales, both attention maps are obtained from the orig-\\ninal tensor F rather than sequentially. In addition, both (1)\\nand (2) include residual connections, while CBAM includes\\na single residual connection over both steps.\\n3.2. Global attention\\nWe extract two matrices capturing global pairwise chan-\\nnel and spatial interaction to weigh the feature map.\\nfeature map\\nconv 1 × 1\\nconv 1 × 1\\nconv 1 × 1\\n×\\n×\\nsoftmax\\nconv 1 × 1\\nattention feature map\\nc′ × hw\\nQs\\nhw × hw\\nc × h × w\\nc′ × hw\\nVs\\nc′ × h × w\\nAg\\ns\\nc × h × w\\nc′ × hw\\nKc\\nF\\nGs\\nFigure 5: Global spatial attention.\\nGlobal channel attention\\nWe introduce a global channel\\nattention mechanism that captures global channel interac-\\ntion. This mechanism is based on the non-local neural net-\\nwork [52], but with the idea of 1D convolution from ECA-\\nNet [51]. As shown in Figure 4, we are given the c × h × w\\nfeature tensor F from our backbone. We apply GAP and\\nsqueeze spatial dimensions, followed by a 1D convolution\\nof kernel size k and a sigmoid function, to obtain 1×c query\\nQc and key Kc tensors. The value tensor Vc is obtained by\\nmere reshaping of F to hw×c, without GAP. Next, we form\\nthe outer product of Kc and Qc, followed by softmax over\\nchannels to obtain a c × c global channel attention map\\nAg\\nc = softmax(Kc\\n⊤Qc).\\n(3)\\nFinally, this attention map is multiplied with Vc and the ma-\\ntrix product VcAg\\nc is reshaped back to c×h×w to give the\\nglobal channel attention feature map Gc. In GSoP [14] and\\nA2-Net [9], a c×c global channel attention map is obtained\\nby multiplication of hw × c matrices; (3) is more efﬁcient,\\nusing only an outer product of 1 × c vectors.\\nGlobal spatial attention\\nSince ordinary convolution ap-\\nplies only a local neighborhood at a time, it cannot capture\\nglobal contextual information. Thus, we apply non-local ﬁl-\\ntering [52], which is a form of self-attention [49] in the spa-\\ntial dimensions. As shown in Figure 5, we are given the\\nsame c × h × w feature tensor F from our backbone. By\\nusing three 1×1 convolutions, which reduce channels to c′,\\nand ﬂattening spatial dimensions to hw, we obtain c′ × hw\\nquery Qs, key Ks, and value Vs tensors, where each col-\\numn is a feature vector corresponding to a particular spatial\\nlocation. We capture pairwise similarities of these vectors\\nby matrix multiplication of Ks and Qs, followed by soft-\\nmax over locations to obtain a hw × hw global spatial at-\\ntention map:\\nAg\\ns = softmax(K⊤\\ns Qs).\\n(4)\\n4\\nThis attention map is multiplied with Vs and the matrix\\nproduct VsAg\\ns is reshaped back to c′ ×h×w by expanding\\nthe spatial dimensions. Finally, using a 1 × 1 convolution,\\nwhich increases channels back to c, we obtain the c×h×w\\nglobal spatial attention feature map Gs.\\nThe right column of Figure 6 shows heat maps for global\\nspatial attention, localizing target objects in images.\\nGlobal attention feature map\\nWe use the global channel\\nattention feature map Fc to weigh F element-wise\\nFg\\nc = F ⊙Gc.\\n(5)\\nWe then use global spatial attention feature map Gs to\\nweigh Fg\\nc element-wise, resulting in the c × h × w global\\nattention feature map\\nFg = Fg\\nc ⊙Gs + Fg\\nc.\\n(6)\\nSimilarly to Fl in (1) and (2), we apply channel attention\\nﬁrst, followed by spatial attention. However, unlike (1),\\nthere is no residual connection in (5). This choice is sup-\\nported by early experiments.\\n3.3. Global-local attention\\nFeature fusion\\nAs shown in Figure 1, we combine the\\nlocal and global attention feature maps, Fl and Fg, with\\nthe original feature F. While concatenation and summation\\nare common operations for feature combination, we use a\\nweighted average with weights wl, wg, w respectively, ob-\\ntained by softmax over three learnable scalar parameters, to\\nobtain a c × h × w global-local attention feature map\\nFgl = wlFl + wgFl + wF.\\n(7)\\nEfﬁcientDet [44] has shown that this is the most effective,\\namong a number of choices, for fusion of features across\\ndifferent scales.\\nPooling\\nWe apply GeM [37], a learnable spatial pooling\\nmechanism, to feature map Fgl (7), followed by a fully-\\nconnected (FC) layer with dropout and batch normalization.\\nThe ﬁnal embedding is obtained by ℓ2-normalization.\\n4. Experiments\\n4.1. Datasets\\nTraining set\\nThere are a number of open landmark\\ndatasets commonly used for training in image retrieval stud-\\nies, including neural code (NC) [3], neural code clean (NC-\\nclean) [16], as well as Google Landmarks v1 (GLDv1) [29]\\nand v2 (GLDv2) [53]. Table 2 shows relevant statistics.\\nThese datasets can be categorized into noisy and clean. The\\nclean sets were obtained from the original noisy sets for\\nmore effective training [16, 53]. The original noisy datasets\\nare much larger, but they have high intra-class variability.\\n(a) input\\n(b) local\\n(c) global\\nFigure 6: Local and global spatial attention. Left: input\\nimages. Middle: local spatial attention heat maps. Right:\\nglobal spatial attention heat maps. Red (blue) means higher\\n(lower) attention weight.\\nEach class can include visually dissimilar images such as\\nexterior and interior views of a building or landmark, in-\\ncluding ﬂoor plans and paintings inside. The clean datasets\\nfocus on views directly relevant to landmark recognition but\\nhave a much smaller number of images.\\nEvaluation set and metrics\\nWe use four common eval-\\nuation datasets for landmark image retrieval: Oxford5k\\n(Ox5k) [32], Paris6k (Par6k) [33], as well as Revisited Ox-\\nford (ROxford or ROxf) and Paris (RParis or RPar) [35].\\nROxford and RParis are used with and without one million\\ndistractors (R1M) [28] and evaluated using the Medium and\\nHard protocols [35]. We evaluate using mean Average Pre-\\ncision (mAP) and mean precision at 10 (mP@10).\\n4.2. Implementation details\\nWe train on 8 TITAN RTX 2080Ti GPUs. All models are\\npre-trained on ImageNet [39] and implemented in PyTorch\\n[31]. For fair comparisons, we set a training environment\\n5\\nFigure 7: Examples of our ranking results. In each row, the ﬁrst image on the left (pink dotted outline) is a query image with a\\ntarget object (red crop box), and the following are the top ranking images for the query. Orange solid outline: positive images\\nfor the query; red solid outline: negative.\\nsimilar to the those of compared studies [56, 53, 28, 35]. We\\nemploy ResNet101 [18] as a backbone model. The kernel\\nsize k of ECANet in subsection 3.1 is set to 3. The param-\\neter p of GeM in subsection 3.3 is set to 3 and the dimen-\\nsion d of ﬁnal embeddings to 512. We adopt ArcFace [10],\\na cosine-softmax based loss, with a margin of 0.3. We use\\nstochastic gradient descent with initial learning rate 10−3,\\nmomentum 0.9 and weight decay 10−5.\\nWe adopt the batch sampling of Yokoo et al. [56] where\\nmini-batch samples with similar aspect ratios are resized to\\na particular size. Here, we use a batch size of 64. For image\\naugmentation, we apply scaling, random cropping, and var-\\nied illumination. At inference, we apply a multi-resolution\\nrepresentation [16] to query and database images.\\nOur method is denoted as GLAM (global-local atten-\\ntion module). Using the backbone model alone is referred\\nto as baseline. It is compatible with recent models based\\non ResNet101-GeM trained with ArcFace [53, 28]. Adding\\nour local attention (subsection 3.1) to the baseline model is\\ndenoted +local, while adding our global attention (subsec-\\ntion 3.2) is denoted +global. Since we focus on representa-\\ntion learning, we do not consider post-processing methods\\nlike geometry-based re-ranking [29, 40, 53] or graph-based\\nre-ranking [11, 21, 55].\\n4.3. Benchmarking\\nNoisy vs. clean training sets\\nWe begin by training our\\nbest model (baseline+local+global) on all training sets of\\nTable 2, except NC-noisy because some images are cur-\\nrently unavailable. As shown in Table 3, even though\\nTRAIN SET\\n#IMAGES\\n#CLASSES\\nNC-noisy\\n213,678\\n672\\nNC-clean\\n27,965\\n581\\nSfM-120k\\n117,369\\n713\\nGLDv1-noisy\\n1,225,029\\n14, 951\\nGLDv2-noisy\\n4,132,914\\n203,094\\nGLDv2-clean\\n1,580,470\\n81,313\\nTable 2: Statistics of different training sets.\\nMETHOD\\nTRAIN SET\\nDIM OXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nGeM-Siamese [37, 35]\\nSfM-120k\\n2048\\n87.8\\n92.7\\n64.7\\n77.2\\n38.5\\n56.3\\nSOLAR [28]\\nGLDv1-noisy 2048\\n–\\n–\\n69.9\\n81.6\\n47.9\\n64.5\\nGLDv2 [53]\\nGLDv2-clean 2048\\n–\\n–\\n74.2\\n84.9\\n51.6\\n70.3\\nGLAM (Ours)\\nNC-clean\\n512\\n77.8\\n85.8\\n51.6\\n68.1\\n20.9\\n44.7\\nGLDv1-noisy 512\\n92.8\\n95.0\\n73.7\\n83.5\\n49.8\\n69.4\\nGLDv2-noisy 512\\n93.3\\n95.3\\n75.7\\n86.0\\n53.1\\n73.8\\nGLDv2-clean 512\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 3: mAP comparison of our best model (base-\\nline+local+global) trained on different training sets against\\n[53, 28]. All models use ResNet101-GeM. Red: best results.\\nBlue: GLAM higher than SOLAR [28] on GLDv1-noisy.\\nGLDv2-noisy has 2.6 times more images than GLDv2-\\nclean, the latter is superior by a large margin. This shows\\nthat, in training, a cleaner dataset can be more important\\nthan a larger one. By contrast, NC-clean has the worst\\nperformance despite being clean, aparently because it is\\n6\\nMETHOD\\nTRAIN SET\\nDIM\\nBASE\\nMEDIUM\\nHARD\\nOx5k Par6k\\nROxf\\n+R1M\\nRPar\\n+R1M\\nROxf\\n+R1M\\nRPar\\n+R1M\\nmAP\\nmAP mAP mP mAP mP mAP mP mAP mP\\nmAP mP mAP mP mAP mP mAP mP\\nSPoC-V16 [2, 35]\\n[O]\\n512\\n53.1∗\\n–\\n38.0 54.6 17.1 33.3 59.8 93.0 30.3 83.0 11.4 20.9\\n0.9\\n2.9\\n32.4 69.7\\n7.6\\n30.6\\nSPoC-R101 [35]\\n[O]\\n2048\\n–\\n–\\n39.8 61.0 21.5 40.4 69.2 96.7 41.6 92.0 12.4 23.8\\n2.8\\n5.6\\n44.7 78.0 15.3 54.4\\nCroW-V16 [24, 35]\\n[O]\\n512\\n70.8\\n79.7\\n41.4 58.8 22.5 40.5 62.9 94.4 34.1 87.1 13.9 25.7\\n3.0\\n6.6\\n36.9 77.9 10.3 45.1\\nCroW-R101 [35]\\n[O]\\n2048\\n–\\n–\\n42.4 61.9 21.2 39.4 70.4 97.1 42.7 92.9 13.3 27.7\\n3.3\\n9.3\\n47.2 83.6 16.3 61.6\\nMAC-V16-Siamese [36, 35]\\n[O]\\n512\\n80.0\\n82.9\\n37.8 57.8 21.8 39.7 59.2 93.3 33.6 87.1 14.6 27.0\\n7.4\\n11.9 35.9 78.4 13.2 54.7\\nMAC-R101-Siamese [35]\\n[O]\\n2048\\n–\\n–\\n41.7 65.0 24.2 43.7 66.2 96.4 40.8 93.0 18.0 32.9\\n5.7\\n14.4 44.1 86.3 18.2 67.7\\nRMAC-V16-Siamese [36, 35]\\n[O]\\n512\\n80.1\\n85.0\\n42.5 62.8 21.7 40.3 66.2 95.4 39.9 88.9 12.0 26.1\\n1.7\\n5.8\\n40.9 77.1 14.8 54.0\\nRMAC-R101-Siamese [35]\\n[O]\\n2048\\n–\\n–\\n49.8 68.9 29.2 48.9 74.0 97.7 49.3 93.7 18.5 32.2\\n4.5\\n13.0 52.1 87.1 21.3 67.4\\nRMAC-R101-Triplet [16, 35]\\nNC-clean\\n2048\\n86.1\\n94.5\\n60.9 78.1 39.3 62.1 78.9 96.9 54.8 93.9 32.4 50.0 12.5 24.9 59.4 86.1 28.0 70.0\\nGeM-R101-Siamese [37, 35]\\nSfM-120k\\n2048\\n87.8\\n92.7\\n64.7 84.7 45.2 71.7 77.2 98.1 52.3 95.3 38.5 53.0 19.9 34.9 56.3 89.1 24.7 73.3\\nAGeM-R101-Siamese [17]\\nSfM-120k\\n2048\\n–\\n–\\n67.0\\n–\\n–\\n–\\n78.1\\n–\\n–\\n–\\n40.7\\n–\\n–\\n–\\n57.3\\n–\\n–\\n–\\nSOLAR-GeM-R101-Triplet/SOS [28] GLDv1-noisy 2048\\n–\\n–\\n69.9 86.7 53.5 76.7 81.6 97.1 59.2 94.9 47.9 63.0 29.9 48.9 64.5 93.0 33.4 81.6\\nDELG-GeM-R101-ArcFace [5]\\nGLDv1-noisy 2048\\n–\\n–\\n73.2\\n–\\n54.8\\n–\\n82.4\\n–\\n61.8\\n–\\n51.2\\n–\\n30.3\\n–\\n64.7\\n–\\n35.5\\n–\\nGeM-R101-ArcFace [53]\\nGLDv2-clean 2048\\n–\\n–\\n74.2\\n–\\n–\\n–\\n84.9\\n–\\n–\\n–\\n51.6\\n–\\n–\\n–\\n70.3\\n–\\n–\\n–\\nGLAM-GeM-R101-ArcFace baseline\\nGLDv2-clean\\n512\\n91.9\\n94.5\\n72.8 86.7 58.1 78.2 84.2 95.9 63.9 93.3 49.9 62.1 31.6 49.7 69.7 88.4 37.7 73.7\\n+local\\nGLDv2-clean\\n512\\n91.2\\n95.4\\n73.7 86.2 60.5 77.4 86.5 95.6 68.0 93.9 52.6 65.3 36.1 55.6 73.7 89.3 44.7 79.1\\n+global\\nGLDv2-clean\\n512\\n92.3\\n95.3\\n77.2 87.0 63.8 79.3 86.7 95.4 67.8 93.7 57.4 69.6 38.7 57.9 75.0 89.4 45.0 77.0\\n+global+local\\nGLDv2-clean\\n512\\n94.2\\n95.6\\n78.6 88.2 68.0 82.4 88.5 97.0 73.5 94.9 60.2 72.9 43.5 62.1 76.8 93.4 53.1 84.0\\nTable 4: mAP comparison of our GLAM against SOTA methods based on global descriptors without re-ranking. V16:\\nVGG16; R101: ResNet101. [O]: Off-the-shelf (pre-trained on ImageNet). ∗: dimension d = 256 [2]. mP: mP@10. Red:\\nbest results. Black bold: best previous methods. Blue: GLAM higher than previous methods. Weyand et al. [53] is the only\\nmodel other than ours trained on GLDv2-clean, while [28] is trained on GLDv1-noisy and compared in Table 3.\\ntoo small. To achieve best possible performance, we use\\nGLDv2-clean as a training set in the remaining experiments.\\nComparisons on same training set\\nIt is common to com-\\npare methods regardless of training sets as more become\\navailable, e.g., [35, 28]. Since GLDv2-clean is relatively\\nnew, Weyand et al. [53], which introduced the dataset, is the\\nonly study that has trained the same backbone with the same\\nsettings (ResNet101-GeM with ArcFace) on GLDv2-clean.\\nOur baseline is lower than [53], because our dimensinality is\\n512, while other models based on ResNet101 use 2048. Yet,\\nTable 3 shows that our best model trained on GLDv2-clean\\noutperforms [53] by a large margin. But the most impor-\\ntant comparison is with SOLAR [28], also based on self-\\nattention, which has trained ResNet101-GeM on GLDv1-\\nnoisy. On this training set, our best model clearly outper-\\nforms [28] despite lower dimensionality.\\nComparison with state of the art\\nTable 4 shows the\\nperformance of four variants of our model, i.e. baseline\\nwith or without local/global attention, and compares them\\nagainst state-of-the-art (SOTA) methods based on global de-\\nscriptors without re-ranking on the complete set of bench-\\nmarks, including distractors. Both local and global atten-\\ntion bring signiﬁcant gain over the baseline. The effect\\nof global is stronger, while the gain of the two is addi-\\ntive in the combination. The best results are achieved by\\nthe global-local attention network (baseline+global+local).\\nWith this model, we outperform previous best methods\\non most benchmarks except mP@10 on RParis (medium)\\nand RParis+R1M (medium), where we are outperformed\\nby [37, 35]. These results demonstrate that our approach is\\neffective for landmark image retrieval. Figure 7 shows some\\nMETHOD\\nOXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nGLAM baseline\\n91.9\\n94.5\\n72.8\\n84.2\\n49.9\\n69.7\\n+local-channel\\n91.3\\n95.3\\n72.2\\n85.8\\n48.3\\n73.1\\n+local-spatial\\n91.0\\n95.1\\n72.1\\n85.3\\n48.3\\n71.9\\n+local\\n91.2\\n95.4\\n73.7\\n86.5\\n52.6\\n75.0\\n+global-channel\\n92.5\\n94.4\\n73.3\\n84.4\\n49.8\\n70.1\\n+global-spatial\\n92.4\\n95.1\\n73.2\\n86.3\\n50.0\\n72.7\\n+global\\n92.3\\n95.3\\n77.2\\n86.7\\n57.4\\n75.0\\n+global+local\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 5: mAP comparison of spatial and channel variants\\nof our local (+local, subsection 3.1) and global (+global,\\nsubsection 3.1) attention modules to the baseline.\\nMETHOD\\nOXF5K\\nPAR6K\\nRMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nCBAM style\\n93.8\\n95.7\\n75.6\\n88.4\\n53.3\\n76.8\\nGLAM (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 6: mAP comparison between CBAM style and our\\nlocal spatial attention.\\nexamples of our ranking results.\\n4.4. Ablation study\\nOur ablation study uses the Google Landmark v2 clean\\ndataset (GLDv2-clean) [53] for training, which is shown to\\nbe the most effective in Table 3.\\n7\\nMETHOD\\nOXF5K\\nPAR6K\\nRMEDIUM\\nRHARD\\nROxf\\nRPar\\nROxf\\nRPar\\nConcatenate\\n89.5\\n95.1\\n73.6\\n86.5\\n54.0\\n73.7\\nSum (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 7: mAP comparison between weighted concatenation\\nand weighted average for feature fusion.\\nMETHOD\\nOXF5K PAR6K\\nRMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nFixed-size\\n76.1\\n82.6\\n55.7\\n68.4\\n29.2\\n47.5\\nGroup-size (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 8: mAP comparison between ﬁxed-size (224 × 224)\\nand group-size sampling methods.\\nQUERY DATABASE OXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nSingle\\nSingle\\n93.3\\n95.2\\n76.9\\n87.1\\n58.6\\n74.7\\nMulti\\nSingle\\n93.9\\n95.4\\n78.0\\n87.7\\n59.0\\n75.5\\nSingle\\nMulti\\n93.6\\n95.6\\n77.0\\n87.8\\n57.1\\n76.0\\nMulti\\nMulti\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 9: mAP comparison of using multiresolution repre-\\nsentation (Multi) or not (Single) on query or database.\\nEffect of attention modules\\nWe ablate the effect of our\\nlocal and global attention networks as well as their com-\\nbination. Table 5 shows the results, which are more ﬁne-\\ngrained than those of Table 4. In particular, it shows the ef-\\nfect of the channel and spatial variants of both local and\\nglobal attention. We observe that, when used alone, the\\nchannel and spatial variants of local attention are harmful\\nin most cases. Even the combination, baseline+local, is not\\nalways effective. By contrast, when used alone, the channel\\nand spatial variants of global attention are mostly beneﬁcial,\\nespecially the latter. Their combination, baseline+global, is\\nimpressive, bringing gain of up to 7.5%. Importantly, the\\ncombination baseline+global+local improves further by up\\nto another 2.8%. This result shows the necessity of local\\nattention in the ﬁnal model.\\nCBAM vs. our local spatial attention\\nWe experiment\\nwith the local spatial attention of CBAM [54]. CBAM ap-\\nplies average and max-pooling to input features and con-\\ncatenates the two for spatial attention. We apply this vari-\\nant to our local spatial attention module for comparison.\\nFor the CBAM style module, we keep the overall design\\nof our module as shown in Figure 3, but apply average and\\nmax-pooling to each of the four convolutional layer outputs\\nbefore concatenation. Table 6 shows that the CBAM style\\nmodule is considerably worse than ours on all benchmarks\\nexcept Paris6k, where it is only slightly better.\\nConcatenation vs. sum for feature fusion\\nWe use a\\nsoftmax-based weighted average of local and global atten-\\ntion feature maps with the original feature map (7). Here,\\nwe compare this weighted average with weighted concate-\\nnation, where concatenation replaces the sum operation\\nin (7). As shown in Table 7, the weighted average outper-\\nforms the weighted concatenation.\\nFixed-size vs. group-size sampling\\nNumerous studies\\nhave proposed methods for constructing batches according\\nto image size for efﬁcient training. For instance, Gordo et\\nal. [16], DELF [29], and Yokoo et al. [56] employed dif-\\nferent image sizes per batch for training instead of a single\\nﬁxed size. We adopt the method of Yokoo et al., which con-\\nstructs a batch with images of similar aspect ratio, so that\\nthe images can be resized to a size with an aspect ratio that\\nis similar to their own. We call this method group-size sam-\\npling. Table 8 compares ﬁxed-size (224 × 224) with group-\\nsize sampling. We observe that maintaining aspect ratios by\\nusing dynamic input sizes is much more effective.\\nMulti-resolution\\nWe use the multi-resolution representa-\\ntion [16] for the ﬁnal feature of an image at inference time.\\nThis method: (1) resizes an image into multiple scales; (2)\\nextracts features from the resized images; and (3) averages\\nthe features to obtain the ﬁnal feature of the image. The\\nmethod is applied to both query and database images to en-\\nhance ranking results, especially for small target objects.\\nTable 9 compares the four cases of applying this method or\\nnot to query or database images.\\n5. Conclusion\\nWe have introduced a novel approach that extracts global\\nand local contextual information using attention mecha-\\nnisms for instance-level image retrieval. It is manifested as\\na network architecture consisting of global and local atten-\\ntion components, each operating on both spatial and chan-\\nnel dimensions. This constitutes a comprehensive study and\\nempirical evaluation of all four forms of attention that have\\npreviously been studied only in isolation. Our ﬁndings indi-\\ncate that the gain (or loss) brought by one form of attention\\nalone strongly depends on the presence of the others, with\\nthe maximum gain appearing when all forms are present.\\nThe output is a modiﬁed feature tensor that can be used in\\nany way, for instance with local feature detection instead of\\nspatial pooling for image retrieval.\\nWith the advent of vision transformers [12, 58] and their\\nrecent application to image retrieval [13], attention is ex-\\npected to play a more and more signiﬁcant role in vi-\\nsion. According to our classiﬁcation, transformers perform\\nglobal spatial attention alone. It is of great interest to in-\\nvestigate the role of the other forms of attention, where our\\n8\\napproach may yield a basic building block of such archi-\\ntectures. One may even envision an extension to language\\nmodels, where transformers originate from [50].\\nReferences\\n[1] Relja Arandjelovi´c, Petr Gronat, Akihiko Torii, Tomas Pa-\\njdla, and Josef Sivic.\\nNetVLAD: CNN architecture for\\nweakly supervised place recognition. In CVPR, 2016. 2\\n[2] Artem Babenko and Victor Lempitsky. Aggregating Local\\nDeep Features for Image Retrieval. In ICCV, 2015. 1, 2, 7\\n[3] Artem Babenko, Anton Slesarev, Alexandr Chigorin, and\\nVictor Lempitsky.\\nNeural Codes for Image Retrieval.\\nIn\\nECCV, 2014. 1, 2, 5\\n[4] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\\nand Quoc V. Le.\\nAttention augmented convolutional net-\\nworks. In ICCV, 2019. 2, 3\\n[5] Bingyi Cao, Andr´e Araujo, and Jack Sim. Unifying deep\\nlocal and global features for image search. In ECCV, 2020.\\n2, 3, 7\\n[6] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu.\\nGCNet: Non-Local Networks Meet Squeeze-Excitation Net-\\nworks and Beyond. In ICCV, 2019. 2\\n[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\\nHartwig Adam. Rethinking atrous convolution for seman-\\ntic image segmentation. arXiv preprint arXiv:1706.05587,\\n2017. 4\\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. In ICML, 2020. 1\\n[9] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\\nYan, and Jiashi Feng. Aˆ2-nets: Double attention networks.\\nIn NeurIPS, 2018. 1, 2, 3, 4\\n[10] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\\nZafeiriou. ArcFace: Additive Angular Margin Loss for Deep\\nFace Recognition. In CVPR, 2019. 6\\n[11] Michael Donoser and Horst Bischof. Diffusion Processes for\\nRetrieval Revisited. In CVPR, 2013. 1, 2, 6\\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale.\\narXiv preprint\\narXiv:2010.11929, 2020. 8\\n[13] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\\nHerv´e J´egou.\\nTraining vision transformers for image re-\\ntrieval. Technical report, 2021. 8\\n[14] Zilin Gao, Jiangtao Xie, Qilong Wang, and Peihua Li. Global\\nsecond-order pooling convolutional networks.\\nIn CVPR,\\n2019. 2, 3, 4\\n[15] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. Deep image retrieval: Learning global representations\\nfor image search. In ECCV, 2016. 2\\n[16] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. End-to-end learning of deep visual representations for\\nimage retrieval. IJCV, 2017. 1, 2, 5, 6, 7, 8\\n[17] Yinzheng Gu, Chuanpeng Li, and Jinbin Xie.\\nAttention-\\naware generalized mean pooling for image retrieval. arXiv\\npreprint arXiv:1811.00202, 2018. 2, 3, 7\\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition.\\nIn CVPR,\\n2016. 6\\n[19] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea\\nVedaldi. Gather-excite: Exploiting feature context in con-\\nvolutional neural networks. In NeurIPS, 2018. 2\\n[20] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu.\\nSqueeze-and-Excitation Networks. In CVPR, 2018. 1, 2\\n[21] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon,\\nand Ondrej Chum. Efﬁcient diffusion on region manifolds:\\nRecovering small objects with compact cnn representations.\\nIn CVPR, 2017. 2, 6\\n[22] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\\nC. Schmid. Aggregating local image descriptors into com-\\npact codes. PAMI, (99):1–1, 2011. 2\\n[23] Albert Jimenez, Jose M. Alvarez, and Xavier Gir´o-i-Nieto.\\nClass weighted convolutional features for visual instance\\nsearch. In BMVC, 2017. 2, 3\\n[24] Yannis Kalantidis, Clayton Mellina, and Simon Osindero.\\nCrossdimensional weighting for aggregated deep convolu-\\ntional features. In ECCV, 2016. 1, 2, 3, 7\\n[25] Hyo Jin Kim, Enrique Dunn, and Jan-Michael Frahm.\\nLearned Contextual Feature Reweighting for Image Geo-\\nLocalization. In CVPR, 2017. 1, 2, 3\\n[26] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak.\\nProxy anchor loss for deep metric learning. In CVPR, 2020.\\n1\\n[27] David G. Lowe.\\nDistinctive image features from scale-\\ninvariant keypoints. In IJCV, 2004. 1, 2\\n[28] Tony Ng, Vassileios Balntas, Yurun Tian, and Krystian\\nMikolajczyk. SOLAR: Second-Order Loss and Attention for\\nImage Retrieval. In ECCV, 2020. 1, 2, 3, 5, 6, 7\\n[29] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand,\\nand Bohyung Han. Large Scale Image Retrieval with Atten-\\ntive Deep Local Features. In ICCV, 2017. 1, 2, 3, 5, 6, 8\\n[30] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio\\nSavarese. Deep metric learning via lifted structured feature\\nembedding. In CVPR, 2016. 1\\n[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\\nAndreas K¨opf, Edward Yang, Zach DeVito, Martin Raison,\\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\\nFang, Junjie Bai, and Soumith Chintala. PyTorch: An im-\\nperative style, high-performance deep learning. In NeurIPS,\\n2019. 5\\n[32] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Object retrieval with large vocabularies\\nand fast spatial matching. In CVPR, 2007. 2, 5\\n[33] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Lost in quantization:Improving particu-\\nlar object retrieval in large scale image databases. In CVPR,\\n2008. 5\\n9\\n[34] Tobias Pl¨otz and Stefan Roth. Neural nearest neighbors net-\\nworks. In NeurIPS, 2018. 2, 3\\n[35] Filip Radenovi´c, Ahmet Iscen, Giorgos Tolias, Yannis\\nAvrithis, and Ondˇrej Chum. Revisiting Oxford and Paris:\\nLarge-Scale Image Retrieval Benchmarking. In CVPR, 2018.\\n5, 6, 7\\n[36] Filip Radenovi´c, Giorgos Tolias, and Ondˇrej Chum. CNN\\nimage retrieval learns from BoW: Unsupervised ﬁne-tuning\\nwith hard examples. In ECCV, 2016. 2, 7\\n[37] Filip Radenovi´c, Giorgos Tolias, and Ondˇrej Chum. Fine-\\nTuning CNN Image Retrieval with No Human Annotation.\\nIn TPAMI, 2019. 1, 2, 5, 6, 7\\n[38] Ali Sharif Razavian, Josephine Sullivan, Stefan Carlsson,\\nand Atsuto Maki. Visual Instance Retrieval with Deep Con-\\nvolutional Networks. In CoRR, 2015. 2\\n[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\\nLi Fei-Fei. ImageNet Large Scale Visual Recognition Chal-\\nlenge. In International booktitle of Computer Vision, 2015.\\n5\\n[40] Oriane Sim´eoni, Yannis Avrithis, and Ondrej Chum. Local\\nfeatures and visual words emerge in activations. In CVPR,\\n2019. 2, 6\\n[41] O. Sim´eoni, A. Iscen, G. Tolias, Y. Avrithis, and O. Chum.\\nGraph-based particular object discovery. Machine Vision and\\nApplications, 30(2):243–254, 3 2019. 3\\n[42] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototyp-\\nical networks for few-shot learning. In NeurIPS, 2017. 1\\n[43] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich.\\nGoing deeper with\\nconvolutions. In CVPR, 2015. 3\\n[44] Mingxing Tan, Ruoming Pang, and Quoc V. Le. EfﬁcientDet:\\nScalable and Efﬁcient Object Detection. In CVPR, 2020. 5\\n[45] Marvin Teichmann, Andre Araujo, Menglong Zhu, and Jack\\nSim.\\nDetect-to-retrieve: Efﬁcient regional aggregation for\\nimage search. In CVPR, 2019. 2\\n[46] Giorgios Tolias, Yannis Avrithis, and Herv´e J´egou. To aggre-\\ngate or not to aggregate: Selective match kernels for image\\nsearch. In ICCV, 2013. 2\\n[47] Giorgos Tolias, Tomas Jenicek, and Ondˇrej Chum. Learn-\\ning and aggregating deep local descriptors for instance-level\\nrecognition. In ECCV, 2020. 2, 3\\n[48] Giorgos Tolias, Ronan Sicre, and Herv´e J´egou. Particular ob-\\nject retrieval with integral max-pooling of CNN activations.\\nIn ICLR, 2016. 2\\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 4\\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 9\\n[51] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wang-\\nmeng Zuo, and Qinghua Hu.\\nECA-Net: Efﬁcient Chan-\\nnel Attention for Deep Convolutional Neural Networks. In\\nCVPR, 2020. 2, 3, 4\\n[52] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\\ning He. Non-local Neural Networks. In CVPR, 2018. 1, 2,\\n3, 4\\n[53] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.\\nGoogle Landmarks Dataset v2 - A Large-Scale Benchmark\\nfor Instance-Level Recognition and Retrieval.\\nIn CVPR,\\n2020. 1, 2, 5, 6, 7\\n[54] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So\\nKweon. CBAM: Convolutional Block Attention Module. In\\nECCV, 2018. 1, 2, 4, 8\\n[55] Fan Yang, Ryota Hinami, Yusuke Matsui, Steven Ly, and\\nShin’ichi Satoh. Efﬁcient image retrieval via decoupling dif-\\nfusion into online and ofﬂine processing. In AAAI, 2019. 2,\\n6\\n[56] Shuhei Yokoo, Kohei Ozaki, Edgar Simo-Serra, and Satoshi\\nIizuka. Two-stage Discriminative Re-ranking for Large-scale\\nLandmark Retrieval. In arXiv:2003.11211, 2020. 6, 8\\n[57] Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated\\nresidual networks. In CVPR, 2017. 4\\n[58] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\\nto-token vit: Training vision transformers from scratch on\\nimagenet. arXiv preprint arXiv:2101.11986, 2021. 8\\n[59] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\\nself-attention for image recognition. In CVPR, 2020. 2, 3\\n10\\n'), Document(metadata={'Published': '2023-06-02', 'Title': 'RITA: Group Attention is All You Need for Timeseries Analytics', 'Authors': 'Jiaming Liang, Lei Cao, Samuel Madden, Zachary Ives, Guoliang Li', 'Summary': \"Timeseries analytics is of great importance in many real-world applications.\\nRecently, the Transformer model, popular in natural language processing, has\\nbeen leveraged to learn high quality feature embeddings from timeseries, core\\nto the performance of various timeseries analytics tasks. However, the\\nquadratic time and space complexities limit Transformers' scalability,\\nespecially for long timeseries. To address these issues, we develop a\\ntimeseries analytics tool, RITA, which uses a novel attention mechanism, named\\ngroup attention, to address this scalability issue. Group attention dynamically\\nclusters the objects based on their similarity into a small number of groups\\nand approximately computes the attention at the coarse group granularity. It\\nthus significantly reduces the time and space complexity, yet provides a\\ntheoretical guarantee on the quality of the computed attention. The dynamic\\nscheduler of RITA continuously adapts the number of groups and the batch size\\nin the training process, ensuring group attention always uses the fewest groups\\nneeded to meet the approximation quality requirement. Extensive experiments on\\nvarious timeseries datasets and analytics tasks demonstrate that RITA\\noutperforms the state-of-the-art in accuracy and is significantly faster --\\nwith speedups of up to 63X.\"}, page_content='RITA: Group Attention is All You Need for Timeseries Analytics\\nJiaming Liang\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nliangjm@seas.upenn.edu\\nLei Cao∗\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nlcao@csail.mit.edu\\nSamuel Madden\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nmadden@csail.mit.edu\\nZachary Ives\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nzives@cis.upenn.edu\\nGuoliang Li\\nTsinghua University\\nBeijing, China\\nliguoliang@tsinghua.edu.cn\\nABSTRACT\\nTimeseries analytics is of great importance in many real-world\\napplications. Recently, the Transformer model, popular in natu-\\nral language processing, has been leveraged to learn high quality\\nfeature embeddings from timeseries, core to the performance of\\nvarious timeseries analytics tasks. However, the quadratic time and\\nspace complexities limit Transformers’ scalability, especially for\\nlong timeseries. To address these issues, we develop a timeseries an-\\nalytics tool, RITA, which uses a novel attention mechanism, named\\ngroup attention, to address this scalability issue. Group attention dy-\\nnamically clusters the objects based on their similarity into a small\\nnumber of groups and approximately computes the attention at\\nthe coarse group granularity. It thus significantly reduces the time\\nand space complexity, yet provides a theoretical guarantee on the\\nquality of the computed attention. The dynamic scheduler of RITA\\ncontinuously adapts the number of groups and the batch size in the\\ntraining process, ensuring group attention always uses the fewest\\ngroups needed to meet the approximation quality requirement. Ex-\\ntensive experiments on various timeseries datasets and analytics\\ntasks demonstrate that RITA outperforms the state-of-the-art in\\naccuracy and is significantly faster — with speedups of up to 63X.\\n1\\nINTRODUCTION\\nMotivation. Many data driven applications involve processing\\nmassive timeseries data, including IoT [11], medical AI [14], stock\\nmarket [27], and so on. As such, there is a great need for timeseries\\nanalytics, such as forecasting [8], classification [20], clustering [31],\\nsimilarity search [39], and anomaly detection [50], with applications\\nranging from automatically diagnosing diseases [5], recognizing\\nhuman activities [29], to stopping financial fraud [59].\\nEffective feature extraction [40] lies at the core of almost all\\nthese timeseries analytics tasks. Recently researchers [61] have\\nstarted leveraging the self-supervised pre-training methodology of\\nTransformers [4, 16, 52], which have proven remarkably successful\\nin natural language processing (NLP), to automatically learn high\\nquality feature embeddings from timeseries. In NLP, self-supervised\\npre-training exploits the sequential patterns (correlations) among\\nthe words in sentences to produce contextualized feature embed-\\ndings. Timeseries bear similarity to natural language, because in\\ntimeseries data the sequential order among the values (stock price,\\nvolume, etc.) over time matters. That is, each value is highly cor-\\nrelated with other values observed before or after it. Therefore,\\n∗Corresponding Author\\npre-training a Transformer model which takes the correlations\\namong different observations into account is a natural idea to learn\\nfeature embeddings from timeseries. Indeed, the experiments in [61]\\nconfirm that Transformer-based methods outperform traditional\\ntimeseries analytics techniques.\\nHowever, existing work [61] that directly applies Transformers\\nto learn features from timeseries data have been shown not to be\\nscalable to long timeseries [30]. The idea of self-attention [52] is\\ncentral to pre-training methods in NLP: It computes pairwise cor-\\nrelations among different semantic units in a sequence (in NLP, a\\nsentence); as such, it has quadratic time and space complexity in\\nthe length of the input sequence. Such an approach places limits on\\nthe model’s scalability, especially when handling large sequences,\\nwhich are common in real-world timeseries applications such as\\nIoT, medical AI, and finance [6, 34, 62]. Predictions about timeseries\\nmay need to look at months or years of historical data to make ac-\\ncurate predictions, spanning hundreds of thousands of samples. As\\nan example, in collaboration with a research hospital we have been\\ndeveloping a seizure classifier that automatically detects seizures\\nbased on EEG signals (timeseries) collected during the clinical ob-\\nservation of patients. As seizures last only a few seconds, we chunk\\nlong EEG data into many 2 second segments and detect seizures at\\na segment level. However, the classification of a particular segment\\ndepends on up to 12 hours of prior signal to determine if one 2\\nsecond segment indicates seizure or not, because seizure diagnosis\\nneeds to consider long-term trends in the EEG data [6]. The number\\nof segments in 12 hours is more than 21k. This is far larger than\\nthe number of semantic units the typical NLP tasks expect. For\\nexample, BERT [16] limits the number of units to 512 and even\\nmassive models like GPT-3 [4] limit the number of units to 2048.\\nAlthough in NLP some lower-complexity methods have been\\nproposed to approximately compute self-attention [10, 26, 54], their\\nperformance degrades dramatically when used on timeseries, due\\nto the gap between natural language and timeseries, as we will\\nshow in our experiments.\\nProposed Approach. To tackle the aforementioned problem, we\\ndevelop RITA, a Transformer-based timeseries analytics tool, which\\nuses a novel attention mechanism, called group attention, to scale\\nto long timeseries.\\nLeveraging the periodicity of timeseries, RITA chunks the input\\ntimeseries into segments and dynamically clusters the segments\\ninto a small number (denoted as 𝑁) of groups. Segments in the\\nsame group possess similar feature embeddings during the current\\ntraining iteration, thus enabling them to approximately share the\\n1\\narXiv:2306.01926v1  [cs.LG]  2 Jun 2023\\ncomputation of attention. As the timeseries increases in length,\\nmore sharing opportunities become available. RITA then computes\\nthe self-attention at a group level and produces a compressed group\\nattention matrix. In this way, group attention eliminates both com-\\nputation and memory bottlenecks in Transformer-style models and\\nthus more scalable to long timeseries.\\nHowever, making this idea effective and efficient in Transformer\\narchitectures is challenging for several reasons:\\n• Efficiently Producing High Quality Feature Embeddings.\\nAlthough RITA computes the attention matrix at a group level, to\\npreserve the quality of the feature embeddings, it still has to pro-\\nduce different embeddings for different segments. This is because\\neven if some segments share the attention score temporally, it does\\nnot mean they should have the same feature embedding. However,\\nusing the group attention matrix, the existing self-attention mech-\\nanism will only produce a single feature vector for each group. A\\nnaive solution would be to restore the original attention matrix\\nfrom the group attention matrix. However, in this case we again\\nget an attention matrix with quadratic space complexity. Because\\nGPUs have limited memory, GPU memory will remain a bottleneck\\nin group attention.\\n• The Number of Groups N. In RITA, the number of groups\\n𝑁is a crucial factor that balances the speed up and the quality of\\nattention approximation. A small 𝑁will lead to a large speedup,\\nbut the approximation errors can also be significant. On the other\\nhand, although a large 𝑁tends to produce high-quality approxima-\\ntions, it inevitably slows down the training process. Therefore, an\\nappropriate 𝑁is essential to the performance of group attention.\\nHowever, 𝑁depends on the distributional properties of the dataset.\\nFurthermore, like the classical transformer models, RITA stacks\\nmultiple attention layers to produce better embeddings. Ideally,\\ndifferent layers should also use different values of 𝑁. In addition,\\nduring the model training phrase, group attention should use dif-\\nferent values of 𝑁at different iterations to adapt to the varying\\nfeature embeddings. This makes manually setting appropriate 𝑁\\nalmost impossible.\\n• Batch Size. Moreover, as we want to dynamically adjust 𝑁\\nduring training, a fixed batch size is sub-optimal: as 𝑁decreases,\\nthe memory usage of a single sample decreases. This allows a larger\\nbatch size which is beneficial, because: (1) it makes full use of GPU\\nmemory; (2) high-parallelism across the samples in a big batch\\nbrings better performance. Our experimental study shows that\\ndoubling the batch size reduces the training time by 30%, while still\\npreserving the quality of the model. Thus, RITA should dynamically\\nadjust batch size as 𝑁changes.\\nTo address the above problems, we first propose an embedding\\naggregation strategy and a customized group softmax function to\\nreplace the classical softmax function [52]. Together they ensure\\nRITA is able to directly use the compressed attention matrix to\\nproduce different feature embeddings for different segments. We\\ntheoretically show the embeddings RITA produces in this way are\\nidentical to those produced by first re-storing the original large\\nattention matrix. Thus RITA is able to produce high quality embed-\\ndings without introducing extra overhead. Further, we design a GPU\\nfriendly algorithm to group the segments in parallel, effectively\\nminimizing the grouping cost.\\nP0\\nPosition\\nEmbedding\\nW1\\n+\\n+\\n+\\nWindow \\nEmbedding\\n+\\nE0\\nRaw\\nTimeseries\\nTime-aware \\nConvolution\\nW[CLS]\\nW2\\n⊗\\n.....\\nWn\\nP1\\nP2\\n.....\\nPn\\n.....\\nE1\\nE2\\nEn\\n.....\\nO0\\nO1\\nO2\\nOn\\n.....\\nRITA Encoder\\nScale & Input\\nFigure 1: RITA Architecture\\nSecond, we design an adaptive scheduler which dynamically de-\\ncides an appropriate 𝑁for each group attention layer during the\\ntraining process. It starts with a large 𝑁and iteratively merges\\ngroups that are similar to each other. Guided by an error bound on\\nthe approximated self-attention that users can tolerate, it automati-\\ncally determines if two groups are mergeable, performing merging\\nefficiently in a GPU-friendly way.\\nMoreover, we propose a learning-based method to model the\\ncorrelation between the number of groups 𝑁and the batch size 𝐵.\\nThis model is used to predict 𝐵for a given 𝑁when training RITA.\\nSpecifically, we first sample some 𝑁values in a reasonable range.\\nFor each sampled 𝑁, we find a batch size that consumes up to a\\ncertain percentage of GPU memory in a cost-efficient way. Using a\\nsmall set of mathematical functions as a prior, RITA learns a model\\nwith only a few <N, B> pairs as ground truth labels.\\nOur experiments on public timeseries benchmarks and the MGH\\nEEG data [6] confirm that RITA outperforms state-of-the-art meth-\\nods in accuracy on various timeseries analytics tasks, while our\\ngroup attention mechanism achieves a 63X speedup with much\\nless memory required, compared to existing self-attention mecha-\\nnisms [10, 52, 54].\\nContributions. The key contributions of this work include:\\n• Our group attention mechanism leverages the periodicity of\\ntimeseries, reducing the time and space complexity of the self-\\nattention mechanism with accuracy guarantees, allowing RITA to\\nscale to long timeseries data.\\n• Guided by an approximation error bound, our adaptive sched-\\nuler dynamically adapts the number of groups and the batch size\\nto the distribution properties of the evolving feature embeddings,\\nmaking group attention efficient and easily tunable.\\n• We conduct experiments on various datasets and different ana-\\nlytics tasks, demonstrating that RITA is 4 to 63 times faster than\\nthe state-of-the-art while achieving better accuracy when handling\\nlong timeseries (length ≥2000).\\n2\\n2\\nBACKGROUND\\nWe provide some background on the canonical self-attention mod-\\nule in the Transformer[52]. A self-attention module takes 𝑛hidden\\nembedding vectors 𝐻∈R𝑛∗𝑑ℎas input, then projects them to\\nqueries (𝑄), keys (𝐾) and values (𝑉) and performs Scaled-dot Prod-\\nuct Attention, which given input hidden state 𝐻, is computed by:\\n𝑄= 𝐻𝑊𝑄, 𝐾= 𝐻𝑊𝐾,𝑉= 𝐻𝑊𝑉\\n𝑂= 𝐴𝑉= 𝑆𝑜𝑓𝑡𝑀𝑎𝑥( 𝑄𝐾𝑇\\n√︁\\n𝑑𝑘\\n)𝑉\\n(1)\\nWhere 𝑊𝑄∈R𝑑ℎ∗𝑑𝑘,𝑊𝐾∈R𝑑ℎ∗𝑑𝑘,𝑊𝑉∈R𝑑ℎ∗𝑑𝑣are projection\\nmatrices for generating 𝑄, 𝐾,𝑉. 𝑄∈R𝑛∗𝑑𝑘is also regarded as the\\npacking of 𝑛query vectors {𝑞1, ...,𝑞𝑛} with dimension 𝑑𝑘into a\\nmatrix. 𝐾∈R𝑛∗𝑑𝑘,𝑉∈R𝑛∗𝑑𝑣are regarded as the packing of key\\nvectors {𝑘1, ...,𝑘𝑛} and value vectors {𝑣1, ..., 𝑣𝑛} in the same way.\\nGiven a matrix 𝑀∈R𝐿∗𝑛, the softmax function normalizes 𝑀\\nto ensure the sum of each row equals to 1, as shown below.\\n𝑆𝑜𝑓𝑡𝑀𝑎𝑥(𝑀𝑖,𝑗) =\\n𝑒𝑥𝑝(𝑀𝑖,𝑗)\\nÍ𝑛−1\\n𝑘=0 𝑒𝑥𝑝(𝑀𝑖,𝑘)\\n(2)\\nNote the attention matrix A is an 𝑛×𝑛matrix, where 𝑛represents\\nthe number of elements in the input sequence (e.g. words in NLP).\\n3\\nRITA OVERVIEW\\nGiven a collection of unlabeled timeseries, RITA first pre-trains\\na Transformer-style model to produce high quality feature em-\\nbeddings for timeseries data. This pre-trained model is then used\\nto support various downstream tasks, similar to BERT [16]. Next,\\nwe overview the model architecture of RITA. We show how RITA\\nsupports various downstream tasks in Appendix A.7.\\nAs shown in Fig. 1, RITA is consist of two components: (1) Time-\\naware Convolution Layer (2) RITA Encoder.\\nTime-aware Convolution Layer fills the gap between timeseries\\nand natural language. Despite their high-level similarity, there is a\\nbig gap between timeseries and natural language. First, in natural\\nlanguage each word, as a discrete semantic unit, has an indepen-\\ndent meaning, while each element in a timeseries is a continuous,\\nnumerical value and does not necessarily constitute an independent\\nevent. Furthermore, the input sequences are single-channeled in\\nNLP, but often multi-channeled in timeseries (i.e., sensor data often\\nconsists of several related channels).\\nRITA leverages the classical convolution [28] strategy to solve\\nthis problem. Convolution is widely used to capture the local struc-\\ntures of an image. We use convolution to chunk one input timeseries\\ninto a sequence of windows and learn the local structure of each\\nwindow, similar to the discrete semantic units in natural language.\\nIt also discovers the correlations across different channels, thus\\nnaturally solving the multi-channel problem.\\nMore specifically, treating a multi-variate timeseries of length 𝑛\\nand with𝑚variables as an n × m matrix𝑇, RITA uses𝑑convolution\\nkernels to chunk𝑇into n windows and produce one d-dimensional\\nembedding per window using the convolution operation [28]. Each\\nconvolution kernel corresponds to a w × m matrix, where 𝑤defines\\nthe number of timestamps that each convolution kernel covers,\\nidentical to the window size in sliding window.\\nRITA Encoder functions as Transformer Encoder as described in\\nthe original Transformer work[52]. It takes the embeddings of 𝑛\\nsemantic units 𝑋1,𝑋2, ...,𝑋𝑛(𝑋𝑖∈𝑅𝑑) as input (e.g. embeddings of\\n𝑛windows for a timeseries), then models the correlations between\\nthe semantic units and outputs 𝑌1, ...,𝑌𝑛(𝑌𝑖∈𝑅𝑑) as the context-\\naware embedding of each unit.\\nWhat makes RITA Encoder different from Transformer Encoder\\nis that: at the core of Transformer Encoder lies self-attention mech-\\nanism which incurs a 𝑂(𝑛2) time complexity and memory usage.\\nThis quadratic cost becomes prohibitive for long timeseries and\\nlimits the scalablity of Transformer-based models. To make the\\nattention computation efficient yet high-quality, we replace the\\ncanonical self-attention with our proposed group attention.\\nSelf-supervised Pretraining. Inspired by the “cloze text” pre-\\ntraining task in NLP, we designed a mask-and-predict task as the\\npretraining task for our model. The timeseries is randomly masked\\nand the model should recover the masked values based on corre-\\nsponding contextual information.\\nTo be specific, we generate masks on time-stamps, with a mask\\nrate 𝑝. The timeseries is scaled to be non-negative and the values\\nacross all the channels on the masked timestamps are set to be -1,\\nan impossible value on normal timestamps. Then the masked time-\\nseries is fed into RITA and the output representation is translated\\nto the recovered timeseries by a Transpose Convolution layer.\\n4\\nGROUP ATTENTION MECHANISM\\nGroup attention, a novel and efficient approximate attention mecha-\\nnism, addresses the performance bottleneck of self-attention in the\\nvanilla Transformer. In this section, we first introduce the frame-\\nwork of group attention and then theoretically establish the bound\\nof its approximation error.\\n4.1\\nThe Idea of Group Attention\\nAs periodicity is a natural property of timeseries [56], similar\\nwindows frequently occur. Similar windows result in similar\\nqueries/keys for attention computation, bringing opportunities for\\nsaving computation.\\nAs discussed in Sec. 2, 𝐴𝑖𝑗, the attention score of window 𝑖onto\\nwindow 𝑗, is determined by the inner product between the query\\nvector of window 𝑖and the key vector of window 𝑗, that is, 𝑞𝑖· 𝑘𝑗.\\nGiven another window 𝑥, if window 𝑥has the similar key vector\\nto window 𝑗, that is, 𝑘𝑗≈𝑘𝑥, then 𝑞𝑖· 𝑘𝑗≈𝑞𝑖· 𝑘𝑥. In other words,\\n𝐴𝑖𝑗≈𝐴𝑖𝑥when 𝑘𝑗≈𝑘𝑥.\\nThis observation inspires our group attention mechanism. That\\nis, we group the windows by their similarity in keys. Assuming\\nall windows in the same group have the same attention score onto\\nanother window 𝑘, we then only compute the attention once by\\nusing one single key to represent this group, for example the centroid\\nof the group of keys. This thus saves significant computation cost.\\nBetter yet, after grouping 𝑛windows into 𝑁groups, group atten-\\ntion compresses the attention matrix from an𝑛×𝑛matrix to an𝑛×𝑁\\nmatrix. Because 𝑁(number of groups) tends to be much smaller\\nthan 𝑛(number of windows) due to the periodicity of timeseries,\\ngroup attention consumes much less memory than the original\\nself-attention mechanism, successfully eliminating the memory\\nbottleneck. Note that it also doesn’t hurt quality all that much, as\\nconfirmed in our experiments (Sec. 6.2).\\n3\\nGrouping\\nAverage\\nK\\nQ\\nMatMul\\nAttention Matrix\\nWeighted\\nSoftMax\\nV\\nSum\\nAggregate\\nTranspose\\nMatMul\\nOutput\\nQ \\nK \\nV\\nFigure 2: Group Attention\\n4.2\\nComputing the Output Feature Embedding\\nWe now discuss how to efficiently compute the output feature\\nembeddings using the small compressed group attention matrix.\\n4.2.1\\nProblem: Producing Embeddings w/ Group Attention Matrix\\nAs described in the Background, once we have acquired the at-\\ntention matrix 𝐴, canonical self-attention computes the output\\nembedding 𝑂as O = AV. Because 𝐴is an 𝑛× 𝑛matrix and 𝑉is an\\n𝑛×𝑑𝑣matrix, the matrix product operation still produces an 𝑛×𝑑𝑣\\nmatrix 𝑂. That is, it produces a 𝑑𝑣dimensional feature vector for\\neach window. However, our group attention will produce an 𝑛× 𝑁\\nattention matrix e\\n𝐴, where 𝑁corresponds to the number of groups.\\nIn this case the matrix product will produce a 𝑁×𝑑𝑣matrix e\\n𝑂. That\\nis, it produces a feature vector for each group. However, our goal\\nis to produce different embeddings for different windows, because\\neven if some windows share the attention score temporally, it does\\nnot mean they should have the same feature embedding.\\nA Naive Solution. A naive solution would be to restore the full\\nattention matrix 𝐴from the group attention matrix e\\n𝐴. For example,\\ngiven one group composed of 𝑤𝑖𝑛𝑖and 𝑤𝑖𝑛𝑗, we map its group\\nattention vector in e\\n𝐴into two rows that correspond to 𝑤𝑖𝑛𝑖and\\n𝑤𝑖𝑛𝑗in 𝐴. However, in this case we again get a 𝑛× 𝑛attention\\nmatrix; and GPU memory remains a bottleneck in group attention.\\n4.2.2\\nSolution: Embedding Aggregation and Group SoftMax\\nUsing an embedding aggregation operation and a group softmax\\nfunction, RITA produces 𝑛embeddings without restoring the full\\nattention matrix. Fig. 2 shows the workflow of group attention.\\nEmbedding Aggregation. The idea is inspired by the observation\\non the matrix product operation O = AV conducted on the fully\\nrestored attention matrix 𝐴.\\nGiven an element𝑂𝑖,𝑗of𝑂corresponding to the 𝑗𝑡ℎdimension of\\n𝑤𝑖𝑛𝑖’s feature vector,𝑂𝑖,𝑗= 𝑎𝑖·𝑣𝑗, where vector ai ∈Rn denotes the\\n𝑖𝑡ℎrow of the attention matrix 𝐴and vector vj ∈Rn denotes the 𝑗𝑡ℎ\\ndimension of all the 𝑛feature vectors. Given ai =< a1\\ni , a2\\ni , · · · , an\\ni >\\nand vj =< v1\\nj , v2\\nj , · · · , vn\\nj >, 𝑂𝑖,𝑗= Ín\\nk=1 ak\\ni vk\\nj .\\nAs an example, assume 𝑤𝑖𝑛1 and 𝑤𝑖𝑛2 belong to the same group\\n𝐺1. Then 𝑎1\\n𝑖= 𝑎2\\n𝑖= e𝑎1\\n𝑖, where e𝑎1\\n𝑖∈e\\n𝐴corresponds to the attention\\nof group 𝐺1 onto 𝑤𝑖𝑛𝑖. Therefore, 𝑎1\\n𝑖𝑣1\\n𝑗+ 𝑎2\\n𝑖𝑣2\\n𝑗= e𝑎1\\n𝑖(𝑣1\\n𝑗+ 𝑣2\\n𝑗).\\nAs an immediate generalization of the above analysis, if we ag-\\ngregate up the windows that belong to the same group and convert\\nthe n-dimensional feature vector 𝑣𝑗into a 𝑁-dimensional group fea-\\nture vectore𝑣𝑗beforehand, we could directly use the group attention\\nvector e𝑎𝑖and the group feature vector e𝑣𝑗to compute 𝑂𝑖,𝑗.\\nUsing embedding aggregation, RITA is able to produce the fea-\\nture embedding e\\n𝑂that is identical to the embedding 𝑂produced\\nby using the full attention matrix 𝐴and the embedding matrix 𝑉.\\nGroup Softmax Function. In canonical self-attention the atten-\\ntion matrix 𝐴is computed as 𝐴= SoftMax( QKT\\n√\\ndk ). To compute 𝐴,\\nwe have to first compute 𝑄𝐾𝑇(denoted as 𝑃) which is an 𝑛× 𝑛\\nmatrix. Then normalizing the 𝑃matrix with softmax produces the\\nattention matrix 𝐴.\\nGroup attention follows the same procedure. But after grouping\\nkeys into e𝐾, 𝑄e𝐾𝑇produces an 𝑛× 𝑁matrix e𝑃. Due to the non-\\nlinearity of the softmax function, applying softmax directly on e𝑃\\nwill result in a group attention matrix e\\n𝐴from which we are not able\\nto recover a full attention matrix that is identical to first restoring\\ne𝑃to 𝑃and then applying softmax on 𝑃. The 𝐴matrix produced\\nby the latter is desirable, as we want to approximate the original\\nattention matrix as accurately as possible. However, restoring the\\nsmall 𝑛× 𝑁e𝑃matrix is not memory efficient, as it will end up with\\na full 𝑛× 𝑛matrix 𝑃.\\nTo solve the above problems, we introduce a new group softmax\\nfunction to replace the original softmax function (Eq. 2).\\n𝐺𝑟𝑜𝑢𝑝𝑆𝑜𝑓𝑡𝑀𝑎𝑥(g\\n𝑃𝑖,𝑗) =\\n𝑒𝑥𝑝(𝑃𝑖,𝑗)\\nÍ𝑁−1\\n𝑘=0 𝑐𝑜𝑢𝑛𝑡𝑘𝑒𝑥𝑝(𝑃𝑖,𝑘)\\n(3)\\nIn Eq. 3, 𝑐𝑜𝑢𝑛𝑡𝑘represents the number of windows that Group\\n𝐺𝑘contains. Compared to the original softmax, our group softmax\\nconsiders each group 𝐺𝑘as 𝑐𝑜𝑢𝑛𝑡𝑘elements and counts it 𝑐𝑜𝑢𝑛𝑡𝑘\\ntimes when summing up the exponential of each group’s 𝑃𝑖,𝑘. In\\nthis way, the group softmax function operating on the small e𝑃\\nmatrix will produce exactly the same result to the softmax function\\noperating on the full 𝑃matrix.\\nTheoretical Guarantee. In Appendix A.4, we prove that the group\\nsoftmax function and the embedding aggregation operation produce\\nthe same output feature embedding with the naive method that has\\nto first restore the big full attention matrix.\\nWe show an efficient implementation of the embedding aggrega-\\ntion operation and group softmax function in Appendix A.2, Alg. 1.\\nTime Complexity. The time complexity of Alg. 1 is 𝑂(𝑛𝑁𝑑) and\\nthe space complexity is𝑂(𝑛𝑁), while the time and space complexity\\nof the original self-attention mechanism are 𝑂(𝑛2𝑑) and 𝑂(𝑛2).\\n4.3\\nError Bound\\nGroup attention produces a group attention matrix e\\n𝐴which approxi-\\nmates the attention matrix𝐴produced by the classical self-attention\\nwith a bounded error, as shown in Lemma 1.\\nLemma 1. Let 𝑅be the radius of the ball where all key vectors\\nlive; e𝑘𝑖be the representative of the group that contains key 𝑘𝑖. Let 𝐴\\ndenote the full attention matrix restored from e\\n𝐴. Suppose the distance\\nbetween e𝑘𝑖and 𝑘𝑖(||ek𝑖−k𝑖||) satisfies: ||ek𝑖−k𝑖|| ≤d.\\nThen ∀𝜖> 1, if d ≤ln(𝜖)\\n2R , 1\\n𝜖≤Ai,j\\nAi,j ≤𝜖\\nLemma 1 shows that the error bound 𝜖of the group attention is\\ndetermined by the distance 𝑑. As discussed in Sec. 5.1, it inspires\\nus to design a strategy to dynamically determine the number of\\ngroups 𝑁– the most critical parameter of group attention. Please\\nrefer to Appendix A.5 for the proof.\\n4\\n4.4\\nGPU Friendly Grouping Method\\nIn this section, we discuss the implementation of a grouping method.\\nTo make group attention efficient and effective, the grouping\\nmethod has to satisfy the following requirements:\\n(1) Tight distance bound: to ensure the approximation quality,\\nthe distance between each key and its group representative should\\nbe minimized according to Lemma 1.\\n(2) Lightweight: to ensure the performance gain, the grouping\\nmethod must be lightweight, at worst not exceeding the complexity\\nof group attention itself (𝑂(𝑁𝑛)).\\n(3) GPU friendly: to take advantage of GPUs, we prefer a group-\\ning method that mainly consists of matrix operations, which can\\nbe efficiently executed on a GPU.\\nTo satisfy the above requirements, after thorough investigation\\non various clustering algorithms, we design a GPU friendly K-\\nmeans [35] as the grouping method.\\nFirst, K-means minimizes the overall distance between any object\\nand its cluster center, hence naturally satisfying Requirement 1.\\nSecond, given 𝑁centers, in each iteration the time and space\\ncomplexity of K-means is 𝑂(𝑛𝑁). Usually, the iteration goes until\\nconvergence. However, we observe that rather than seeking a per-\\nfect K-means clustering, training a few iterations is sufficient to\\nget a good grouping for group attention, because typically the later\\niterations only slightly update the clustering and group attention\\nis robust to such imperfection.\\nThird, we design a GPU-friendly implementation of K-means.\\nThe performance bottleneck of K-means comes from the dis-\\ntance computation between each vector and its center, that is,\\n|vi −cj| =\\n√︃\\n(vi −cj)2, i ∈[1, n], j ∈[1, N]. The performance bot-\\ntleneck is 𝑣𝑖−𝑐𝑗. We instead use a different formulation: |𝑣𝑖−\\n𝑐𝑗| = |vi −cj| =\\n√︃\\n|vi|2 + |cj|2 −2vi · cj, i ∈[1, n], j ∈[1, N]. This is\\nbecause in this formulation, the performance bottleneck is 𝑣𝑖· 𝑐𝑗,\\nwhich could be implemented as a matrix product operation. Al-\\nthough the complexity of the two formulations is the same, in GPUs\\nmatrix product is much more efficient than pairwise difference.\\n5\\nADAPTIVE SCHEDULER\\nNext, we present the adaptive scheduler of RITA which addresses\\nthe challenges of determining an appropriate number of groups\\n𝑁and accordingly the batch size 𝐵, as described in Introduction.\\nUsing a dynamic scheduling method we propose, the scheduler\\nautomatically determines and adjusts 𝑁and 𝐵based on the distri-\\nbutional properties of the feature embeddings produced over the\\niterative training process, while guaranteed to produce high quality\\nattention approximation that meets the requirement of users.\\nIn Sec. 5.1 we show how RITA automatically determines 𝑁. Then\\nwe introduce in Sec. 5.2 the learning-based method which given an\\n𝑁, immediately predicts a good batch size.\\n5.1\\nDynamically Determining the Number of\\nGroups N\\nWithout loss of generality, we use one group attention module as\\nan example to show how RITA automatically gets an appropriate 𝑁.\\nThe adaptive scheduler of RITA starts with a large 𝑁and decreases\\nit dynamically. This is because in the training process of RITA, the\\nfeature embeddings produced epoch by epoch tend to get stabler\\nand stabler and gradually converge, thus no need to increase 𝑁.\\nRITA reduces the number of groups by merging similar groups.\\nIntuitively, given two groups, we could measure their similarity\\nbased on the distance of their centers. If the distance between\\ntheir centers is smaller than a distance threshold, then the two\\ngroups could be merged. However, setting an appropriate distance\\nthreshold seems hard – as difficult as setting an appropriate 𝑁.\\nTo solve this problem, RITA leverages the error bound of group\\nattention introduced in Sec. 4.3. It only requires users to set an\\nerror bound 𝜖, and then uses Lemma 1 to translate 𝜖to a distance\\nthreshold 𝑑. RITA then uses Lemma 2 to determine if merging some\\ngiven clusters still meets the error bound threshold 𝜖.\\nLemma 2. Denote 𝑐𝑘to be the cluster center of 𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘. Assume\\nthe existing grouping satisfies ∀k,\\nmax\\nx∈clusterk\\n|ck −x| ≤d , thus satis-\\nfying an error bound 𝜖by Lemma 1. If there exist 𝑚clusters, namely,\\n𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘1,𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘2, ...,𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘𝑚, satisfying that:\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘𝑖\\n|𝑐𝑘𝑖−𝑐𝑘𝑗| + |𝑥−𝑐𝑘𝑖| ≤𝑑,𝑖, 𝑗∈[1,𝑚]\\n(4)\\nmerging them into one cluster still meets the error bound 𝜖.\\nPlease refer to Appendix A.6 for the proof.\\nFinding the Mergable Clusters. We formulate the problem of\\nfinding mergeable clusters using graph theory:\\n(1) each cluster is a node in the graph;\\n(2) if 𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖and 𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗satisfy:\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖\\n|𝑐𝑖−𝑐𝑗|+|𝑥−𝑐𝑖| ≤𝑑, and\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗\\n|𝑐𝑗−𝑐𝑖|+|𝑥−𝑐𝑗| ≤𝑑\\nthere is an undirected edge between 𝑛𝑜𝑑𝑒𝑖and 𝑛𝑜𝑑𝑒𝑗;\\nIn this scenario, finding the maximum number of mergeable\\nclusters is equivalent to finding the minimal clique cover in the\\ncorresponding graph, which is an NP-hard problem [24]. Such\\nheavy computation overhead is not acceptable for RITA. We thus\\noffer a simplified solution:\\n(1) Halve the clusters into two sets 𝑆1,𝑆2;\\n(2) If 𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖∈𝑆1 and 𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗∈𝑆2 satisfy:\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖\\n|𝑐𝑖−𝑐𝑗| + |𝑥−𝑐𝑖| ≤𝑑,\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗\\n|𝑐𝑗−𝑐𝑖| + |𝑥−𝑐𝑗| ≤𝑑\\n2\\n(5)\\n𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗is marked.\\n(3) Decrease the number of clusters by counting the masks in 𝑆2.\\nIn this solution, clusters in 𝑆1 can be regarded as transfer nodes.\\nIf (5) holds for (𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖∈𝑆1,𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗1 ∈𝑆2) and (𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖∈\\n𝑆1,𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗2 ∈𝑆2), respectively, we have,\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗1\\n|𝑐𝑗1 −𝑐𝑗2 | + |𝑥−𝑐𝑗1 |\\n≤\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗1\\n|𝑐𝑗1 −𝑐𝑖| + |𝑐𝑖−𝑐𝑗2 | + |𝑥−𝑐𝑗1 |\\n≤\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗1\\n|𝑐𝑗1 −𝑐𝑖| + |𝑐𝑖−𝑐𝑗2 | + |𝑥−𝑐𝑗1 | + |𝑥−𝑐𝑗2 | ≤𝑑\\n(6)\\nThus (4) holds when merging several clusters in 𝑆2 with one\\ncluster in 𝑆1. As a result, we can greedily merge clusters in 𝑆2, as\\nillustrated in step(3).\\nAssume the number of clusters decreases by 𝐷after merging,\\nwe apply a momentum update [42] on the number of clusters 𝑁, as\\nis commonly used in machine learning to smooth the changing of\\n𝑁and avoid sample selection bias. To be specific: 𝑁𝑛𝑒𝑤= 𝛼(𝑁−\\n𝐷) + (1 −𝛼)𝑁, where 𝛼is a hyper-parameter for momentum.\\n5\\n5.2\\nDynamically Determining the Batch Size\\nBecause of the dynamic grouping operation, the computational\\ngraph in deep learning training [1] varies from sample to sample. As\\na result, it is impossible to precisely compute a batch’s GPU memory\\nusage without indeed feeding it into the model. To overcome this\\nproblem, RITA learns a batch size prediction function offline; then\\nat the RITA training time, given a number of groups 𝑁, RITA uses\\nthis function to predict a proper batch size.\\nWhen the model architecture and hardware are fixed, the batch\\nsize depends on the length of the timeseries 𝐿and the average\\ngroup number among all attention module 𝑁. So RITA samples\\nseveral (𝐿𝑖, 𝑁𝑖) pairs and estimate a proper batch size for each pair.\\nMore specifically, given a user-defined timeseries maximal length\\n𝐿𝑚𝑎𝑥, we randomly sample integral points (𝐿𝑖, 𝑁𝑖) from plane\\n{1 ≤𝐿≤𝐿𝑚𝑎𝑥, 1 ≤𝑁≤𝐿}. Then we use a binary search based\\nalgorithm to find the maximal batch size 𝐵𝑖that consumes less than\\n90% available GPU memory, aiming to avoid wasting GPU memory\\nand the risks of out of memory (OOM).\\nTreating these pairs as ground truth labels, we use function\\nfitting [18] to learn the batch size predicting function B = f (L, N),\\nwhere B is a function of two variables 𝐿and 𝑁.\\nLearning the Prediction Function. We apply curve fit from\\nSciPy [53] as the function fitting tool to fit the two-variable function\\n𝐵𝑖= 𝑓(𝐿𝑖, 𝑁𝑖) on plane {1 ≤𝐿≤𝐿𝑚𝑎𝑥, 1 ≤𝑁≤𝐿}.\\nWe observe that applying one function to the whole plane incurs\\na huge estimation error. So we develop a dynamic-programming\\n(DP) method to divide the plane into several sub-planes and apply\\na distinct function to each sub-plane respectively. It is optimal in\\nminimizing the total estimation error on all sub-planes\\nWith the learned prediction function 𝑓, we can estimate a proper\\nbatch size for any (𝐿, 𝑁) during training, even if it is not seen in\\nthe sampled (𝐿𝑖, 𝑁𝑖) pairs.\\nThe Algorithms and Optimality Proof. Please refer to Appen-\\ndix A.3 for the pseudo code of the binary search-based algorithm\\nand the description of the DP method for plane-division and the\\nproof for its optimality.\\n6\\nEVALUATION\\nOur experimental study focuses on the following questions:\\n1. Effectiveness and efficiency of RITA: How does RITA com-\\npare with other Transformer-based methods and traditional time-\\nseries representation learning methods in accuracy and efficiency?\\n2. Ablation Study: How do the key techniques of RITA work?\\n6.1\\nExperimental Setup\\nDatasets. We evaluate RITA on classification and imputation tasks\\nusing 5 multi-variate and 3 uni-variate timeseries datasets.\\n• WISDM [55] is a popular multivariate timeseries dataset gen-\\nerated from the accelerometer in the mobile phone. The subjects\\nperformed 18 daily activities (e.g. walking, jogging). The dataset\\nwas collected from 51 subjects and the sampling rate is 20 Hz.\\n• HHAR dataset [46] contains sensing data of accelerometer col-\\nlected from 9 users performing 5 activities with 12 different smart-\\nphones (varying in sampling rate). This increases the complexity\\nof the task and thus can test the model’s robustness.\\n• RWHAR RealWorld HAR dataset [48] covers 15 subjects per-\\nforming 8 locomotion-style activities. Each subject wears the sen-\\nsors for approximately ten minutes. The sampling rate is 50 Hz.\\n• ECG dataset [34] consists of 10,000 EEG recordings for arrhyth-\\nmia classification. Each recording has an uncertain length ranging\\nfrom 6 to 60 seconds sampled at 500 Hz. The ECG recordings corre-\\nspond to 9 types of heart problems such as atrial fibrillation (AF)\\nand premature atrial contraction (PAC), etc.\\n• MGH [6] is a EEG dataset collected by Mass. General Hospital.\\nEach timeseries corresponds to the EEG data observed from one\\npatient during their stay in ICU for a couple of days. The EEG\\nmonitoring produced data with 20 channels. The sampling rate is\\n200 HZ. So it produces very long timeseries.\\n• WISDM*/HHAR*/RWHAR* are three uni-variate datasets de-\\nrived by picking one channel from WISDM/HHAR/RWHAR.\\nTraining/Validation Data Generation. We apply a sliding win-\\ndow on the raw timeseries to get training/validation samples. The\\nsize of the sliding window is set as 200 on small datasets (WISDM,\\nHHAR, RWHAR), 2000 on medium size dataset (ECG), and 10,000\\non the large dataset (MGH). Table 1 shows the statics of the gen-\\nerated datasets. They are randomly split into training/validation\\nset in a proportion of 0.9/0.1. In “pretraining + few-label finetun-\\ning” scenario, we use 100 labeled data per class for finetuning. We\\nguarantee that training set does not overlap with the validation set.\\nDataset\\nTrain. Size\\nValid. Size\\nLength\\nChannel\\nClasses\\nWISDM\\n28,280\\n3,112\\n200\\n3\\n18\\nHHAR\\n20,484\\n2,296\\n200\\n3\\n5\\nRWHAR\\n27,253\\n3,059\\n200\\n3\\n8\\nECG\\n31,091\\n3,551\\n2000\\n12\\n9\\nMGH\\n8,550\\n950\\n10000\\n21\\nN/A\\nTable 1: The statistics of the datasets\\nAlternative Methods. We compare RITA against the SOTA Trans-\\nformer based timeseries representation learning method TST [61].\\nTo evaluate our group attention (referred to as Group Attn.), we\\ndevelop three baselines by replacing the group attention compo-\\nnent in RITA with the classic vanilla Self-Attention [52](referred\\nto as Vanilla) and two SOTA methods that reduce the complexity\\nof self-attention by approximation in NLP, namely, Performer [10]\\n(referred to as Performer) and Linformer [54] (referred to as Lin-\\nformer). Similar to our proposed Group Attn., Vanilla, Performer,\\nLinformer all use RITA’s time-aware convolution operation (Sec. 3)\\nto turn timeseries segments into input feature vectors.\\nWe also compare Group Attn. against GRAIL [40], which is\\nthe SOTA of the non-deep learning methods for timeseries repre-\\nsentation learning. GRAIL supports classification tasks by feeding\\nthe learned representations into a Support-Vector Machine [12]\\nor K-Nearest Neighbor [17] classifier. Note GRAIL only targets\\nuni-variate timeseries and cannot support imputation tasks.\\nMethodology. We mainly focus on two downstream tasks:\\n(1) Classification. First, we train Group Attn. and the base-\\nlines with full labels from scratch to test the effectiveness of RITA\\nframework and the approximation quality of our group attention.\\nSecond, to measure the effectiveness of self-supervised pretrain-\\ning, we evaluate the accuracy of training on few labeled timeseries\\nwith/without pretraining on large scales of unlabeled timeseries. To\\nbe specific, we split the training set into a pretraining set and a fine-\\ntuning set, with very few data in the latter (100 labeled samples per\\n6\\n(a) Effectiveness \\n(b) Efficiency\\nTraining Time/sec\\nFigure 3: Full-label classification results (multi-variate data).\\nclass in our experiment). We train the model on the cloze pretrain-\\ning task with a mask rate 𝑝= 0.2. Then we train two classification\\nmodels using the finetuning set, either based on the pretrained\\nversion or from scratch. We repeat the experiment 5 times with\\nrandom data splits and report the median accuracy.\\n(2) Imputation. We run the imputation task on the datasets used\\nin classification as well as the large unlabeled MGH dataset, and\\nmeasure the mean square error and absolute imputation error. To\\nget timeseries with missing values, we randomly mask the values\\nwith an expected mask rate of 𝑝= 0.2. The masked values are\\nreplaced with a special value.\\nFinally, to evaluate Group Attn.’s benefit on efficiency, the total\\ntime of forward computation, backward propagation, and grouping\\nare measured for all methods in all the experiments.\\nTo save space, we only report the average training time per epoch\\nhere and refer readers to Appendix A.8 for the inference time.\\nWe first compare against the Transformer-based methods on\\nmulti-variate datasets (sec. 6.2, 6.3), then compare against the non-\\ndeep learning method GRAIL on uni-variate datasets (sec. 6.4).\\nConfiguration. Please refer to Appendix A.1 for the experiment\\nconfiguration and hyper-parameter settings.\\n6.2\\nEffectiveness: Transformer-Based Methods\\nWe first evaluate the quality of the models trained with full labels\\nfrom scratch. We then show how the pretraining of RITA increases\\nthe accuracy of the downstream tasks.\\n6.2.1\\nfull-label training (Multi-variate classification)\\nResults shown in Figure 3(a) get us the following observations:\\n(1) RITA’s advantage over TST. On all four datasets for the clas-\\nsification tasks, Group Attn. and the other three baselines that use\\nRITA architecture (Vanilla, Performer, and Linformer) outperform\\nTST. In particular, Group Attn. outperforms TST by 49 percentage\\npoints on the ECG dataset (88.48% vs 39.93%) with long timeseries.\\nTwo deficiencies in TST may cause its poor performance on the long\\ntimeseries. Firstly, TST concatenates the output embedding vector\\nof each time stamp, then uses a linear classifier to do classification\\non the concatenated vector. When the timeseries is long, the linear\\nclassifier has so many parameters that it tends to overfit easily.\\nSecondly, TST replaces Layer Normalization in vanilla Transformer\\nwith Batch Normalization. When the timeseries is long, it can only\\naccommodate a small number of timeseries in each batch, leading\\nto bias in Batch Normalization.\\n(2) Group-attention’s advantage over other attention mech-\\nanisms. Group Attn. is better than Performer and Linformer on\\n3 out of 4 datasets for classification. Although Linformer works\\nslightly better than Group Attn. on the ECG dataset (90.37% vs\\n88.84%), its performance is the worst in all other cases compared\\nto any other RITA-based methods. Vanilla computes the attention\\nscores precisely. Thus it is expected to work well. However, Group\\nAttn. outperforms Vanilla on WISDM (87.50% vs 86.95%) and is very\\nclose to it on other 3 datasets. This suggests that group attention’s\\napproximation quality is good.\\n6.2.2\\npretraining + few label finetune (Multi-variate classification)\\nThe results shown in Table 3 get us the following observation:\\n(1) Pretraining is effective. Pretraining always leads to better\\naccuracy than training with a few labels from scratch. In particular,\\non WISDM data all the methods using RITA architecture increase\\nthe accuracy by at least 10%. This is impressive considering we do\\nnot have a very large unlabeled pre-training set to use.\\n(2) RITA’s advantage over TST. our Group Attn. and other\\nthree baselines using RITA architecture (Vanilla, Performer, and\\nLinformer) significantly outperform TST on all four classification\\ndatasets by 25 percentage points.\\n(3) Group Attention’s advantage over other attention mech-\\nanisms. Group Attn. is better than Performer and Linformer on 3\\nout of 4 datasets. When compared to Vanilla, Group Attn. is better\\non HHAR and ECG, and comparable on the other two, further con-\\nfirming its high quality on approximation. Further, we notice that\\nLinformer struggles in this setting: in average its accuracy is worse\\nthan Vanilla by 8.22% and Group Attn. by 8.01%. This is because the\\nlow-rank projection operation introduces extra model parameters,\\nmaking Linformer more easily overfit, while overfitting is especially\\nharmful when there are only a few labeled training samples.\\n6.2.3\\nfull-dataset training (Multi-variate imputation)\\nSimilar to classification tasks, the results of imputation tasks\\n(Table.2) show that Group Attn. consistently outperforms the base-\\nlines in training time while achieving comparable/better MSE. Again,\\non the large dataset MGH (length = 10,000), TST and Vanilla fail due\\nto out of memory (OOM) errors. Methods using RITA framework\\n(Group Attn., Performer, Linformer) all achieve very low MSE (are\\nhighly accurate). Among them Linformer is the worst.\\n6.3\\nEfficiency: Transformer-based Methods\\nWe measure the efficiency by the average training time per epoch\\nincluding the cost of the forward computation + backward propaga-\\ntion and the grouping overhead. We first show the results on all the\\n5 datasets in Sec. 6.3.1. We then vary the length of the timeseries\\non the MGH dataset to show group attention’s scalability on long\\ntimeseries in Sec. 6.3.2.\\n6.3.1\\nTraining Time: All Multi-variate Datasets\\nThe results in Fig. 3(b) and Table 2 lead to the below observations:\\n(1) Vanilla Self-Attention is not scalable. In average, it takes\\n2-3 minutes to train one epoch when the length of the timeseries is\\nonly 200 (WISDM, HHAR, RWHAR), takes over 15 minutes when\\nthe length increases to 2,000 (ECG), and fails on the long MGH data\\nwhen the length reaches 10,000 due to out of GPU memory.\\n(2) Group Attn.’s advantage over all other attention mecha-\\nnisms. As we have shown in Sec. 6.2, Group Attn. is more accurate\\n7\\nDataset\\nLength\\nTST [61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nWISDM\\n200\\n13.30\\n150.3\\n3.240\\n178.1\\n3.449\\n162.6\\n3.852\\n141.9\\n3.277\\n136.7\\nHHAR\\n200\\n1.085\\n78.2\\n0.2968\\n97.4\\n0.2980\\n82.6\\n0.3198\\n81.1\\n0.2974\\n73.3\\nRWHAR\\n200\\n0.0882\\n83.9\\n0.0478\\n108.1\\n0.0489\\n89.1\\n0.0572\\n98.4\\n0.0478\\n81.3\\nECG\\n2000\\n0.0905\\n696.3\\n0.0037\\n857.9\\n0.0033\\n270.2\\n0.0035\\n291.38\\n0.0038\\n164.36\\nMGH\\n10000\\nN/A\\nN/A\\nN/A\\nN/A\\n0.00014\\n356.2\\n0.00088\\n404.9\\n0.00042\\n54.4\\nTable 2: Imputation results (multi-variate data). The best results are marked with bold.\\nDataset\\nPretrain Size\\nTST [61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nWISDM\\n62,231\\n49.13%\\n50.03%\\n66.16%\\n75.89%\\n66.09%\\n73.97%\\n50.12%\\n67.44%\\n62.56%\\n75.06%\\nHHAR\\n68,294\\n72.56%\\n75.30%\\n75.60%\\n81.35%\\n76.52%\\n80.70%\\n65.94%\\n76.52%\\n76.17%\\n82.62%\\nRWHAR\\n63,599\\n69.46%\\n80.41%\\n85.68%\\n91.14%\\n87.54%\\n91.33%\\n81.03%\\n86.33%\\n86.13%\\n89.63%\\nECG\\n561,358\\n20.98%\\n27.99%\\n42.05%\\n46.16%\\n43.34%\\n45.58%\\n27.19%\\n31.34%\\n42.58%\\n46.39%\\nTable 3: Pretrain + few-label finetuning results. The best results are marked with bold.\\nTraining Time/sec\\nMSE\\n(a) Effectiveness\\n(b) Efficiency\\nFigure 4: Varying the lengths of timeseries.\\nthan Performer and Linformer in classification and imputation tasks,\\nwhile Group Attn. is always faster than Performer, Linformer, and\\nall other baselines on all 5 multi-variate datasets, thus a win-win.\\n(3) The longer the timeseries, the larger the speedup. On\\nthe medium sized ECG dataset with a length of 2,000, Group Attn.\\nhas a speedup of 3.86/1.36/2.27 compared to Vanilla/Performer/Lin-\\nformer. When the length increases to 10,000, the speedup on the\\nMGH dataset increases to 6.59/7.48 compared to Performer/Lin-\\nformer (Vanilla and TST failed in this case) on imputation task\\n(Table. 2). However, even on the short WISDM, HHAR, RWHAR\\ndatasets, Group Attn. still consistently outperforms other methods,\\nconfirming that it does not introduce much overhead. This is be-\\ncause when the length of the timeseries gets longer, Group Attn.\\ngets more opportunities to find windows with similar properties.\\n6.3.2\\nTraining time: Varying the Length\\nIn this experiment, we truncate the original MGH timseries into\\nsequences with the lengths at 2000/4000/6000/8000/10000, and com-\\npare Group Attn. against Vanilla and other attention mechanisms.\\nVanilla cannot handle sequences longer than 8000.\\nThe results in Fig. 4 again show that the longer the timeseries, the\\nlarger the speed up. With comparable MSE, Group Attn. outperforms\\nVanilla by 63X. Moreover, as the length increases from 2000 to 10000,\\nthe training time of Group Attn. only increases from 31.2 seconds\\nto 54.4 seconds per epoch. The reason is that as the timeseires\\nbecomes longer, there are more grouping opportunities because of\\nthe similarity of the timeseries segments.\\nAccuracy\\nTraining Time/sec\\n(a)\\n(b)\\nFigure 5: Comparison to non-deep learning method (uni-\\nvariate data).\\n6.4\\nComparison to Non-deep Learning Methods\\nWe compare against GRAIL, the SOTA of non-deep learning time-\\nseries representation learning. We use the three uni-variate datasets,\\nbecause GRAIL only targets uni-variate timeseries.\\nResults in Fig. 5 show that on all 3 datasets RITA significantly\\noutperforms GRAIL in accuracy by 45, 16, and 21 percentage points\\nbecause of the expressive power of Transformer. Moreover, thanks\\nto the GPU-friendly design of RITA, it is at least 2× faster than\\nGRAIL in training time.\\n6.5\\nAblation Study\\n6.5.1\\nAdaptive Scheduler\\nTo evaluate the effectiveness of RITA’s adaptive scheduler (Sec. 5),\\nwe compare it against a baseline using a fixed group number 𝑁. We\\nvary 𝑁and the error bound threshold 𝜖used by RITA.\\nFrom the results in Table 4 we get the following observations:\\n(1) Adaptive Scheduler is better than fixed 𝑁. Training with\\nAdaptive Scheduler already achieves better or comparable perfor-\\nmance compared to the best performing 𝑁. More specifically, on\\nthe MGH dataset, dynamic scheduler always achieves better accu-\\nracy and is much faster compared to fixed 𝑁. On the ECG dataset,\\nalthough fixed 𝑁is slightly better than adaptive scheduler in accu-\\nracy when setting the N as 512, it runs much slower than adaptive\\nscheduler. Of course, finding the best 𝑁that balances the accuracy\\nand running time requires careful tuning.\\n(2) Adaptive Scheduler is tuning free. It is robust on both\\naccuracy and running time when 𝜖varies, while the results of\\nfixed 𝑁vary significantly when the value of 𝑁changes. Therefore,\\nAdaptive Scheduler frees the users from tuning the 𝜖threshold,\\nwhile it is hard to find an appropriate 𝑁for a given dataset.\\n8\\nDataset\\nTask\\nScheduler\\nParameter\\nMetric\\nTime\\nECG\\nClass.\\nDynamic\\n1.5\\n88.34%\\n292.5\\n2\\n88.48%\\n236.8\\n3\\n87.83%\\n216.8\\nFixed\\n64\\n87.50%\\n255.2\\n128\\n88.96%\\n297.2\\n256\\n88.82%\\n414.1\\n512\\n90.03%\\n662.6\\n1024\\n88.65%\\n873.7\\nMGH\\nImput.\\nDynamic\\n1.5\\n0.00041\\n60.7\\n2\\n0.00040\\n57.9\\n3\\n0.00042\\n54.4\\nFixed\\n128\\n0.00054\\n128.6\\n256\\n0.00053\\n190.2\\n512\\n0.00049\\n240.8\\n1024\\n0.00046\\n323.3\\nTable 4: Adaptive Scheduling VS Fixed N.\\nPretrain Data size\\nFew-label Accuracy\\nN/A\\n62.56%\\n12,446\\n72.94%\\n24,892\\n72.78%\\n37,338\\n74.10%\\n49,784\\n74.22%\\n62,231\\n75.06%\\nTable 5: RITA Pretraining: increasing sizes of pretrain set.\\n6.5.2\\nThe Sizes of the Pretraining Data\\nNext, we evaluate how the number of unlabeled data influences the\\neffectiveness of pretraining. To get empirical results, we pretrain\\nRITA on WISDM dataset with 20%/40%/60%/80% of the pretraining\\ndata and finetune each pretrained model with 100 labels per class.\\nThe results in Table 5 show that: (1) The more pretraining data,\\nthe larger the improvement. The accuracy increases with the\\nsizes of the pretraining data; (2) Marginal utility diminishing.\\nThe first 20% pretraining data gives a 10.38% improvement in accu-\\nracy (72.94% vs 62.56%), while the remaining 80% pretraining data\\nonly gives an additional improvement of 2.12% (75.06% vs 72.94%).\\n7\\nRELATED WORK\\n7.1\\nTimeseries Analytics\\nThere is a great deal of prior work on timeseries analytics methods.\\nThis work can be divided into three categories: (1) non-deep learn-\\ning methods; (2) CNN/RNN-based deep learning methods; and (3)\\nTransformer-based deep learning methods.\\nTraditional Methods. These methods, such as TS-CHIEF [45],\\nHIVE-COTE [33], ROCKET [15] have achieved notable performance\\non public datasets. Despite that, traditional methods suffer from\\none or more issues: they (1) rely on expert knowledge for feature\\nextraction; (2) incur heavy computation cost and are inappropriate\\nfor GPU devices; (3) support only uni-variate timeseries; (4) perform\\nclassification solely. Some work [61] shows that the transformed-\\nbased methods outperform these traditional methods especially on\\nmulti-variate timeseries.\\nIn particular, as the SOTA of timeseries representation learn-\\ning, GRAIL [40] extracts landmarks from data and computes the\\nrepresentations with the combination of the landmarks. However,\\nGRAIL only supports uni-variate timeseries. Our experiments (Sec. 6.4)\\nshow that RITA significantly outperforms GRAIL in both effective-\\nness and efficiency on uni-variate timeseries.\\nCNN/RNN-based Deep Learning Methods. CNN-based methods,\\nsuch as InceptionTime [21] and Resnet [19], are good at classifica-\\ntion tasks, but can not handle generative tasks such as forecasting\\nbecause of the inductive bias of convolution networks. RNN-based\\nmethods, such as Brit [7] and deepAR [44], are capable for classifi-\\ncation, regression and generation. However, the recurrent structure\\nbrings a lot of problems: (1) limiting the model’s ability in captur-\\ning long-range correlation; (2) notoriously difficult to train [41]\\nbecause of gradient vanishing and exploding problem. As a result,\\nsuch methods can hardly scale to very long timeseries.\\nTransformer-based Deep Learning Methods. Given that Trans-\\nformer is the best choice for backbone in almost all sequence mod-\\neling tasks, some effort has been made to apply Transformer to\\ntimeseries analytics. Targeting forecasting of uni-variate timeseries,\\nLogTrans [30] introduced a log sparsity assumption to attention\\ncomputation. Informer [62] pushes LogTrans a step further and\\nscales forecasting to multi-variate timeseries. Autoformer [57] per-\\nforms forecasting by decomposing timeseries into two parts, i.e.\\nthe trend part and the seasonal part.\\nFor imputation tasks, CDSA [37] outperforms statistical meth-\\nods and the SOTA of RNN-based method Brit [7] on 3 public and\\n2 competition datasets. For timeseries classification, AutoTrans-\\nformer [43] performs architecture search to adapt to the tasks\\nin different domains. For timeseries anomaly detection, Anomaly\\nTransformer [58] outperforms many widely-used methods such\\nas OmniAnomaly [47], assuming the attention score maps show\\nGaussian distribution.\\nAll of these works are designed for specific tasks, rather than\\nfunctioning as a representation learning framework to serve\\ndifferent downstream tasks. To fill this gap, some researchers pro-\\nposed a Transformer-based architecture, called TST [61]. Like RITA,\\nTST supports regression, classification, and unsupervised learning\\nthrough the “cloze test” pretraining task on timeseries. However,\\nTST directly uses the classical Vanilla self-attention, thus not scal-\\nable to long timeseries as shown in our experiments (Sec. 6.3.2).\\n7.2\\nEfficient Transformers\\nThe need of improving the scalability of Transformers has led to\\nmore efficient variations of Transformers, especially for accommo-\\ndating long text data in NLP [49].\\nIntroducing fixed/random patterns to self-attention mechanism\\nis an intuitive idea. Sparse Transformer [9] and Longformer [3] only\\ncompute attention at fixed intervals. ETC [2] and BigBird [60] use\\nglobal-local attention: the attention computation is limited within\\na fixed radius, while some auxiliary tokens are added to attend/get\\nattended globally. The deficiencies of fixed attention patterns are\\nobvious: it heavily depends on users to give an optimal setting.\\nTo decrease the reliance on human labor, some works seek to\\nintroduce learnable/adaptive attention patterns instead of fixed\\npatterns. Reformer [26] proposed only computing the dominant\\nattention terms based on their observation of sparsity in atten-\\ntion matrix from language/image data. Such sparsity is intuitive\\nin language data, in which a word’s attention mainly focuses on\\nthe nearby sentences. However, attention in timeseries data shows\\nstrong seasonal patterns rather than sparse patterns, mainly as\\n9\\nresult of the periodicity of timeseries data. Therefore, such works\\ndo not work well for timeseries.\\nApart from introducing attention patterns, some works seek\\nto solve this problem with applied mathematics techniques. Lin-\\nformer [54] performs a projection to decrease the size of query,\\nkey and value matrices before attention computation, because the\\nattention matrix tends to be low-ranked. Performer [10] uses linear\\nfunctions to approximate the kernel function softmax, making at-\\ntention computation commutative. When the sequence length is far\\ngreater than the dimension of embedding vectors, Performer ben-\\nefits from changing the order of matrix multiplication. Linformer\\nand Performer do not depend on the unique properties of language\\ndata, thus potentially fitting timeseries better than other techniques,\\nwhich is why we compared against them in our experiments. How-\\never as shown in Sec. 6, our group attention significantly outper-\\nforms them in both accuracy and efficiency (training time), because\\ngroup attention fully leverages the periodicity of timeseries.\\n8\\nCONCLUSION\\nIn this work, we presented RITA, an automatic, self-supervised, and\\nscalable timeseries analytics tool. RITA effectively adapts Trans-\\nformer, popular in NLP, into timeseries analytics. As the key com-\\nponent of RITA, group attention eliminates the performance bottle-\\nneck of the classical self-attention mechanisms, thus successfully\\nscaling RITA to highly complex, long timeseries data. Our experi-\\nments confirm that RITA significantly speeds up the state-of-the-art\\nby 63X with a better accuracy.\\nREFERENCES\\n[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,\\nCraig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al.\\n2016. Tensorflow: Large-scale machine learning on heterogeneous distributed\\nsystems. arXiv preprint arXiv:1603.04467 (2016).\\n[2] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher,\\nPhilip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020.\\nETC: Encoding long and structured inputs in transformers.\\narXiv preprint\\narXiv:2004.08483 (2020).\\n[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-\\ndocument transformer. arXiv preprint arXiv:2004.05150 (2020).\\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\\ninformation processing systems 33 (2020), 1877–1901.\\n[5] C Bui, N Pham, A Vo, A Tran, A Nguyen, and T Le. 2017. Time series forecasting\\nfor healthcare diagnosis and prognostics with the focus on cardiovascular dis-\\neases. In International conference on the development of biomedical engineering in\\nVietnam. Springer, 809–818.\\n[6] Lei Cao, Wenbo Tao, Sungtae An, Jing Jin, Yizhou Yan, Xiaoyu Liu, Wendong\\nGe, Adam Sah, Leilani Battle, Jimeng Sun, Remco Chang, M. Brandon Westover,\\nSamuel Madden, and Michael Stonebraker. 2019. Smile: A System to Support\\nMachine Learning on EEG Data at Scale. Proc. VLDB Endow. 12, 12 (2019), 2230–\\n2241.\\n[7] Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. 2018.\\nBrits:\\nBidirectional recurrent imputation for time series. Advances in neural information\\nprocessing systems 31 (2018).\\n[8] Chris Chatfield. 2000. Time-series forecasting. Chapman and Hall/CRC.\\n[9] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating\\nlong sequences with sparse transformers. arXiv preprint arXiv:1904.10509 (2019).\\n[10] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,\\nAndreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\\nLukasz Kaiser, et al. 2020. Rethinking attention with performers. arXiv preprint\\narXiv:2009.14794 (2020).\\n[11] Andrew A Cook, Göksel Mısırlı, and Zhong Fan. 2019. Anomaly detection for IoT\\ntime-series data: A survey. IEEE Internet of Things Journal 7, 7 (2019), 6481–6494.\\n[12] Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine\\nlearning 20, 3 (1995), 273–297.\\n[13] David R Cox. 1958. The regression analysis of binary sequences. Journal of the\\nRoyal Statistical Society: Series B (Methodological) 20, 2 (1958), 215–232.\\n[14] Benjamin F Crabtree, Subhash C Ray, Priscilla M Schmidt, Patrick T O’Connor,\\nand David D Schmidt. 1990. The individual over time: time series applications in\\nhealth care research. Journal of clinical epidemiology 43, 3 (1990), 241–260.\\n[15] Angus Dempster, François Petitjean, and Geoffrey I. Webb. 2020. ROCKET: excep-\\ntionally fast and accurate time series classification using random convolutional\\nkernels. Data Min. Knowl. Discov. 34, 5 (2020), 1454–1495.\\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\\nProceedings of the 2019 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,\\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). 4171–\\n4186.\\n[17] Evelyn Fix and Joseph Lawson Hodges. 1989. Discriminatory analysis. Nonpara-\\nmetric discrimination: Consistency properties. International Statistical Review/Re-\\nvue Internationale de Statistique 57, 3 (1989), 238–247.\\n[18] Philip George Guest and Philip George Guest. 2012. Numerical methods of curve\\nfitting. Cambridge University Press.\\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\\nlearning for image recognition. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition. 770–778.\\n[20] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar,\\nand Pierre-Alain Muller. 2019. Deep learning for time series classification: a\\nreview. Data mining and knowledge discovery 33, 4 (2019), 917–963.\\n[21] Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, Charlotte Pelletier,\\nDaniel F Schmidt, Jonathan Weber, Geoffrey I Webb, Lhassane Idoumghar, Pierre-\\nAlain Muller, and François Petitjean. 2020. Inceptiontime: Finding alexnet for\\ntime series classification. Data Mining and Knowledge Discovery 34, 6 (2020),\\n1936–1962.\\n[22] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization\\nfor nearest neighbor search. IEEE transactions on pattern analysis and machine\\nintelligence 33, 1 (2010), 117–128.\\n[23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity\\nsearch with gpus. IEEE Transactions on Big Data 7, 3 (2019), 535–547.\\n[24] Richard M Karp. 1972. Reducibility among combinatorial problems. In Complexity\\nof computer computations. Springer, 85–103.\\n[25] Eamonn Keogh, Kaushik Chakrabarti, Michael Pazzani, and Sharad Mehrotra.\\n2001. Dimensionality reduction for fast similarity search in large time series\\ndatabases. Knowledge and information Systems 3, 3 (2001), 263–286.\\n[26] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient\\ntransformer. arXiv preprint arXiv:2001.04451 (2020).\\n[27] John Kraft and Arthur Kraft. 1977. Determinants of common stock prices: A time\\nseries analysis. The journal of finance 32, 2 (1977), 417–425.\\n[28] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Clas-\\nsification with Deep Convolutional Neural Networks. In Advances in Neural\\nInformation Processing Systems, F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein-\\nberger (Eds.), Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/\\npaper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\\n[29] Oscar D Lara and Miguel A Labrador. 2012. A survey on human activity recog-\\nnition using wearable sensors. IEEE communications surveys & tutorials 15, 3\\n(2012), 1192–1209.\\n[30] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,\\nand Xifeng Yan. 2019. Enhancing the locality and breaking the memory bottle-\\nneck of transformer on time series forecasting. Advances in Neural Information\\nProcessing Systems 32 (2019).\\n[31] T Warren Liao. 2005. Clustering of time series data—a survey. Pattern recognition\\n38, 11 (2005), 1857–1874.\\n[32] Rake& Agrawal King-lp Lin and Harpreet S Sawhney Kyuseok Shim. 1995. Fast\\nsimilarity search in the presence of noise, scaling, and translation in time-series\\ndatabases. In Proceeding of the 21th International Conference on Very Large Data\\nBases. 490–501.\\n[33] Jason Lines, Sarah Taylor, and Anthony Bagnall. 2018. Time Series Classification\\nwith HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based\\nEnsembles. ACM Trans. Knowl. Discov. Data 12, 5, Article 52 (jul 2018), 35 pages.\\n[34] Feifei Liu, Chengyu Liu, Lina Zhao, Xiangyu Zhang, Xiaoling Wu, Xiaoyan\\nXu, Yulin Liu, Caiyun Ma, Shoushui Wei, Zhiqiang He, et al. 2018. An open\\naccess database for evaluating the algorithms of electrocardiogram rhythm and\\nmorphology abnormality detection. Journal of Medical Imaging and Health\\nInformatics 8, 7 (2018), 1368–1373.\\n[35] Stuart Lloyd. 1982. Least squares quantization in PCM. IEEE transactions on\\ninformation theory 28, 2 (1982), 129–137.\\n[36] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization.\\narXiv preprint arXiv:1711.05101 (2017).\\n[37] Jiawei Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, Anthony Vetro, and\\nShih-Fu Chang. 2019. CDSA: cross-dimensional self-attention for multivariate,\\ngeo-tagged time series imputation. arXiv preprint arXiv:1905.09904 (2019).\\n10\\n[38] Yu A Malkov and Dmitry A Yashunin. 2018. Efficient and robust approximate\\nnearest neighbor search using hierarchical navigable small world graphs. IEEE\\ntransactions on pattern analysis and machine intelligence 42, 4 (2018), 824–836.\\n[39] Tripti Negi and Veena Bansal. 2005. Time series: Similarity search and its appli-\\ncations. In Proceedings of the International Conference on Systemics, Cybernetics\\nand Informatics: ICSCI-04, Hyderabad, India. 528–533.\\n[40] John Paparrizos and Michael J Franklin. 2019. Grail: efficient time-series repre-\\nsentation learning. Proceedings of the VLDB Endowment 12, 11 (2019), 1762–1777.\\n[41] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty\\nof training recurrent neural networks. In International conference on machine\\nlearning. PMLR, 1310–1318.\\n[42] Ning Qian. 1999. On the momentum term in gradient descent learning algorithms.\\nNeural networks 12, 1 (1999), 145–151.\\n[43] Yankun Ren, Longfei Li, Xinxing Yang, and Jun Zhou. 2022. AutoTransformer:\\nAutomatic Transformer Architecture Design for Time Series Classification. In\\nPacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 143–\\n155.\\n[44] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.\\nDeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter-\\nnational Journal of Forecasting 36, 3 (2020), 1181–1191.\\n[45] Ahmed Shifaz, Charlotte Pelletier, François Petitjean, and Geoffrey I. Webb. 2020.\\nTS-CHIEF: a scalable and accurate forest algorithm for time series classification.\\nData Mining and Knowledge Discovery 34 (2020), 742–775.\\n[46] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow,\\nMikkel Baun Kjærgaard, Anind Dey, Tobias Sonne, and Mads Møller Jensen.\\n2015. Smart devices are different: Assessing and mitigatingmobile sensing het-\\nerogeneities for activity recognition. In Proceedings of the 13th ACM conference\\non embedded networked sensor systems. 127–140.\\n[47] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019. Robust\\nanomaly detection for multivariate time series through stochastic recurrent\\nneural network. In Proceedings of the 25th ACM SIGKDD international conference\\non knowledge discovery & data mining. 2828–2837.\\n[48] Timo Sztyler and Heiner Stuckenschmidt. 2016. On-body localization of wearable\\ndevices: An investigation of position-aware activity recognition. In 2016 IEEE\\nInternational Conference on Pervasive Computing and Communications (PerCom).\\nIEEE, 1–9.\\n[49] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient\\ntransformers: A survey. ACM Computing Surveys (CSUR) (2020).\\n[50] Mingyan Teng. 2010. Anomaly detection on time series. In 2010 IEEE International\\nConference on Progress in Informatics and Computing, Vol. 1. IEEE, 603–608.\\n[51] Patrick A Thompson. 1990. An MSE statistic for comparing forecast accuracy\\nacross series. International Journal of Forecasting 6, 2 (1990), 219–227.\\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\\nyou Need. In Advances in Neural Information Processing Systems 30: Annual Con-\\nference on Neural Information Processing Systems 2017, December 4-9, 2017, Long\\nBeach, CA, USA. 5998–6008.\\n[53] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler\\nReddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser,\\nJonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jar-\\nrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern,\\nEric Larson, C J Carey, İlhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas,\\nDenis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,\\nCharles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa,\\nPaul van Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental Al-\\ngorithms for Scientific Computing in Python. Nature Methods 17 (2020), 261–272.\\nhttps://doi.org/10.1038/s41592-019-0686-2\\n[54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Lin-\\nformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768\\n(2020).\\n[55] Gary M Weiss, Kenichi Yoneda, and Thaier Hayajneh. 2019. Smartphone and\\nsmartwatch-based biometrics using activities of daily living. IEEE Access 7 (2019),\\n133190–133202.\\n[56] Qingsong Wen, Kai He, Liang Sun, Yingying Zhang, Min Ke, and Huan Xu. 2021.\\nRobustPeriod: Robust Time-Frequency Mining for Multiple Periodicity Detection.\\nIn Proceedings of the 2021 International Conference on Management of Data (Virtual\\nEvent, China) (SIGMOD ’21). Association for Computing Machinery, New York,\\nNY, USA, 2328–2337. https://doi.org/10.1145/3448016.3452779\\n[57] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De-\\ncomposition transformers with auto-correlation for long-term series forecasting.\\nAdvances in Neural Information Processing Systems 34 (2021), 22419–22430.\\n[58] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2021. Anomaly\\nTransformer: Time Series Anomaly Detection with Association Discrepancy.\\narXiv preprint arXiv:2110.02642 (2021).\\n[59] Dianmin Yue, Xiaodan Wu, Yunfeng Wang, Yue Li, and Chao-Hsien Chu. 2007. A\\nreview of data mining-based financial fraud detection research. In 2007 Interna-\\ntional Conference on Wireless Communications, Networking and Mobile Computing.\\nIeee, 5519–5522.\\n[60] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris\\nAlberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\\net al. 2020. Big bird: Transformers for longer sequences. Advances in Neural\\nInformation Processing Systems 33 (2020), 17283–17297.\\n[61] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and\\nCarsten Eickhoff. 2021. A Transformer-based Framework for Multivariate Time\\nSeries Representation Learning. In KDD ’21: The 27th ACM SIGKDD Conference\\non Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18,\\n2021. 2114–2124.\\n[62] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,\\nand Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-\\nquence time-series forecasting. In Proceedings of AAAI.\\nA\\nAPPENDIX: SUPPLEMENTARY MATERIAL\\nA.1\\nExperiment Configuration and\\nHyper-parameter Settings\\nConfiguration. All models were trained on an NVIDIA Tesla V100\\n16GB GPU. All the methods are optimized with AdamW [36] of\\nwhich the starting learning rate and weight decay parameter are\\nboth 1𝑒−4. In full-label training scenario, we train the models for\\n100 epochs. In “pretraining + few-label finetuning scenario”, as the\\npretrained models require fewer epochs to converge [61], we train\\nthe model for 50 epochs. For a fair comparison, the baselines use a\\nmaximal batch size within GPU’s capacity during training.\\nAs for model hyper-parameter setting, RITA and the baselines\\nuse a Transformer structure balancing Vanilla ’s accuracy and\\nefficiency: 8-layer stack of 2-head attention with hidden vectors\\nin dimension of 64. Convolution kernel size is set to 5 by default.\\nWe set the error bound threshold (𝜖, Sec. 5.1) of Group Attention\\nto 2, as it balances the accuracy and the efficiency in general on\\nall datasets. Because Linformer requires the users to set the sizes\\nof projection matrix, in different settings we choose an accuracy-\\nefficiency balancing one among {64,128,256,512}.\\nA.2\\nEfficient Computation of Group Attention\\nAlgorithm 1 Efficient Computation of Group Attention\\nRequire: 𝑄,𝑉, 𝑅,𝐶𝑂𝑈𝑁𝑇, 𝐵𝐸𝐿𝑂𝑁𝐺\\nEnsure: 𝑄,𝑉∈R𝑛∗𝑑,𝑅∈R𝑁∗𝑑,𝐶𝑂𝑈𝑁𝑇∈N𝑁,𝐵𝐸𝐿𝑂𝑁𝐺∈N𝑛\\n1: function group_attention(𝑄,𝑉, 𝑅)\\n2:\\nfor 𝑖= 0 →𝑁−1 do\\n3:\\ne𝑣𝑖←Í𝑛−1\\n𝑗=0 (𝐵𝐸𝐿𝑂𝑁𝐺𝑗== 𝑖)𝑣𝑗\\n4:\\ne𝑃←𝑄𝑅𝑇\\n5:\\nfor 𝑖= 0 →𝑛−1 do\\n6:\\nfor 𝑗= 0 →𝑁−1 do\\n7:\\n𝑤𝑖,𝑗←𝑒𝑥𝑝(e𝑃𝑖,𝑗)𝐶𝑂𝑈𝑁𝑇𝑗\\n8:\\nfor 𝑖= 0 →𝑛−1 do\\n9:\\n𝑠𝑖←Í𝑁−1\\n𝑗=0 𝑤𝑖,𝑗\\n10:\\nfor 𝑖= 0 →𝑛−1 do\\n11:\\n𝑜𝑖←Í𝑁−1\\n𝑗=0\\n𝑒𝑥𝑝( e𝑃𝑖,𝑗)\\n𝑠𝑖\\ne𝑣𝑗\\n12:\\nreturn 𝑂\\nIn Alg. 1, we denote𝐶𝑂𝑈𝑁𝑇𝑖to be the size of the 𝑖𝑡ℎgroup, 𝑁to\\nbe the number of groups, r𝑖to be the representative key of the 𝑖𝑡ℎ\\ngroup and R to be the matrix consisting of all r𝑖, 𝐵𝐸𝐿𝑂𝑁𝐺𝑖to be\\nthe group that k𝑖belongs to. 𝑄,𝑉are the packing matrices of query\\nvectors and value vectors as described in Sec.2. Alg. 1 outputs the\\n11\\npacking matrix 𝑂for new feature emebddings {𝑜1, ...,𝑜𝑛}, where 𝑜𝑖\\ncorresponds to the feature embedding of 𝑤𝑖𝑛𝑖. Lines 2-3 implement\\nthe embedding aggregation operation, while Lines 8-11 implement\\nthe group softmax function.\\nA.3\\nThe Algorithms and Optimality Proof for\\nDynamically Determing Batch Size\\nAlgorithm 2 Binary Search for Batch Size\\nRequire: 𝐿, 𝑁\\nEnsure: 1 ≤𝐿≤𝐿𝑚𝑎𝑥, 1 ≤𝑁≤𝐿\\n1: function binary_search(𝐿, 𝑁)\\n2:\\n𝐿←1\\n3:\\n𝑅←𝑀𝑎𝑥𝐵𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒\\n4:\\n𝑑𝑎𝑡𝑎←𝑅𝑎𝑛𝑑𝑜𝑚𝑇𝑖𝑚𝑒𝑆𝑒𝑟𝑖𝑒𝑠𝑖𝑛𝑙𝑒𝑛𝑔𝑡ℎ𝐿\\n5:\\n𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙\\n6:\\nwhile 𝐿≤𝑅do\\n7:\\n𝐼𝑛𝑝𝑢𝑡←𝑑𝑎𝑡𝑎× 𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙\\n8:\\n𝑀𝑜𝑑𝑒𝑙𝐹𝑜𝑟𝑤𝑎𝑟𝑑(𝐼𝑛𝑝𝑢𝑡)\\n9:\\n𝑀𝑜𝑑𝑒𝑙𝐵𝑎𝑐𝑘𝑤𝑎𝑟𝑑\\n10:\\n𝑢←𝑃𝑒𝑎𝑘𝑀𝑒𝑚𝑜𝑟𝑦𝑈𝑠𝑎𝑔𝑒\\n𝑇𝑜𝑡𝑎𝑙𝑀𝑒𝑚𝑜𝑟𝑦\\n11:\\nif 0.9 > 𝑢then\\n12:\\n𝐿←𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙+ 1\\n13:\\n𝐵←𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙\\n14:\\nelse\\n15:\\n𝑅←𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙−1\\n16:\\n𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙←⌊𝐿+𝑅⌋\\n2\\n17:\\nreturn 𝐵\\nAlgorithm 3 Dynamic Programming for Plane Division\\nRequire: 𝐿𝑖, 𝑁𝑖, 𝐵𝑖, 𝐿𝑚𝑎𝑥\\nEnsure: 1 ≤𝐿𝑖≤𝐿𝑚𝑎𝑥, 1 ≤𝑁𝑖≤𝐿𝑖\\n1: function cost(S)\\n2:\\nif |𝑆| < 𝑀then return +∞\\n3:\\n𝐿, 𝑁, 𝐵←𝑝𝑜𝑖𝑛𝑡𝑠𝑖𝑛𝑆\\n4:\\n𝑓←𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛𝑓𝑖𝑡𝑡𝑖𝑛𝑔(𝐵|𝐿, 𝑁)\\nreturn 𝐸(𝐵, 𝐿, 𝑁|𝑓)\\n5: function dynamic_programming(𝐿𝑖, 𝑁𝑖, 𝐿𝑚𝑎𝑥)\\n6:\\nfor 𝑙1 = 1 →𝐿𝑚𝑎𝑥do\\n7:\\nfor 𝑙2 = 1 →𝑙1 do\\n8:\\nfor 𝑛= 1 →𝑙1 do\\n9:\\n𝑆←𝑝𝑜𝑖𝑛𝑡𝑠𝑠𝑒𝑡𝑖𝑛{𝑙2 ≤𝐿≤𝑙1, 𝑁≤𝑛}\\n10:\\n𝑔(𝑛) ←𝐶𝑂𝑆𝑇(𝑆)\\n11:\\nfor 𝑖= 1 →𝑛do\\n12:\\n𝑆←𝑝𝑜𝑖𝑛𝑡𝑠𝑠𝑒𝑡𝑖𝑛{𝑙2 ≤𝐿≤𝑙1,𝑖≤𝑁≤𝑛}\\n13:\\n𝑔(𝑛) ←𝑚𝑖𝑛(𝑔(𝑛),𝑔(𝑖) + 𝐶𝑂𝑆𝑇(𝑆))\\n14:\\n𝑓𝑙2,𝑙1 ←𝑔(𝑙1)\\n15:\\n16:\\nfor 𝑙= 1 →𝐿𝑚𝑎𝑥do\\n17:\\n𝑑𝑝(𝑙) ←𝑓(1,𝑙)\\n18:\\nfor 𝑖= 1 →𝑙do\\n19:\\n𝑑𝑝(𝑙) ←𝑚𝑖𝑛(𝑑𝑝(𝑙),𝑑𝑝(𝑖) + 𝑓(𝑖,𝑙))\\nreturn 𝑑𝑝(𝐿𝑚𝑎𝑥)\\nWe describe Alg. 3 and intuitively show its optimality. We assume\\nthat Scipy [53] learns an optimal function in Line 4 so that function\\nCOST gives the optimal estimation error when fitting the points in\\nset 𝑆. When fitting very few points, we assign an infinite cost to\\nprevent a biased fitting function (Line 2). 𝑔(𝑛) denotes the minimal\\nestimation error for points in sub-plane {𝑙2 ≤𝐿≤𝑙1, 𝑁≤𝑛}. In\\nLines 11-13, we enumerate all possible ways of cutting {𝑙2 ≤𝐿≤\\n𝑙1, 𝑁≤𝑛} horizontally into two sub-plane {𝑙2 ≤𝐿≤𝑙1, 𝑁≤𝑖} and\\n{𝑙2 ≤𝐿≤𝑙1,𝑖≤𝑁≤𝑛} by iterating 𝑖from 1 to n. Choosing the\\ncutting strategy that minimizes estimation error gets us a𝑔(𝑙1) with\\nminimal estimation error for sub-plane {𝑙2 ≤𝐿≤𝑙1, 𝑁≤𝑙1}, which\\nis recorded as 𝑓𝑙1,𝑙2 in Line 14. 𝑑𝑝(𝑙) denotes the minimal estimation\\nerror for sub-plane {𝐿≤𝑙}. We enumerate all the possible ways\\nof cutting {𝐿≤𝑙} vertically into two sub-plane {𝐿≤𝑖} and {𝑖≤\\n𝐿≤𝑙} by iterating 𝑖from 1 to 𝑙(Line 17-19). Finally, we have the\\nminimal estimation error for the whole plane as 𝑑𝑝(𝐿𝑚𝑎𝑥). Based\\non the above discussion, this algorithm guarantees to not miss any\\nbetter solution, hence optimal.\\nA.4\\nThe Correctness of Group Attention\\nLemma 3. Assuming the windows belonging to the same group 𝐺𝑖\\nhave the same key vector, i.e. 𝑘𝑗= 𝑟𝑖(𝑤𝑖𝑛𝑗∈𝐺𝑖), then the feature\\nembedding 𝑂produced by the original self-attention mechanism is\\nidentical to the output of our group attention mechanism implemented\\nin Algorithm 1.\\nProof. Denote e\\n𝑘𝑗to be the representative vectors of 𝑘𝑗, i.e. e\\n𝑘𝑗=\\n𝑟𝑖= 𝑘𝑗(𝑤𝑖𝑛𝑗∈𝐺𝑖). Algorithm 1 gives that\\ne𝑣𝑖=\\n𝑛−1\\n∑︁\\n𝑗=0\\n(𝐵𝐸𝐿𝑂𝑁𝐺𝑗== 𝑖)v𝑗, e𝑃𝑖,𝑗= q𝑖· r𝑗\\n𝑠𝑖=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑒𝑥𝑝(e𝑃𝑖,𝑗)𝐶𝑂𝑈𝑁𝑇𝑗, e𝑜𝑖=\\n𝑁−1\\n∑︁\\n𝑗=0\\ne𝑃𝑖,𝑗\\n𝑠𝑖\\ne𝑣𝑗\\n(7)\\nBy the canonical self-attention mechanism introduced in Sec. 2,\\nwe get:\\n𝑃𝑖,𝑗= q𝑖· kj, 𝐴𝑖,𝑗=\\n𝑒𝑥𝑝(𝑃𝑖,𝑗)\\nÍ𝑛−1\\n𝑘=0 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\n, o𝑖=\\n𝑛−1\\n∑︁\\n𝑗=0\\n𝐴𝑖,𝑗v𝑗\\n(8)\\nWith 7 and 8, we have\\n𝑛−1\\n∑︁\\n𝑗=0\\n𝑒𝑥𝑝(𝑃𝑖,𝑗) =\\n𝑛−1\\n∑︁\\n𝑗=0\\n𝑒𝑥𝑝(q𝑖· k𝑗)\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑛−1\\n∑︁\\n𝑥=0\\n(𝐵𝐸𝐿𝑂𝑁𝐺𝑥== 𝑗)𝑒𝑥𝑝(q𝑖· k𝑥)\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑒𝑥𝑝(q𝑖· r𝑗)\\n𝑛−1\\n∑︁\\n𝑥=0\\n(𝐵𝐸𝐿𝑂𝑁𝐺𝑥== 𝑗)\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑒𝑥𝑝(q𝑖· r𝑗)𝐶𝑂𝑈𝑁𝑇𝑗\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑒𝑥𝑝(e𝑃𝑖,𝑗)𝐶𝑂𝑈𝑁𝑇𝑗\\n= 𝑠𝑖\\n(9)\\n12\\nFurther,\\no𝑖=\\n𝑛−1\\n∑︁\\n𝑗=0\\n𝐴𝑖,𝑗vj\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑛−1\\n∑︁\\n𝑥=0\\n(𝐵𝐸𝐿𝑂𝑁𝐺𝑥== 𝑗)𝐴𝑖,𝑥v𝑥\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑛−1\\n∑︁\\n𝑥=0\\n(𝐵𝐸𝐿𝑂𝑁𝐺𝑥== 𝑗)\\n𝑒𝑥𝑝(𝑃𝑖,𝑥)\\nÍ𝑛−1\\n𝑘=0 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\nv𝑥\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑛−1\\n∑︁\\n𝑥=0\\n(𝐵𝐸𝐿𝑂𝑁𝐺𝑥== 𝑗)\\n𝑒𝑥𝑝(q𝑖· k𝑥)\\nÍ𝑛−1\\n𝑘=0 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\nv𝑥\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑛−1\\n∑︁\\n𝑥=0\\n(𝐵𝐸𝐿𝑂𝑁𝐺𝑥== 𝑗)\\n𝑒𝑥𝑝(q𝑖· rj)\\nÍ𝑛−1\\n𝑘=0 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\nv𝑥\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑒𝑥𝑝(q𝑖· rj)\\nÍ𝑛−1\\n𝑘=0 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\n𝑛−1\\n∑︁\\n𝑥=0\\n(𝐵𝐸𝐿𝑂𝑁𝐺𝑥== 𝑗)v𝑥\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑒𝑥𝑝(q𝑖· rj)\\nÍ𝑛−1\\n𝑘=0 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\ne𝑣𝑗\\n(10)\\nCombining (7), (9) (10), we have oi = ÍN −1\\nj=0\\nePi,j\\nsi evj = eoi.\\nThis concludes that the output of our group attention is identical\\nto vanilla self-attention’s.\\n□\\nA.5\\nThe Proof of Error Bound (Lemma 1)\\nProof. We have\\n𝑒𝑥𝑝(𝑃𝑖,𝑗)\\n𝑒𝑥𝑝(𝑃𝑖,𝑗) = 𝑒𝑥𝑝(q𝑖· ek𝑗)\\n𝑒𝑥𝑝(q𝑖· k𝑗) = 𝑒𝑥𝑝(q𝑖· (ek𝑗−k𝑗))\\n= 𝑒𝑥𝑝(||q𝑖|| · ||ek𝑗−k𝑗|| · 𝑐𝑜𝑠(q𝑖,ek𝑗−k𝑗))\\n(11)\\nSo\\n𝑒𝑥𝑝(−𝑑𝑅) ≤𝑒𝑥𝑝(𝑃𝑖,𝑗)\\n𝑒𝑥𝑝(𝑃𝑖,𝑗) ≤𝑒𝑥𝑝(𝑑𝑅)\\n(12)\\nThen we have:\\n𝐴𝑖,𝑗\\n𝐴𝑖,𝑗\\n=\\n𝑒𝑥𝑝(𝑃𝑖,𝑗)\\nÍ𝑛\\n𝑘=1 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\n/\\n𝑒𝑥𝑝(𝑃𝑖,𝑗)\\nÍ𝑛\\n𝑘=1 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\n= 𝑒𝑥𝑝(𝑃𝑖,𝑗)\\n𝑒𝑥𝑝(𝑃𝑖,𝑗)\\nÍ𝑛\\n𝑘=1 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\nÍ𝑛\\n𝑘=1 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\n(13)\\nCombining (12) (13), the error is bounded by\\n𝑒𝑥𝑝(−2𝑑𝑅) ≤𝐴𝑖,𝑗\\n𝐴𝑖,𝑗\\n≤𝑒𝑥𝑝(2𝑑𝑅)\\n(14)\\nThus, if d ≤ln(𝜖)\\n2R , 1\\n𝜖≤Ai,j\\nAi,j ≤𝜖. This proves Lemma 1.\\nA.6\\nThe Proof of Merge Operation (Lemma 2)\\nProof. Denote the cluster size of 𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘to be 𝑛𝑘.After merge-\\ning, the new center will be:\\n𝑐′ =\\nÍ𝑚\\n𝑖=1 𝑛𝑘𝑖𝑐𝑘𝑖\\nÍ𝑚\\n𝑖=1 𝑛𝑘𝑖\\nFor ∀𝑖∈[1,𝑚], ∀𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘𝑖, it holds that:\\n|𝑥−𝑐′| ≤|𝑥−𝑐𝑘𝑖| + |𝑐𝑘𝑖−𝑐′| (𝑇𝑟𝑖𝑎𝑛𝑔𝑙𝑒𝑖𝑛𝑒𝑞𝑢𝑎𝑙𝑖𝑡𝑦)\\n= |𝑥−𝑐𝑘𝑖| + |\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗\\n𝑐𝑘𝑖−\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗𝑐𝑘𝑗\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗\\n|\\n= |𝑥−𝑐𝑘𝑖| + |\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗(𝑐𝑘𝑖−𝑐𝑘𝑗)\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗\\n|\\n= |𝑥−𝑐𝑘𝑖| +\\n| Í𝑚\\n𝑗=1 𝑛𝑘𝑗(𝑐𝑘𝑖−𝑐𝑘𝑗) |\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗\\n≤|𝑥−𝑐𝑘𝑖| +\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗|𝑐𝑘𝑖−𝑐𝑘𝑗|\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗\\n=\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗(|𝑐𝑘𝑖−𝑐𝑘𝑗| + |𝑥−𝑐𝑘𝑖|)\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗\\n≤\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗𝑑\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗\\n= 𝑑\\n(15)\\nA.7\\nDownstream Tasks\\nRITA supports a variety of downstream tasks. In this section, we\\nshow that with minimal modification RITA can effectively support\\nclassification, imputation and forecasting tasks. Other unsupervised\\ntasks such as similarity search or clustering are naturally supported\\nby extracting feature embeddings from RITA.\\nA.7.1\\nClassification\\nTo classify timeseries, we input timeseries to the model as described\\nin Sec. 3 and attach a special token [CLS] as the first input em-\\nbedding. [CLS]’s embedding acts as the embedding for the entire\\ntimeseries, and the output representation of [CLS] is fed into a\\nclassifier: y = Softmax(WclsZ[CLS] + Bcls), where 𝑍[𝐶𝐿𝑆] ∈R𝑑is\\nthe output representation of [CLS], C is the number of classes, and\\nWcls ∈RC×d, Bcls ∈RC are learnable parameters for classification\\ntask. The result vector 𝑦∈R𝐶represents the possibility that the\\ninput timeseries belongs to each class.\\nWe apply Cross Entropy Loss as the loss function of the classi-\\nfication task [13]: L = 1\\nC\\nÍC\\ni=1 −ˆy(i)log(y(i)), where ˆ𝑦is a binary\\nindicator for ground truth label:\\nˆ𝑦(𝑖) =\\n(\\n1\\n𝑖is ground truth label\\n0\\n𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒\\n(16)\\nA.7.2\\nImputation\\nTimeseries are mainly generated by sensors, a common problem\\nof which is missing values. This becomes a challenge when many\\ndownstream analytics require the missing values to be recovered.\\nThe recovering task is imputation.\\nDenote the real timeseries as𝑇𝑟∈R𝑡×𝑚, the observed timeseries\\nwith missing values as 𝑇𝑜∈R𝑡×𝑚, and the set of missing values’\\npositions as 𝑀. We scale the values of all timeseries to non-negative\\nand use a special value (-1) to indicate missing values:\\n𝑇𝑜(𝑖, 𝑗) =\\n(\\n−1\\n(𝑖, 𝑗) ∈𝑀\\n𝑇𝑟(𝑖, 𝑗)\\n(𝑖, 𝑗) ∉𝑀\\n(17)\\n𝑇𝑜is fed into the RITA as input, and the output representa-\\ntions are concatenated and fed into a Transpose Convolution layer\\nwhich decodes the output embedding vectors from hidden space to\\ntimeseries values, corresponding to the convolution operation in\\n13\\nthe input stage, i.e., Y = TransposeCNN (Z1 +○Z2 +○... +○Zn), where\\n𝑌∈R𝑡×𝑚is the recovered timeseries, and 𝑍𝑖∈R𝑑is the output of\\neach position.\\nHere Mean Square Error is chosen as the loss function [51]:\\n𝐿=\\n1\\n|𝑀|\\nÍ\\n(𝑖,𝑗)∈𝑀(𝑌(𝑖, 𝑗) −𝑇𝑟(𝑖, 𝑗))2.\\nA.7.3\\nForecasting\\nForecasting can be regarded as a special case of imputation, in\\nwhich all missing values are at the end of timeseries.\\nSo like in imputation task, we scale the timeseries to non-\\nnegative and use a special value (-1) to indicate the values to be\\npredicted:\\n𝑇𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑(𝑖, 𝑗) =\\n(\\n𝑇𝑟𝑒𝑎𝑙(𝑖, 𝑗)\\n𝑖≤𝑡𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑\\n−1\\n𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒\\n(18)\\nWhere 𝑡𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑is the observed timestamp. Then the output\\nrepresentations are fed into a Transpose Convolution layer using\\nMean Squared Error as loss function, as described above.\\nA.7.4\\nOther Unsupervised Tasks\\nRITA naturally supports other unsupervised tasks, such as similar-\\nity search and clustering [25, 31, 32], by producing the embedding\\nof one timeseries (output representation of the special token [CLS]).\\nClustering can be performed on the embeddings with flexible choice\\nof distance metrics. Similarly, a high dimensional similarity search\\nsystem [22, 23, 38] can be built on the embeddings.\\nA.8\\nInference Time\\nDataset\\nLength\\nTST[61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nWISDM\\n200\\n2.18\\n2.26\\n2.35\\n2.22\\n2.17\\nHHAR\\n200\\n1.19\\n1.23\\n1.28\\n1.21\\n1.18\\nRWHAR\\n200\\n1.32\\n1.37\\n1.42\\n1.34\\n1.31\\nECG\\n2000\\n18.44\\n15.26\\n5.80\\n6.08\\n5.16\\nTable 6: Inference time: Classification on multi-variate data\\n(seconds).\\nDataset\\nLength\\nTST[61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nWISDM\\n200\\n2.03\\n2.11\\n2.19\\n2.07\\n2.02\\nHHAR\\n200\\n1.11\\n1.14\\n1.19\\n1.12\\n1.10\\nRWHAR\\n200\\n1.23\\n1.27\\n1.32\\n1.25\\n1.22\\nECG\\n2000\\n17.22\\n14.32\\n4.73\\n4.99\\n4.11\\nMGH\\n10000\\nN/A\\nN/A\\n6.58\\n6.88\\n1.35\\nTable 7: Inference time: Imputation on multi-variate data\\n(seconds).\\nIn this section, we present the average inference time on valida-\\ntion sets. The results in Table. 6 and 7 correspond to the average\\ninference time on validation sets of classification and imputation\\ntasks, respectively. Consistent with the results in Section. 6.3, our\\nmethod Group Attn. outperforms the baselines on both classifica-\\ntion and imputation tasks, particularly on the datasets comprising\\nlong timeseries (ECG and MGH).\\n14\\n')]\n"
     ]
    }
   ],
   "source": [
    "# Arxiv - URL PDF loader\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "docs = ArxivLoader(query=\"Attention is All you Need\", load_max_docs=2).load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'title': 'Generative AI pornography', 'summary': 'Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including Generative adversarial network (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.', 'source': 'https://en.wikipedia.org/wiki/Generative_AI_pornography'}, page_content='Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including Generative adversarial network (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.\\n\\n\\n== History ==\\nThe use of generative AI in the adult industry began in the late 2010s, initially focusing on AI-generated art, music, and visual content. This trend accelerated in 2022 with Stability AI\\'s release of Stable Diffusion (SD), an open-source text-to-image model that enables users to generate images, including NSFW content, from text prompts using the LAION-Aesthetics subset of the LAION-5B dataset. Despite Stability AI\\'s warnings against sexual imagery, SD\\'s public release led to dedicated communities exploring both artistic and explicit content, sparking ethical debates over open-access AI and its use in adult media. By 2020, AI tools had advanced to generate highly realistic adult content, amplifying calls for regulation.\\n\\n\\n=== AI-generated influencers ===\\nOne application of generative AI technology is the creation of AI-generated influencers on platforms such as OnlyFans and Instagram. These AI personas interact with users in ways that can mimic real human engagement, offering an entirely synthetic but convincing experience. While popular among niche audiences, these virtual influencers have prompted discussions about authenticity, consent, and the blurring line between human and AI-generated content, especially in adult entertainment.\\n\\n\\n=== The growth of AI porn sites ===\\nBy 2023, websites dedicated to AI-generated adult content had gained traction, catering to audiences seeking customizable experiences. These platforms allow users to create or view AI-generated pornography tailored to their preferences. These platforms enable users to create or view AI-generated adult content appealing to different preferences through prompts and tags, customizing body type, facial features, and art styles. Tags further refine the output, creating niche and diverse content. Many sites feature extensive image libraries and continuous content feeds, combining personalization with discovery and enhancing user engagement. AI porn sites, therefore, attract those seeking unique or niche experiences, sparking debates on creativity and the ethical boundaries of AI in adult media.\\n\\n\\n== Ethical concerns and misuse ==\\nThe growth of generative AI pornography has also attracted some cause for criticism. AI technology can be exploited to create non-consensual pornographic material, posing risks similar to those seen with deepfake revenge porn and AI-generated NCII (Non-Consensual Intimate Image). A 2023 analysis found that 98% of deepfake videos online are pornographic, with 99% of the victims being women. Some famous celebrities victims of deepfake include Scarlett Johansson, Taylor Swift, and Maisie Williams.\\nOpenAI is exploring whether NSFW content, such as erotica, can be responsibly generated in age-appropriate contexts while maintaining its ban on deepfakes. This proposal has attracted criticism from child safety campaigners who argue it undermines OpenAI\\'s mission to develop \"safe and beneficial\" AI. Additionally, the Internet Watch Foundation has raised concerns about AI being used to generate sexual abuse content involving children.\\n\\n\\n=== AI-generated non-consensual intimate imagery (AI Undress) ===\\nSeveral US states are taking actions against using deepfake apps and sharing them on the internet. In 2024, San Francisco filed a landmark lawsuit to shut down \"undress\" apps that allow users to generate non-consensual AI nude images, citing violations of state laws. The case aligns with California\\'s recent legislation—SB 926, SB 942, and SB 981—championed by Senators Aisha Wahab a'), Document(metadata={'title': 'Generative artificial intelligence', 'summary': 'Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\\nGenerative AI tools have become more common since an \"AI boom\" in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora. Technology companies developing generative AI include OpenAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.\\nGenerative AI has raised many ethical questions. It can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.\\nGenerative AI is used across many industries. Examples include software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design.', 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}, page_content='Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\\nGenerative AI tools have become more common since an \"AI boom\" in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora. Technology companies developing generative AI include OpenAI, Anthropic, Meta AI, Microsoft, Google, DeepSeek, and Baidu.\\nGenerative AI has raised many ethical questions. It can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.\\nGenerative AI is used across many industries. Examples include software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design.\\n\\n\\n== History ==\\n\\n\\n=== Early history ===\\nThe first example of an algorithmically generated media is likely the Markov chain. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is learned on a text corpus, it can then be used as a probabilistic text generator.\\nComputers were needed to go beyond Markov chains. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.\\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\\n\\n\\n=== Generative neural networks (2014–2019) ===\\n\\nSince inception, the field of machine learning has used both discriminative models and generative models to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress, and research in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling.\\nIn 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images.\\nIn 2017, the Transformer network enabled advancements in generative models compared to older Long-Short Term Memory models, leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018. This was followed in 2019 by GPT-2, which demonstrated the ability to generalize unsupervised to many different tasks as a Foundation model.\\nThe new generative models introduced')]\n"
     ]
    }
   ],
   "source": [
    "# Wikipedia content read\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "docs = WikipediaLoader(query=\"Generative AI\", load_max_docs=2).load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5742e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07adfdea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b76607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20772e14",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
